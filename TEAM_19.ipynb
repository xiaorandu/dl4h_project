{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j01aH0PR4Sg-"
      },
      "source": [
        "<!-- # Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "**Github: https://github.com/xiaorandu/dl4h_project**\n",
        "<br>**Background of the problem**\n",
        "  * **type of problem:** Artificial intelligence (AI) is being used to help aid drug discovery; however, many of these processes focus on the studies of chemical structures and largely ignoring the plethora of information found in text-based instructions. This limitation hinders the advancement of textual descriptions for drug design, molecule editing, and predicting complex biological activities.\n",
        "\n",
        "  * **importance/meaning of solving the problem:** Solving this problem will help enable faster iterations of drug discovery, such as re-purposing and multi-objective lead optimization.\n",
        "\n",
        "  * **the difficulty of the problem:** This problem contains many difficult aspects including the zero shot learning tasks (which are especially difficult in the context of bio chemistry) and understanding natural language. Data insufficiency (PubChemSTM consists of 250,000 molecules and 281,000 structure-text pairs vs. 400 million in the vision-language domain used by peers from other domains) is another limitation, and the expressiveness of chemical structure models is also a bottleneck of this work.\n",
        "\n",
        "  * **the state of the art methods and effectiveness:** A multi modal model was designed that incorporates both molecular structural information and textual knowledge. A multi modal model, MoleculeSTM, which consists of two brances, the chemical structure branch (to handle molecules' internal structure)  and textual description branch (to handle external domain knowledge) was designed. Such design enables the model to be integrated with existing models trained on each seperately , i.e., molecular structural models and scientific language models. A large multi-modal structure-text dataset was created to align the two branhes of MoleculeSTM. Two challenging downstream tasks were desinged, the structure retrieval task and text based molecule editing task and petrained MoleculeSTM was applied on them in a zero-shot manner. By studing these tasks two main attributes of MoleculeSTM were summaried, open vocabulary and composibilty. Open vocabulary means the model can support exploring a wide range of biochemical concepts with unbound vocabulary. Compositionality means complex concepts can be expressed by decomposing it into several simpler concepts. Results had shown the effectiveness of MoleculeSTM can reach the best performance on six zero-shot retrival tasks, which is up to 50% higher accuracy and twenty zero-shot text-based editing tasks, which is up to 40% higher hit ratio when comparing with the stsate-of-the-art methods. Additionally, MoleculeSTM was able to detect critical structure inferred by text descriptions for molecular editing tasks.\n",
        "\n",
        "<br>**Paper explanation**\n",
        "  * **what did the paper propose:** The paper introduced MoleculeSTM, a model that integrates chemical structures of molecules with their textual descriptions using a contrastive learning approach. The model aims to perform tasks such as structure-text retrieval and molecule editing based on text instructions in a zero-shot setting. It utilizes a vast, multi-modal dataset, PubChemSTM, containing over 280,000 chemical structure-text pairs.\n",
        "\n",
        "  * **innovations of the method:** MoleculeSTM uniquely combines chemical structural data with textual information, enhancing the model's understanding and generalization capabilities across various biochemical contexts. It demonstrates significant effectiveness in zero-shot scenarios, where the model performs tasks without having been explicitly trained on them. The model supports open-ended vocabularies and can decompose complex instructions into simpler concepts, making it versatile in handling diverse and novel scientific queries.\n",
        "\n",
        "  * **how well the proposed method work (in its own metrics):** MoleculeSTM significantly outperformed existing methods in zero-shot retrieval and text-based molecule editing tasks. It achieved up to 50% higher accuracy in retrieval tasks and up to 40% higher hit ratios in editing tasks compared to state-of-the-art methods. This indicates a robust ability to generalize and effectively apply learned knowledge to new and unseen data.\n",
        "\n",
        "  * **what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem):** The research incorporated textual descriptions with chemical structures for molecule representation learning.The multi-modal model, MoleculeSTM, consistently showed improved performance when compared to the existing models. MoleculeSTM might accelerate various downstream drug discovery practices, since it was observed that the model can successfully modify molecule substructures to gain desired properties and also retrieve novel drug-target relations. This paper is important as it was able to illustrate the effictiveness of incorporating textual descriptions in addition to chemical structures for molecule representation learning. It did have two limitations,data insufficiency and expressiveness of the chemical structure models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper we will test and the corresponding experiments you will run.\n",
        "\n",
        "Hypothesis:\n",
        "1. MoleculeSTM achieves state-of-the-art performance in zero-shot structure-text retrieval and molecule editing tasks compared to the existing method.\n",
        "2. Incorporating textual descriptions through contrastive learning will significantly improve the model's performance on zero-shot retrieval and molecule editing tasks.\n",
        "\n",
        "Experiments:\n",
        "Retraining the model on the constructed PubChemSTM dataset, consisting of over 280,000 chemical structure-text pairs, and evaluating it on specified zero-shot tasks: structure-text retrieval and molecule editing.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j7hn1WK6WMR"
      },
      "source": [
        "##  Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLF7R1Kv6r05"
      },
      "source": [
        "### Python version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2WqmBOX64Bf",
        "outputId": "dc4610d9-21d1-4f98-c74d-7f3304ff4044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.3\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "TIfhWJ9ZQqYX"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt2nzVvzhZfO"
      },
      "source": [
        "Python 3.10.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIHuhq7A6myH"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rdkit in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (2023.9.6)\n",
            "Requirement already satisfied: numpy in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from rdkit) (10.3.0)\n",
            "Requirement already satisfied: torch in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (2.3.0)\n",
            "Requirement already satisfied: torchvision in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: requests in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (2.31.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (4.66.4)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.8.4-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
            "Collecting spacy\n",
            "  Using cached spacy-3.7.4-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
            "Collecting Levenshtein\n",
            "  Using cached Levenshtein-0.25.1-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
            "Collecting boto3\n",
            "  Using cached boto3-1.34.98-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting deepspeed\n",
            "  Using cached deepspeed-0.14.2.tar.gz (1.3 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Getting requirements to build wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [23 lines of output]\n",
            "      [WARNING] Unable to import torch, pre-compiling ops will be disabled. Please visit https://pytorch.org/ to see how to properly install torch on your system.\n",
            "      \u001b[93m [WARNING] \u001b[0m unable to import torch, please install it if you want to pre-compile any deepspeed ops.\n",
            "      DS_BUILD_OPS=1\n",
            "      Traceback (most recent call last):\n",
            "        File \"c:\\Users\\Benjamin\\Desktop\\Illinois\\598_DLH\\dl4h_project\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
            "          main()\n",
            "        File \"c:\\Users\\Benjamin\\Desktop\\Illinois\\598_DLH\\dl4h_project\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
            "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"c:\\Users\\Benjamin\\Desktop\\Illinois\\598_DLH\\dl4h_project\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
            "          return hook(config_settings)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Benjamin\\AppData\\Local\\Temp\\pip-build-env-ecrxoiny\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 325, in get_requires_for_build_wheel\n",
            "          return self._get_build_requires(config_settings, requirements=['wheel'])\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Benjamin\\AppData\\Local\\Temp\\pip-build-env-ecrxoiny\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 295, in _get_build_requires\n",
            "          self.run_setup()\n",
            "        File \"C:\\Users\\Benjamin\\AppData\\Local\\Temp\\pip-build-env-ecrxoiny\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 487, in run_setup\n",
            "          super().run_setup(setup_script=setup_script)\n",
            "        File \"C:\\Users\\Benjamin\\AppData\\Local\\Temp\\pip-build-env-ecrxoiny\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 311, in run_setup\n",
            "          exec(code, locals())\n",
            "        File \"<string>\", line 148, in <module>\n",
            "      AssertionError: Unable to pre-compile ops without torch installed. Please install torch before attempting to pre-compile ops.\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: subprocess-exited-with-error\n",
            "\n",
            "× Getting requirements to build wheel did not run successfully.\n",
            "│ exit code: 1\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ogb==1.2.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (1.2.0)\n",
            "Requirement already satisfied: torch>=1.2.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from ogb==1.2.0) (2.3.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from ogb==1.2.0) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from ogb==1.2.0) (4.66.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from ogb==1.2.0) (1.4.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from ogb==1.2.0) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from ogb==1.2.0) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from ogb==1.2.0) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from pandas>=0.24.0->ogb==1.2.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from pandas>=0.24.0->ogb==1.2.0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from pandas>=0.24.0->ogb==1.2.0) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->ogb==1.2.0) (1.13.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->ogb==1.2.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->ogb==1.2.0) (3.5.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch>=1.2.0->ogb==1.2.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch>=1.2.0->ogb==1.2.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch>=1.2.0->ogb==1.2.0) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch>=1.2.0->ogb==1.2.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch>=1.2.0->ogb==1.2.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch>=1.2.0->ogb==1.2.0) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch>=1.2.0->ogb==1.2.0) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from tqdm>=4.29.0->ogb==1.2.0) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.2.0->ogb==1.2.0) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.2.0->ogb==1.2.0) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from jinja2->torch>=1.2.0->ogb==1.2.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from sympy->torch>=1.2.0->ogb==1.2.0) (1.3.0)\n",
            "Requirement already satisfied: transformers==4.30.2 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from transformers==4.30.2) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from transformers==4.30.2) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from transformers==4.30.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from transformers==4.30.2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from transformers==4.30.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from transformers==4.30.2) (2024.4.28)\n",
            "Requirement already satisfied: requests in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from transformers==4.30.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from transformers==4.30.2) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from transformers==4.30.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from transformers==4.30.2) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.11.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.30.2) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from requests->transformers==4.30.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from requests->transformers==4.30.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from requests->transformers==4.30.2) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from requests->transformers==4.30.2) (2024.2.2)\n",
            "Requirement already satisfied: torch_geometric in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: scipy in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch_geometric) (1.13.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch_geometric) (2024.3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch_geometric) (5.9.8)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from requests->torch_geometric) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Requirement already satisfied: torch-scatter in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (2.1.2+pt22cu121)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Requirement already satisfied: torch-sparse in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (0.6.18+pt22cu121)\n",
            "Requirement already satisfied: scipy in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch-sparse) (1.13.0)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Requirement already satisfied: torch-cluster in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (1.6.3+pt22cu121)\n",
            "Requirement already satisfied: scipy in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from torch-cluster) (1.13.0)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Requirement already satisfied: torch-spline-conv in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (1.2.2+pt22cu121)\n",
            "Collecting git+https://github.com/MolecularAI/pysmilesutils.git\n",
            "  Cloning https://github.com/MolecularAI/pysmilesutils.git to c:\\users\\benjamin\\appdata\\local\\temp\\pip-req-build-69mbdatx\n",
            "  Resolved https://github.com/MolecularAI/pysmilesutils.git to commit b1e7ced15a42e18e629b984816020b1f0b0a2aa3\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/MolecularAI/pysmilesutils.git 'C:\\Users\\Benjamin\\AppData\\Local\\Temp\\pip-req-build-69mbdatx'\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit\n",
        "!pip install torch torchvision\n",
        "!pip install requests tqdm matplotlib spacy Levenshtein boto3 deepspeed\n",
        "!pip install ogb==1.2.0\n",
        "!pip install transformers==4.30.2\n",
        "\n",
        "!pip install torch_geometric\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
        "!pip install git+https://github.com/MolecularAI/pysmilesutils.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd2xQJO8NTmf",
        "outputId": "0c430667-9248-444f-efdc-9765af3dac09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/MoleculeSTM\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BRL9wMqqHWBk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: packaging in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (24.0)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (0.19.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (from jedi>=0.16) (0.8.4)\n",
            "Requirement already satisfied: cxxfilt>=0.2.0 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (0.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\benjamin\\desktop\\illinois\\598_dlh\\dl4h_project\\.venv\\lib\\site-packages (6.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install packaging\n",
        "!pip install \"jedi>=0.16\"\n",
        "!pip install \"cxxfilt>=0.2.0\"\n",
        "!pip install \"PyYAML>=5.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ETR79LAPGk4B",
        "outputId": "e1c0ba4b-7c6d-429f-d55a-baedc311f674"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/MoleculeSTM'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify package installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSFPUp47H8LJ",
        "outputId": "b0484e07-410a-41cd-a7c6-3b7eb960bc43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cxxfilt >= 0.2.0 is installed (Current version: 0.3.0)\n",
            "tqdm >= 4.28.1 is installed (Current version: 4.66.4)\n",
            "numpy >= 1.15.3 is installed (Current version: 1.26.4)\n",
            "PyYAML >= 5.1 is installed (Current version: 6.0.1)\n",
            "pytest >= 3.5.1 is installed (Current version: 3.5.1)\n",
            "packaging >= 14.0 is installed (Current version: 24.0)\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "\n",
        "# List of packages from apex requirements.txt\n",
        "required_packages = {\n",
        "    \"cxxfilt\": \"0.2.0\",\n",
        "    \"tqdm\": \"4.28.1\",\n",
        "    \"numpy\": \"1.15.3\",\n",
        "    \"PyYAML\": \"5.1\",\n",
        "    \"pytest\": \"3.5.1\",\n",
        "    \"packaging\": \"14.0\"\n",
        "}\n",
        "\n",
        "def check_packages(packages):\n",
        "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
        "    for package, version in packages.items():\n",
        "        current_version = pkg_resources.get_distribution(package).version\n",
        "        if current_version and (pkg_resources.parse_version(current_version) >= pkg_resources.parse_version(version)):\n",
        "            print(f\"{package} >= {version} is installed (Current version: {current_version})\")\n",
        "        else:\n",
        "            missing_version = version if not current_version else f\"{current_version} (Required: {version})\"\n",
        "            print(f\"{package} >= {version} is NOT installed. Current/Required version: {missing_version}\")\n",
        "\n",
        "check_packages(required_packages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syK8SSjkhS2W"
      },
      "outputs": [],
      "source": [
        "# install apex\n",
        "!git clone https://github.com/chao1224/apex.git\n",
        "%cd apex\n",
        "!pip install -v --disable-pip-version-check --no-cache-dir ./\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bwItJOQwJXR"
      },
      "outputs": [],
      "source": [
        "# install metagron\n",
        "# TODO: delete me, lets focus on graph\n",
        "!git clone https://github.com/MolecularAI/MolBART.git --branch megatron-molbart-with-zinc\n",
        "%cd MolBART/megatron_molbart/Megatron-LM-v1.1.5-3D_parallelism\n",
        "!pip install .\n",
        "%cd ../../.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "<!-- Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzVUQS0CHry0"
      },
      "source": [
        "\n",
        "### Data download instruction\n",
        "We can use the following python script to download the pretraining dataset and downstream datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qrB1KAwzX5G"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, snapshot_download\n",
        "api = HfApi()\n",
        "snapshot_download(repo_id=\"chao1224/MoleculeSTM\", repo_type=\"dataset\", local_dir='data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo7bQ8AwzeSL"
      },
      "source": [
        "The data folder can be found at [google drive link](https://drive.google.com/drive/u/0/folders/1pCr0WrY-3lbxxy44D68u2cDzmDONVLBD).\n",
        "<br/>The data folder will include:\n",
        "```\n",
        "data\n",
        "├── PubChemSTM_data/\n",
        "│   └── raw\n",
        "│        └── CID2SMILES.csv\n",
        "│        └── CID2name.json\n",
        "│        └── CID2name_raw.json\n",
        "│        └── molecules.sdf\n",
        "│   └── processed/\n",
        "├── pretrained_SciBERT/\n",
        "├── pretrained_MegaMolBART/\n",
        "├── pretrained_KV-PLM/\n",
        "├── pretrained_GraphMVP/\n",
        "├── pretrained_MoleculeSTM_Raw/\n",
        "├── pretrained_MoleculeSTM/\n",
        "├── DrugBank_data/\n",
        "├── ZINC250K_data/\n",
        "├── Editing_data/\n",
        "│   └── single_multi_property_SMILES.txt\n",
        "│   └── neighbor2drug/\n",
        "│   └── ChEMBL_data/\n",
        "└── MoleculeNet_data/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hANVl1yo7R42"
      },
      "source": [
        "### Data descriptions with helpful charts and visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ioy_Wg6j77jF"
      },
      "outputs": [],
      "source": [
        "%cd MoleculeSTM/data/PubChemSTM_data/raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SMILE (simplified molecular input line entry system) is used to turn a three dimensional chemical structure into a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlSN1l2H8JEX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "CID2SMILES = 'CID2SMILES.csv'\n",
        "df_CID2SMILES = pd.read_csv(CID2SMILES, usecols=['CID', 'SMILES'])\n",
        "df_CID2SMILES.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbw3AlHBgEAp"
      },
      "source": [
        "```\n",
        "index\tCID\tSMILES\n",
        "0\t     1\tCC(=O)OC(CC(=O)[O-])C[N+](C)(C)C\n",
        "1\t     3\tO=C(O)C1=CC=CC(O)C1O\n",
        "2\t     4\tCC(O)CN\n",
        "3\t     5\tNCC(=O)COP(=O)(O)O\n",
        "4\t     6\tO=[N+]([O-])c1ccc(Cl)c([N+](=O)[O-])c1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSB49QE4NsS1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "CID2name_raw = \"CID2name_raw.json\"\n",
        "with open(CID2name_raw, 'r') as file:\n",
        "  data = json.load(file)\n",
        "\n",
        "df_CID2name_raw = pd.DataFrame(list(data.items()), columns=['CID', 'Names'])\n",
        "df_CID2name_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuBWGhV-gWho"
      },
      "source": [
        "```\n",
        "index\tCID\tNames\n",
        "0\t    180\tAcetone,ACETONE,Acetone\n",
        "1\t    222\tAmmonia,AMMONIA SOLUTIONS (CONTAINING MORE THAN 35% BUT NOT MORE THAN 50% AMMONIA),AMMONIA, ANHYDROUS,AMMONIA, SOLUTION, WITH MORE THAN 10% BUT NOT MORE THAN 35% AMMONIA,Ammonia\n",
        "2\t   5359596\tArsenic,ARSENIC,Arsenic,Arsenic atom,Arsenic Compounds\n",
        "3\t    241\tBenzene,BENZENE,Benzene,Benzene\n",
        "4\t   23973\tCadmium,CADMIUM,Cadmium atom,Cadmium,Cadmium Compounds\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im2SnRTR9Umy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "CID2name = \"CID2name.json\"\n",
        "with open(CID2name, 'r') as file:\n",
        "  data = json.load(file)\n",
        "\n",
        "df_CID2name = pd.DataFrame(list(data.items()), columns=['CID', 'Names'])\n",
        "df_CID2name.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh3sN5gAghSu"
      },
      "source": [
        "```\n",
        "index\tCID\tNames\n",
        "0\t    180\tAcetone,Acetone,Acetone,Acetone,Acetone\n",
        "1\t    222\tAmmonia,Ammonia solutions (containing more than 35% but not more than 50% ammonia),Ammonia, anhydrous,Ammonia, solution, with more than 10% but not more than 35% ammonia,Ammonia,Ammonia,Ammonia,Ammonia\n",
        "2\t   5359596\tArsenic,Arsenic,Arsenic,Arsenic atom,Arsenic, a naturally occurring element,,Arsenic\n",
        "3\t    241\tBenzene,Benzene,Benzene,Benzene,Benzene,Benzene\n",
        "4\t   23973\tCadmium,Cadmium,Cadmium atom,The main sources of cadmium in the air,Cadmium,Cadmium,Cadmium\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8rVWwCcCgJO"
      },
      "outputs": [],
      "source": [
        "from rdkit.Chem import PandasTools\n",
        "\n",
        "sdf_file = \"molecules.sdf\"\n",
        "df_molecules = PandasTools.LoadSDF(sdf_file)\n",
        "print(df_molecules.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b_FWKgCMc8g"
      },
      "source": [
        "```\n",
        "# the output would be like this.\n",
        "  PUBCHEM_COMPOUND_CID PUBCHEM_COMPOUND_CANONICALIZED  \\\n",
        "0             29500027                              1   \n",
        "1             29500038                              1   \n",
        "2             29500039                              1   \n",
        "3             29500070                              1   \n",
        "4             29500073                              1   \n",
        "\n",
        "  PUBCHEM_CACTVS_COMPLEXITY PUBCHEM_CACTVS_HBOND_ACCEPTOR  \\\n",
        "0                       460                             7   \n",
        "1                       480                             6   \n",
        "2                       480                             6   \n",
        "3                       543                             6   \n",
        "4                       451                             6   \n",
        "\n",
        "  PUBCHEM_CACTVS_HBOND_DONOR PUBCHEM_CACTVS_ROTATABLE_BOND  \\\n",
        "0                          1                             4   \n",
        "1                          2                             6   \n",
        "2                          2                             6   \n",
        "3                          1                             4   \n",
        "4                          2                             3   \n",
        "\n",
        "                             PUBCHEM_CACTVS_SUBSKEYS  \\\n",
        "0  AAADccB7oABAAAAAAAAAAAAAAAAAAWLAAAA8QAAAAAAAAA...   \n",
        "1  AAADceB7sABgAAAAAAAAAAAAAAAAAWJAAAAsAAAAAAAAAA...   \n",
        "2  AAADceB7sABgAAAAAAAAAAAAAAAAAWJAAAAsAAAAAAAAAA...   \n",
        "3  AAADceB7oABAAAAAAAAAAAAAAAAAAWLAAAAwYAAAAAAAAA...   \n",
        "4  AAADceB7oABAAAAAAAAAAAAAAAAAAWAAAAA8QAAAAAAAAA...   \n",
        "\n",
        "                          PUBCHEM_IUPAC_OPENEYE_NAME  \\\n",
        "0  N-[4-(2-pyridyl)thiazol-2-yl]-4-(tetrazol-1-yl...   \n",
        "1  (3S)-3-acetamido-N-[4-(2-pyridyl)thiazol-2-yl]...   \n",
        "2  (3R)-3-acetamido-N-[4-(2-pyridyl)thiazol-2-yl]...   \n",
        "3  4-(tetrazol-1-yl)-N-[4-(2,4,5-trimethylphenyl)...   \n",
        "4  3-amino-N-[4-(2,4,5-trimethylphenyl)thiazol-2-...   \n",
        "\n",
        "                              PUBCHEM_IUPAC_CAS_NAME  \\\n",
        "0  N-[4-(2-pyridinyl)-2-thiazolyl]-4-(1-tetrazoly...   \n",
        "1  (3S)-3-acetamido-N-[4-(2-pyridinyl)-2-thiazoly...   \n",
        "2  (3R)-3-acetamido-N-[4-(2-pyridinyl)-2-thiazoly...   \n",
        "3  4-(1-tetrazolyl)-N-[4-(2,4,5-trimethylphenyl)-...   \n",
        "4  3-amino-N-[4-(2,4,5-trimethylphenyl)-2-thiazol...   \n",
        "\n",
        "                           PUBCHEM_IUPAC_NAME_MARKUP  ...  \\\n",
        "0  <I>N</I>-(4-pyridin-2-yl-1,3-thiazol-2-yl)-4-(...  ...   \n",
        "1  (3<I>S</I>)-3-acetamido-<I>N</I>-(4-pyridin-2-...  ...   \n",
        "2  (3<I>R</I>)-3-acetamido-<I>N</I>-(4-pyridin-2-...  ...   \n",
        "3  4-(tetrazol-1-yl)-<I>N</I>-[4-(2,4,5-trimethyl...  ...   \n",
        "4  3-amino-<I>N</I>-[4-(2,4,5-trimethylphenyl)-1,...  ...   \n",
        "\n",
        "  PUBCHEM_BOND_UDEF_STEREO_COUNT PUBCHEM_ISOTOPIC_ATOM_COUNT  \\\n",
        "0                              0                           0   \n",
        "1                              0                           0   \n",
        "2                              0                           0   \n",
        "3                              0                           0   \n",
        "4                              0                           0   \n",
        "\n",
        "  PUBCHEM_COMPONENT_COUNT PUBCHEM_CACTVS_TAUTO_COUNT PUBCHEM_COORDINATE_TYPE  \\\n",
        "0                       1                         -1               1\\n5\\n255   \n",
        "1                       1                         -1               1\\n5\\n255   \n",
        "2                       1                         -1               1\\n5\\n255   \n",
        "3                       1                         -1               1\\n5\\n255   \n",
        "4                       1                         -1               1\\n5\\n255   \n",
        "\n",
        "                             PUBCHEM_BONDANNOTATIONS        ID  \\\n",
        "0  1  18  8\\n1  20  8\\n10  12  8\\n10  13  8\\n11  ...  29500027   \n",
        "1  1  11  8\\n1  16  8\\n11  13  8\\n13  15  8\\n15  ...  29500038   \n",
        "2  1  11  8\\n1  16  8\\n11  13  8\\n13  15  8\\n15  ...  29500039   \n",
        "3  1  19  8\\n1  20  8\\n10  14  8\\n11  12  8\\n11  ...  29500070   \n",
        "4  1  18  8\\n1  19  8\\n10  11  8\\n10  12  8\\n11  ...  29500073   \n",
        "\n",
        "                                              ROMol PUBCHEM_XLOGP3  \\\n",
        "0  <rdkit.Chem.rdchem.Mol object at 0x7d30b3ca4cf0>            NaN   \n",
        "1  <rdkit.Chem.rdchem.Mol object at 0x7d30b3ca4c80>            NaN   \n",
        "2  <rdkit.Chem.rdchem.Mol object at 0x7d30b3ca5070>            NaN   \n",
        "3  <rdkit.Chem.rdchem.Mol object at 0x7d30b3ca51c0>            NaN   \n",
        "4  <rdkit.Chem.rdchem.Mol object at 0x7d30b3ca5310>            NaN   \n",
        "\n",
        "  PUBCHEM_REFERENCE_STANDARDIZATION  \n",
        "0                               NaN  \n",
        "1                               NaN  \n",
        "2                               NaN  \n",
        "3                               NaN  \n",
        "4                               NaN  \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfSQcMIk7lmR"
      },
      "source": [
        "### Preprocessing code + command"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv49mufsG2mO"
      },
      "source": [
        "The preprocessing code is inside preprocessing/PubchemSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MusqjT86m8En"
      },
      "source": [
        "##   Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBy2DO_0rtkB"
      },
      "source": [
        "### **Citation to the original paper**\n",
        "```\n",
        "@article{liu2023moleculestm,\n",
        "    title={Multi-modal molecule structure-text model for text-based retrieval and editing},\n",
        "    author={Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Anima},\n",
        "    title={Multi-modal molecule structure--text model for text-based retrieval and editing},\n",
        "    journal={Nature Machine Intelligence},\n",
        "    year={2023},\n",
        "    month={Dec},\n",
        "    day={01},\n",
        "    volume={5},\n",
        "    number={12},\n",
        "    pages={1447-1457},\n",
        "    issn={2522-5839},\n",
        "    doi={10.1038/s42256-023-00759-6},\n",
        "    url={https://doi.org/10.1038/s42256-023-00759-6}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITlAYslsrxmU"
      },
      "source": [
        "### **Link to the original paper’s repo**\n",
        "https://github.com/chao1224/MoleculeSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOoZE1DRnAmG"
      },
      "source": [
        "### **Model descriptions**\n",
        "MoleculeSTM consists of two branches: the **chemical structure branch $x_c$** and the **textual description branch $x_t$**.\n",
        "\n",
        "*   The chemical structure branch illustrates the arrangement of atoms in a moleculem, and we specifically focus on its **two-dimensional molecular graph**, which takes the atoms and bonds as the nodes and edges, respectively.\n",
        "*   The textual description branch provides a high-level description of the molecule’s functionality.\n",
        "\n",
        "\n",
        "This paper applies several models for text-based retrival and editing tasks from multi-modal molecule structure-text model. We apply the models below in our project.\n",
        "\n",
        "*   **Molecule graph encoder $f_c$**: apply a graph neural network (GNN) encoder to get a latent vector as molecule representation. Speciafically, we take a pretrained graph isomorphism network using **GraphMVP** pretraining.\n",
        "\n",
        "  *   GraphMVP doing a multi-view pretraining between the two-dimensional topologies and three-dimensional geometries on 250,000 conformations from the Geometric Ensemble of Molecules (GEOM) dataset.\n",
        "    \n",
        "*   **Text encoder $f_t$**: adapt the pretrained **SciBERT32** languange model, which was pretrained on the textual data from the chemical and biological domain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWmHSgGgrbky"
      },
      "source": [
        "### **Implementation code**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ErFv9PosM5R"
      },
      "source": [
        "#### Molecule graph encoder $f_c$\n",
        "*   `GINConv` and `GCNConv` classes:\n",
        "  *   These are convolutional layers used within the GNN, configured with `*emb_dim` and aggregation method.\n",
        "*   `GNN` class:\n",
        "  *   Constructs a sequence of GNN layers (`gnns`) based on `num_layer` and `gnn_type`.\n",
        "  *   Applies batch normalization (`batch_norms`) across layers.\n",
        "  *   Utilizes dropout as specified by `dropout_ratio`.\n",
        "  *   Incorporates the Jumping Knowledge (JK) network setup through `JK`.\n",
        "*   `GNN_graph` class:\n",
        "  *   Extends the GNN model to predict properties at the graph level.\n",
        "  *   Uses `graph_pooling` for reducing node features to graph features.\n",
        "  *   Integrates the entire node model (`molecule_node_model`) for processing graphs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "R5fLdx2lsLY6"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Could not find module 'C:\\Users\\Benjamin\\Desktop\\Illinois\\598_DLH\\dl4h_project\\.venv\\Lib\\site-packages\\torch_scatter\\_scatter_cuda.pyd' (or one of its dependencies). Try using the full path with constructor syntax.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[34], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glorot, zeros\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_self_loops, softmax, degree\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_scatter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scatter_add\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mogb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraphproppred\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmol_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AtomEncoder, BondEncoder\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n",
            "File \u001b[1;32mc:\\Users\\Benjamin\\Desktop\\Illinois\\598_DLH\\dl4h_project\\.venv\\Lib\\site-packages\\torch_scatter\\__init__.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m spec \u001b[38;5;241m=\u001b[39m cuda_spec \u001b[38;5;129;01mor\u001b[39;00m cpu_spec\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBUILD_DOCS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibrary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mosp\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Benjamin\\Desktop\\Illinois\\598_DLH\\dl4h_project\\.venv\\Lib\\site-packages\\torch\\_ops.py:1032\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1027\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m-> 1032\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
            "File \u001b[1;32mC:\\Python311\\Lib\\ctypes\\__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find module 'C:\\Users\\Benjamin\\Desktop\\Illinois\\598_DLH\\dl4h_project\\.venv\\Lib\\site-packages\\torch_scatter\\_scatter_cuda.pyd' (or one of its dependencies). Try using the full path with constructor syntax."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import (MessagePassing, global_add_pool,\n",
        "                                global_max_pool, global_mean_pool)\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "from torch_geometric.utils import add_self_loops, softmax, degree\n",
        "from torch_scatter import scatter_add\n",
        "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class GINConv(MessagePassing):\n",
        "    def __init__(self, emb_dim, aggr=\"add\"):\n",
        "        '''\n",
        "            emb_dim (int): node embedding dimensionality\n",
        "        '''\n",
        "        super(GINConv, self).__init__(aggr=aggr)\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, emb_dim))\n",
        "        self.eps = torch.nn.Parameter(torch.Tensor([0]))\n",
        "\n",
        "        self.bond_encoder = BondEncoder(emb_dim = emb_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        edge_embedding = self.bond_encoder(edge_attr)\n",
        "        out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, edge_attr):\n",
        "        return F.relu(x_j + edge_attr)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "\n",
        "class GCNConv(MessagePassing):\n",
        "    def __init__(self, emb_dim, aggr=\"add\"):\n",
        "        super(GCNConv, self).__init__(aggr=aggr)\n",
        "\n",
        "        self.linear = torch.nn.Linear(emb_dim, emb_dim)\n",
        "        self.root_emb = torch.nn.Embedding(1, emb_dim)\n",
        "        self.bond_encoder = BondEncoder(emb_dim = emb_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        x = self.linear(x)\n",
        "        edge_embedding = self.bond_encoder(edge_attr)\n",
        "\n",
        "        row, col = edge_index\n",
        "\n",
        "        #edge_weight = torch.ones((edge_index.size(1), ), device=edge_index.device)\n",
        "        deg = degree(row, x.size(0), dtype = x.dtype) + 1\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        return self.propagate(edge_index, x=x, edge_attr = edge_embedding, norm=norm) + F.relu(x + self.root_emb.weight) * 1./deg.view(-1,1)\n",
        "\n",
        "    def message(self, x_j, edge_attr, norm):\n",
        "        return norm.view(-1, 1) * F.relu(x_j + edge_attr)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, num_layer, emb_dim, JK=\"last\", drop_ratio=0., gnn_type=\"gin\"):\n",
        "\n",
        "        if num_layer < 2:\n",
        "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
        "\n",
        "        super(GNN, self).__init__()\n",
        "        self.drop_ratio = drop_ratio\n",
        "        self.num_layer = num_layer\n",
        "        self.JK = JK\n",
        "\n",
        "        self.atom_encoder = AtomEncoder(emb_dim)\n",
        "\n",
        "        ###List of MLPs\n",
        "        self.gnns = nn.ModuleList()\n",
        "        for layer in range(num_layer):\n",
        "            if gnn_type == \"gin\":\n",
        "                self.gnns.append(GINConv(emb_dim, aggr=\"add\"))\n",
        "            elif gnn_type == \"gcn\":\n",
        "                self.gnns.append(GCNConv(emb_dim))\n",
        "\n",
        "        ###List of batchnorms\n",
        "        self.batch_norms = nn.ModuleList()\n",
        "        for layer in range(num_layer):\n",
        "            self.batch_norms.append(nn.BatchNorm1d(emb_dim))\n",
        "\n",
        "    def forward(self, *argv):\n",
        "        if len(argv) == 3:\n",
        "            x, edge_index, edge_attr = argv[0], argv[1], argv[2]\n",
        "        elif len(argv) == 1:\n",
        "            data = argv[0]\n",
        "            x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        else:\n",
        "            raise ValueError(\"unmatched number of arguments.\")\n",
        "\n",
        "        x = self.atom_encoder(x)\n",
        "\n",
        "        h_list = [x]\n",
        "        for layer in range(self.num_layer):\n",
        "            h = self.gnns[layer](h_list[layer], edge_index, edge_attr)\n",
        "            h = self.batch_norms[layer](h)\n",
        "            # h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
        "            if layer == self.num_layer - 1:\n",
        "                # remove relu for the last layer\n",
        "                h = F.dropout(h, self.drop_ratio, training=self.training)\n",
        "            else:\n",
        "                h = F.dropout(F.relu(h), self.drop_ratio, training=self.training)\n",
        "            h_list.append(h)\n",
        "\n",
        "        ### Different implementations of Jk-concat\n",
        "        if self.JK == \"concat\":\n",
        "            node_representation = torch.cat(h_list, dim=1)\n",
        "        elif self.JK == \"last\":\n",
        "            node_representation = h_list[-1]\n",
        "        elif self.JK == \"max\":\n",
        "            h_list = [h.unsqueeze_(0) for h in h_list]\n",
        "            node_representation = torch.max(torch.cat(h_list, dim=0), dim=0)[0]\n",
        "        elif self.JK == \"sum\":\n",
        "            h_list = [h.unsqueeze_(0) for h in h_list]\n",
        "            node_representation = torch.sum(torch.cat(h_list, dim=0), dim=0)[0]\n",
        "        else:\n",
        "            raise ValueError(\"not implemented.\")\n",
        "        return node_representation\n",
        "\n",
        "\n",
        "class GNN_graphpred(nn.Module):\n",
        "    \"\"\"\n",
        "    Extension of GIN to incorporate edge information by concatenation.\n",
        "\n",
        "    Args:\n",
        "        num_layer (int): the number of GNN layers\n",
        "        arg.emb_dim (int): dimensionality of embeddings\n",
        "        num_tasks (int): number of tasks in multi-task learning scenario\n",
        "        JK (str): last, concat, max or sum.\n",
        "        graph_pooling (str): sum, mean, max, attention, set2set\n",
        "\n",
        "    See https://arxiv.org/abs/1810.00826\n",
        "    JK-net: https://arxiv.org/abs/1806.03536 \"\"\"\n",
        "\n",
        "    def __init__(self, num_layer, emb_dim, num_tasks, JK, graph_pooling, molecule_node_model=None):\n",
        "        super(GNN_graphpred, self).__init__()\n",
        "\n",
        "        if num_layer < 2:\n",
        "            raise ValueError(\"# layers must > 1.\")\n",
        "\n",
        "        self.molecule_node_model = molecule_node_model\n",
        "        self.num_layer = num_layer\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_tasks = num_tasks\n",
        "        self.JK = JK\n",
        "\n",
        "        # Different kind of graph pooling\n",
        "        if graph_pooling == \"sum\":\n",
        "            self.pool = global_add_pool\n",
        "        elif graph_pooling == \"mean\":\n",
        "            self.pool = global_mean_pool\n",
        "        elif graph_pooling == \"max\":\n",
        "            self.pool = global_max_pool\n",
        "        else:\n",
        "            raise ValueError(\"Invalid graph pooling type.\")\n",
        "\n",
        "        # For graph-level binary classification\n",
        "        self.mult = 1\n",
        "\n",
        "        if self.JK == \"concat\":\n",
        "            self.graph_pred_linear = nn.Linear(self.mult * (self.num_layer + 1) * self.emb_dim,\n",
        "                                               self.num_tasks)\n",
        "        else:\n",
        "            self.graph_pred_linear = nn.Linear(self.mult * self.emb_dim, self.num_tasks)\n",
        "        return\n",
        "\n",
        "    def from_pretrained(self, model_file):\n",
        "        print(\"Loading from {} ...\".format(model_file))\n",
        "        state_dict = torch.load(model_file)\n",
        "        self.molecule_node_model.load_state_dict(state_dict)\n",
        "        return\n",
        "\n",
        "    def forward(self, *argv):\n",
        "        if len(argv) == 4:\n",
        "            x, edge_index, edge_attr, batch = argv[0], argv[1], argv[2], argv[3]\n",
        "        elif len(argv) == 1:\n",
        "            data = argv[0]\n",
        "            x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "        else:\n",
        "            raise ValueError(\"unmatched number of arguments.\")\n",
        "\n",
        "        node_representation = self.molecule_node_model(x, edge_index, edge_attr)\n",
        "        graph_representation = self.pool(node_representation, batch)\n",
        "        output = self.graph_pred_linear(graph_representation)\n",
        "        return graph_representation, output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHR25Y5Sv7nn"
      },
      "source": [
        "For GraphMVP, check this [repo](https://github.com/chao1224/GraphMVP), and use thie checkpoints on this [link](https://drive.google.com/drive/u/1/folders/1uPsBiQF3bfeCAXSDd4JfyXiTh-qxYfu6).\n",
        "```\n",
        "pretrained_GraphMVP/\n",
        "├── GraphMVP_C\n",
        "│   └── model.pth\n",
        "└── GraphMVP_G\n",
        "    └── model.pth\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnYJzdhZxcKU"
      },
      "source": [
        "#### Text encoder $f_t$\n",
        "This can be done by calling the following from SciBERT:\n",
        "```\n",
        "SciBERT_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
        "SciBERT_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8OX6fDexyrA"
      },
      "source": [
        "### **Pretrained model**\n",
        "In the pretrain phase, MoleculeSTM aims to map the representations extracted from the chemical structure branch and the textual description branch to a joint space via contrastive learning.\n",
        "We initialize the encoders from both branches with the pretrained single-modal checkpoints, and perform contrastive pretraining on the dataset.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/xiaorandu/dl4h_project/main/img/pretraining.png\" width=1500 />\n",
        "Figure 1: MoleculeSTM Contrastive Pretraining (source: https://github.com/chao1224/MoleculeSTM)\n",
        "\n",
        "\\\n",
        "The contrastive learning strategy is adopted by using EBM-NCE and InfoNCE. They align the structure–text pairs for the same molecule and contrast the pairs for different molecules simultaneously. The objectives for EBM-NCE and InfoNCE are as follows:\n",
        "<img src=\"https://raw.githubusercontent.com/xiaorandu/dl4h_project/main/img/formula_1.png\" width=500 />\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyrC_nDe0S_2",
        "outputId": "3c320115-331d-4e1a-da62-a3ba6b120193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arguments\t Namespace(seed=42, device=0, dataspace_path='../data', dataset='PubChemSTM1K', text_type='SciBERT', molecule_type='Graph', batch_size=4, text_lr=0.0001, mol_lr=0.0001, text_lr_scale=0.1, mol_lr_scale=0.1, num_workers=8, epochs=5, decay=0, verbose=1, output_model_dir=None, max_seq_len=512, pretrain_gnn_mode='GraphMVP_G', gnn_emb_dim=300, num_layer=5, JK='last', dropout_ratio=0.5, gnn_type='gin', graph_pooling='mean', SSL_loss='EBM_NCE', SSL_emb_dim=256, CL_neg_samples=1, T=0.1, normalize=True)\n"
          ]
        }
      ],
      "source": [
        "# 1. Load and Customize Arguments\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"--seed\", type=int, default=42)\n",
        "parser.add_argument(\"--device\", type=int, default=0)\n",
        "\n",
        "parser.add_argument(\"--dataspace_path\", type=str, default=\"/content/drive/MyDrive/MoleculeSTM/data\")\n",
        "parser.add_argument(\"--dataset\", type=str, default=\"PubChemSTM1K\")\n",
        "parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\"])\n",
        "parser.add_argument(\"--molecule_type\", type=str, default=\"Graph\", choices=[\"Graph\"])\n",
        "\n",
        "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"--text_lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--mol_lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--text_lr_scale\", type=float, default=0.1)\n",
        "parser.add_argument(\"--mol_lr_scale\", type=float, default=0.1)\n",
        "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
        "parser.add_argument(\"--epochs\", type=int, default=5) #default=100\n",
        "parser.add_argument(\"--decay\", type=float, default=0)\n",
        "parser.add_argument(\"--verbose\", type=int, default=1)\n",
        "parser.add_argument(\"--output_model_dir\", type=str, default=None)\n",
        "\n",
        "########## for SciBERT ##########\n",
        "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
        "\n",
        "########## for 2D GNN ##########\n",
        "parser.add_argument(\"--pretrain_gnn_mode\", type=str, default=\"GraphMVP_G\", choices=[\"GraphMVP_G\"])\n",
        "parser.add_argument(\"--gnn_emb_dim\", type=int, default=300)\n",
        "parser.add_argument(\"--num_layer\", type=int, default=5)\n",
        "parser.add_argument('--JK', type=str, default='last')\n",
        "parser.add_argument(\"--dropout_ratio\", type=float, default=0.5)\n",
        "parser.add_argument(\"--gnn_type\", type=str, default=\"gin\")\n",
        "parser.add_argument('--graph_pooling', type=str, default='mean')\n",
        "\n",
        "########## for contrastive SSL ##########\n",
        "parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
        "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
        "parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
        "parser.add_argument(\"--T\", type=float, default=0.1)\n",
        "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
        "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
        "parser.set_defaults(normalize=True)\n",
        "\n",
        "args = parser.parse_args(\"\")\n",
        "print(\"arguments\\t\", args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SFK3rvB0qZ3"
      },
      "outputs": [],
      "source": [
        "# 2. Load Packages\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/MoleculeSTM')\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader as torch_DataLoader\n",
        "\n",
        "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "from MoleculeSTM.datasets import (\n",
        "    PubChemSTM_Datasets_Graph, PubChemSTM_SubDatasets_Graph,\n",
        "    PubChemSTM_Datasets_Raw_Graph, PubChemSTM_SubDatasets_Raw_Graph\n",
        ")\n",
        "from MoleculeSTM.models import GNN, GNN_graphpred\n",
        "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHeuyHx21ONN"
      },
      "outputs": [],
      "source": [
        "# 3. Supporting functions\n",
        "\n",
        "# create a cyclically shifted version of an array\n",
        "def cycle_index(num, shift):\n",
        "    arr = torch.arange(num) + shift\n",
        "    arr[-shift:] = torch.arange(shift)\n",
        "    return arr\n",
        "\n",
        "#  perform contrastive learning\n",
        "def do_CL(X, Y, args):\n",
        "    if args.normalize:\n",
        "        X = F.normalize(X, dim=-1)\n",
        "        Y = F.normalize(Y, dim=-1)\n",
        "\n",
        "    # Energy-Based Model with Noise-Contrastive Estimation\n",
        "    if args.SSL_loss == 'EBM_NCE':\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        # Generate negative samples for Y by cyclically shifting each sample\n",
        "        neg_Y = torch.cat([Y[cycle_index(len(Y), i + 1)] for i in range(args.CL_neg_samples)], dim=0)\n",
        "        # Repeat X to match the number of negative samples\n",
        "        neg_X = X.repeat((args.CL_neg_samples, 1))\n",
        "\n",
        "        # Compute positive and negative predictions and apply temperature scaling\n",
        "        pred_pos = torch.sum(X * Y, dim=1) / args.T\n",
        "        pred_neg = torch.sum(neg_X * neg_Y, dim=1) / args.T\n",
        "\n",
        "        # Compute loss for positive and negative predictions\n",
        "        loss_pos = criterion(pred_pos, torch.ones(len(pred_pos)).to(pred_pos.device))\n",
        "        loss_neg = criterion(pred_neg, torch.zeros(len(pred_neg)).to(pred_neg.device))\n",
        "\n",
        "        # Calculate the overall contrastive learning loss\n",
        "        CL_loss = (loss_pos + args.CL_neg_samples * loss_neg) / (1 + args.CL_neg_samples)\n",
        "\n",
        "        # Calculate the accuracy for positive and negative predictions\n",
        "        CL_acc = (torch.sum(pred_pos > 0).float() + torch.sum(pred_neg < 0).float()) / \\\n",
        "                 (len(pred_pos) + len(pred_neg))\n",
        "        CL_acc = CL_acc.detach().cpu().item()\n",
        "\n",
        "    # Information Noise-Contrastive Estimation\n",
        "    elif args.SSL_loss == 'InfoNCE':\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        B = X.size()[0]\n",
        "\n",
        "        # Compute the dot product between all pairs of X and Y and apply temperature scaling\n",
        "        logits = torch.mm(X, Y.transpose(1, 0))  # B*B\n",
        "        logits = torch.div(logits, args.T)\n",
        "        labels = torch.arange(B).long().to(logits.device)  # B*1\n",
        "\n",
        "        # Compute the loss using cross-entropy\n",
        "        CL_loss = criterion(logits, labels)\n",
        "\n",
        "        # Determine the predicted class and calculate accuracy\n",
        "        pred = logits.argmax(dim=1, keepdim=False)\n",
        "        CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
        "\n",
        "    else:\n",
        "        raise Exception\n",
        "\n",
        "    return CL_loss, CL_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO_W9fWJ1We5"
      },
      "outputs": [],
      "source": [
        "# 4. Training Function\n",
        "def train(\n",
        "    epoch,\n",
        "    dataloader,\n",
        "    text_model, text_tokenizer,\n",
        "    molecule_model, MegaMolBART_wrapper=None):\n",
        "\n",
        "    text_model.train()\n",
        "    molecule_model.train()\n",
        "    text2latent.train()\n",
        "    mol2latent.train()\n",
        "\n",
        "    if args.verbose:\n",
        "        L = tqdm(dataloader)\n",
        "    else:\n",
        "        L = dataloader\n",
        "\n",
        "    start_time = time.time()\n",
        "    accum_loss, accum_acc = 0, 0\n",
        "    for step, batch in enumerate(L):\n",
        "        description = batch[0]\n",
        "        molecule_data = batch[1]\n",
        "\n",
        "        description_tokens_ids, description_masks = prepare_text_tokens(\n",
        "            device=device, description=description, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
        "        description_output = text_model(input_ids=description_tokens_ids, attention_mask=description_masks)\n",
        "        description_repr = description_output[\"pooler_output\"]\n",
        "        description_repr = text2latent(description_repr)\n",
        "\n",
        "        molecule_data = molecule_data.to(device)\n",
        "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
        "            molecule_data, mol2latent=mol2latent,\n",
        "            molecule_type=molecule_type, molecule_model=molecule_model)\n",
        "\n",
        "        loss_01, acc_01 = do_CL(description_repr, molecule_repr, args)\n",
        "        loss_02, acc_02 = do_CL(molecule_repr, description_repr, args)\n",
        "        loss = (loss_01 + loss_02) / 2\n",
        "        acc = (acc_01 + acc_02) / 2\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accum_loss += loss.item()\n",
        "        accum_acc += acc\n",
        "\n",
        "    accum_loss /= len(L)\n",
        "    accum_acc /= len(L)\n",
        "\n",
        "    global optimal_loss\n",
        "    temp_loss = accum_loss\n",
        "    if temp_loss < optimal_loss:\n",
        "        optimal_loss = temp_loss\n",
        "    print(\"CL Loss: {:.5f}\\tCL Acc: {:.5f}\\tTime: {:.5f}\".format(accum_loss, accum_acc, time.time() - start_time))\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5Nht4Qc1gKe"
      },
      "outputs": [],
      "source": [
        "# 5. Pretraining\n",
        "# 5.1 set seed\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
        "    if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(args.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAXcWeUF1nzp",
        "outputId": "d63a8412-8e28-42b9-9be8-bac128ba85c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download SciBert to ../data/pretrained_SciBERT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# 5.2 prepare text model\n",
        "kwargs = {}\n",
        "\n",
        "if args.text_type == \"SciBERT\":\n",
        "    pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
        "    print(\"Download SciBert to {}\".format(pretrained_SciBERT_folder))\n",
        "    text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
        "    text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
        "    kwargs[\"text_tokenizer\"] = text_tokenizer\n",
        "    kwargs[\"text_model\"] = text_model\n",
        "    text_dim = 768\n",
        "else:\n",
        "    raise Exception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeTT8yyc1v2P",
        "outputId": "99f77afd-6ced-4f19-97e6-9c5d851ac305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(text_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DoXzEc611YS",
        "outputId": "938467c9-344c-4d1a-f588-426b3fe14279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading from ../data/pretrained_GraphMVP/GraphMVP_G/model.pth ...\n"
          ]
        }
      ],
      "source": [
        "# 5.3 starting training MoleculeSTM-Graph\n",
        "\n",
        "# prepare GraphMVP (Graph Model) and Dataset\n",
        "dataset_root = os.path.join(args.dataspace_path, \"PubChemSTM_data\")\n",
        "\n",
        "molecule_type = \"Graph\"\n",
        "\n",
        "# PubChemSTM_Datasets_Graph(dataset_root)\n",
        "dataset = PubChemSTM_SubDatasets_Graph(dataset_root, size=100) #size = 1000\n",
        "\n",
        "dataloader_class = pyg_DataLoader\n",
        "\n",
        "molecule_node_model = GNN(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim,\n",
        "    JK=args.JK, drop_ratio=args.dropout_ratio,\n",
        "    gnn_type=args.gnn_type)\n",
        "molecule_model = GNN_graphpred(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim, JK=args.JK, graph_pooling=args.graph_pooling,\n",
        "    num_tasks=1, molecule_node_model=molecule_node_model)\n",
        "pretrained_model_path = os.path.join(args.dataspace_path, \"pretrained_GraphMVP\", args.pretrain_gnn_mode, \"model.pth\")\n",
        "molecule_model.from_pretrained(pretrained_model_path)\n",
        "\n",
        "molecule_model = molecule_model.to(device)\n",
        "\n",
        "kwargs[\"molecule_model\"] = molecule_model\n",
        "molecule_dim = args.gnn_emb_dim\n",
        "\n",
        "dataloader = dataloader_class(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg9A16LL22Cb",
        "outputId": "cfbc968e-1a3d-4789-e48e-22dc9cbf5319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset size is: 100, sample data shows below:\n",
            "29927686 Scutellarin(1-)\n",
            "29982675 9-cis-4-oxoretinoate\n",
            "29918871 Monacolin J carboxylate\n",
            "29986894 11-cis-retinoate\n",
            "29919282 (R)-imazamox(1-)\n",
            "29919280 (S)-imazamox(1-)\n",
            "29986450 6-(O-phosphocholine)oxyhexanoate(1-)\n",
            "29918994 Tenofovir(1-)\n",
            "29922751 Cidofovir(1-)\n",
            "29986451 6-(O-phosphocholine)oxyhexanoic acid\n",
            "29986850 Cis-resveratrol 3-O-glucuronide\n",
            "29969962 Abacavir 5'-glucuronide\n",
            "29919281 (S)-imazamox\n",
            "29922189 (2S)-2-methylbutanoic acid [(1S,7S,8S,8aR)-8-[2-[(2R,4S)-4-hydroxy-2-oxanyl]ethyl]-7-methyl-1,2,3,7,8,8a-hexahydronaphthalen-1-yl] ester\n",
            "29981063 Homovanillic acid sulfate\n",
            "29919046 N-Desmethylzolmitriptan\n",
            "29921593 Beta-D-Gal-(1->4)-alpha-D-Man\n",
            "29976540 Cyanomethyl 2,3,4,6-tetra-O-acetyl-1-thio-alpha-D-mannoside\n",
            "29918831 6-hydroxyetodolac\n",
            "29976188 (3S)-3-(4-chlorophenyl)-4-hydroxybutanoic acid\n"
          ]
        }
      ],
      "source": [
        "print(f\"dataset size is: {dataset.size}, sample data shows below:\")\n",
        "for i in range(20):\n",
        "  print(dataset.CID_list[i], dataset.text_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_qGz95j2J78",
        "outputId": "a3178696-5cd2-483c-f21b-f93172969776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GNN_graphpred(\n",
            "  (molecule_node_model): GNN(\n",
            "    (atom_encoder): AtomEncoder(\n",
            "      (atom_embedding_list): ModuleList(\n",
            "        (0): Embedding(119, 300)\n",
            "        (1): Embedding(4, 300)\n",
            "        (2-3): 2 x Embedding(12, 300)\n",
            "        (4): Embedding(10, 300)\n",
            "        (5-6): 2 x Embedding(6, 300)\n",
            "        (7-8): 2 x Embedding(2, 300)\n",
            "      )\n",
            "    )\n",
            "    (gnns): ModuleList(\n",
            "      (0-4): 5 x GINConv()\n",
            "    )\n",
            "    (batch_norms): ModuleList(\n",
            "      (0-4): 5 x BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(molecule_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g4LRboq2ZRT",
        "outputId": "670bfcb0-63aa-446c-99be-f237939f7ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text2latent: Linear(in_features=768, out_features=256, bias=True)\n",
            "mol2latent: Linear(in_features=300, out_features=256, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# prepare two project layers\n",
        "text2latent = nn.Linear(text_dim, args.SSL_emb_dim).to(device)\n",
        "mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim).to(device)\n",
        "print(f\"text2latent: {text2latent}\")\n",
        "print(f\"mol2latent: {mol2latent}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB2kgPvr2kwm",
        "outputId": "b174a1a1-0b9a-47c9-93d0-d5d6cc6e689c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            "\n",
            "Parameter Group 2\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 1e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            "\n",
            "Parameter Group 3\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 1e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# prepare optimizers\n",
        "model_param_group = [\n",
        "    {\"params\": text_model.parameters(), \"lr\": args.text_lr},\n",
        "    {\"params\": molecule_model.parameters(), \"lr\": args.mol_lr},\n",
        "    {\"params\": text2latent.parameters(), \"lr\": args.text_lr * args.text_lr_scale},\n",
        "    {\"params\": mol2latent.parameters(), \"lr\": args.mol_lr * args.mol_lr_scale},\n",
        "]\n",
        "optimizer = optim.Adam(model_param_group, weight_decay=args.decay)\n",
        "optimal_loss = 1e10\n",
        "print(optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahej76Bp2svn"
      },
      "outputs": [],
      "source": [
        "# start training\n",
        "for e in range(3):\n",
        "    print(\"Epoch {}\".format(e))\n",
        "    train(e, dataloader, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdIvSq2i7ExF"
      },
      "source": [
        "### **Computational requirements**\n",
        "\n",
        "\n",
        "*   Hardware: Google Colab V100 GPU, RAM 16GB\n",
        "*   Seeds: 42\n",
        "*   Training epochs: 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCXoJGaGcjlD"
      },
      "source": [
        "### **Implementation code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6VHPwwvlnjn"
      },
      "source": [
        "#### **1 Downstream: Zero-shot Structure-text Retrieval**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PW3BqCIGjbv"
      },
      "outputs": [],
      "source": [
        "# 1. Load Packages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/MoleculeSTM')\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader as torch_DataLoader\n",
        "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from MoleculeSTM.datasets import DrugBank_Datasets_SMILES_retrieval, DrugBank_Datasets_Graph_retrieval\n",
        "from MoleculeSTM.models import GNN, GNN_graphpred\n",
        "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network\n",
        "\n",
        "# Set-up the environment variable to ignore warnings\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC6TQ3fzGz4f",
        "outputId": "a5b1d164-8771-45e7-b03c-6cb4e1ba7332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arguments\t Namespace(seed=42, device=0, SSL_emb_dim=256, text_type='SciBERT', load_latent_projector=1, training_mode='zero_shot', dataspace_path='../data', dataset='PubChem', task='molecule_description', test_mode='given_text', T_list=[4, 10, 20], batch_size=32, num_workers=8, epochs=1, text_lr=1e-05, mol_lr=1e-05, text_lr_scale=0.1, mol_lr_scale=0.1, decay=0, SSL_loss='EBM_NCE', CL_neg_samples=1, T=0.1, normalize=True, max_seq_len=512, molecule_type='Graph', gnn_emb_dim=300, num_layer=5, JK='last', dropout_ratio=0.5, gnn_type='gin', graph_pooling='mean', eval_train=0, verbose=0, input_model_dir='demo/demo_checkpoints_Graph', input_model_path='demo/demo_checkpoints_Graph/molecule_model.pth')\n"
          ]
        }
      ],
      "source": [
        "# 2. Setup Arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--seed\", type=int, default=42)\n",
        "parser.add_argument(\"--device\", type=int, default=0)\n",
        "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
        "parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\", \"BioBERT\"])\n",
        "parser.add_argument(\"--load_latent_projector\", type=int, default=1)\n",
        "parser.add_argument(\"--training_mode\", type=str, default=\"zero_shot\", choices=[\"zero_shot\"])\n",
        "\n",
        "########## for dataset and split ##########\n",
        "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
        "parser.add_argument(\"--dataset\", type=str, default=\"PubChem\")\n",
        "parser.add_argument(\"--task\", type=str, default=\"molecule_description\",\n",
        "    choices=[\n",
        "        \"molecule_description\", \"molecule_description_Raw\",\n",
        "        \"molecule_description_removed_PubChem\", \"molecule_description_removed_PubChem_Raw\",\n",
        "        \"molecule_pharmacodynamics\", \"molecule_pharmacodynamics_Raw\",\n",
        "        \"molecule_pharmacodynamics_removed_PubChem\", \"molecule_pharmacodynamics_removed_PubChem_Raw\"])\n",
        "parser.add_argument(\"--test_mode\", type=str, default=\"given_text\", choices=[\"given_text\", \"given_molecule\"])\n",
        "\n",
        "########## for optimization ##########\n",
        "parser.add_argument(\"--T_list\", type=int, nargs=\"+\", default=[4, 10, 20])\n",
        "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
        "parser.add_argument(\"--epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--text_lr\", type=float, default=1e-5)\n",
        "parser.add_argument(\"--mol_lr\", type=float, default=1e-5)\n",
        "parser.add_argument(\"--text_lr_scale\", type=float, default=0.1)\n",
        "parser.add_argument(\"--mol_lr_scale\", type=float, default=0.1)\n",
        "parser.add_argument(\"--decay\", type=float, default=0)\n",
        "\n",
        "########## for contrastive objective ##########\n",
        "parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
        "parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
        "parser.add_argument(\"--T\", type=float, default=0.1)\n",
        "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
        "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
        "parser.set_defaults(normalize=True)\n",
        "\n",
        "########## for BERT model ##########\n",
        "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
        "\n",
        "########## for molecule model ##########\n",
        "parser.add_argument(\"--molecule_type\", type=str, default=\"Graph\", choices=[\"SMILES\", \"Graph\"])\n",
        "\n",
        "########## for 2D GNN ##########\n",
        "parser.add_argument(\"--gnn_emb_dim\", type=int, default=300)\n",
        "parser.add_argument(\"--num_layer\", type=int, default=5)\n",
        "parser.add_argument('--JK', type=str, default='last')\n",
        "parser.add_argument(\"--dropout_ratio\", type=float, default=0.5)\n",
        "parser.add_argument(\"--gnn_type\", type=str, default=\"gin\")\n",
        "parser.add_argument('--graph_pooling', type=str, default='mean')\n",
        "\n",
        "########## for saver ##########\n",
        "parser.add_argument(\"--eval_train\", type=int, default=0)\n",
        "parser.add_argument(\"--verbose\", type=int, default=0)\n",
        "\n",
        "parser.add_argument(\"--input_model_dir\", type=str, default=\"demo/demo_checkpoints_Graph\")\n",
        "parser.add_argument(\"--input_model_path\", type=str, default=\"demo/demo_checkpoints_Graph/molecule_model.pth\")\n",
        "\n",
        "\n",
        "args = parser.parse_args(\"\")\n",
        "print(\"arguments\\t\", args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2WqGnbBG6IF"
      },
      "outputs": [],
      "source": [
        "# 3. Setup Seed\n",
        "np.random.seed(args.seed)\n",
        "torch.random.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
        "    if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDt4xeEwG8Ji",
        "outputId": "b3da9dfc-f5f2-4947-ce43-88d849151b06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading from /content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/text_model.pth...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 224,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4. Load SciBERT\n",
        "pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
        "text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
        "text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
        "text_dim = 768\n",
        "\n",
        "# input_model_path = os.path.join(args.input_model_dir, \"text_model.pth\")\n",
        "input_model_path = \"/content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/text_model.pth\"\n",
        "print(\"Loading from {}...\".format(input_model_path))\n",
        "state_dict = torch.load(input_model_path, map_location='cpu')\n",
        "text_model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wVqX-XB8YJx",
        "outputId": "831c20cd-9a10-400d-92d4-304afd5be9ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(text_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cabfBuQI8ljm",
        "outputId": "ac409efb-2328-496b-f2a8-29c14b5499c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading from /content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/molecule_model.pth...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 293,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 5. Load MoleculeSTM-Graph\n",
        "molecule_node_model = GNN(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim,\n",
        "    JK=args.JK, drop_ratio=args.dropout_ratio,\n",
        "    gnn_type=args.gnn_type)\n",
        "molecule_model = GNN_graphpred(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim, JK=args.JK, graph_pooling=args.graph_pooling,\n",
        "    num_tasks=1, molecule_node_model=molecule_node_model)\n",
        "molecule_dim = args.gnn_emb_dim\n",
        "\n",
        "input_model_path = \"/content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/molecule_model.pth\"\n",
        "print(\"Loading from {}...\".format(input_model_path))\n",
        "state_dict = torch.load(input_model_path, map_location='cpu')\n",
        "molecule_model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4d9aymB9OS-",
        "outputId": "7013bbfc-9d60-4982-a96e-2d1e6dd5ebda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GNN_graphpred(\n",
            "  (molecule_node_model): GNN(\n",
            "    (atom_encoder): AtomEncoder(\n",
            "      (atom_embedding_list): ModuleList(\n",
            "        (0): Embedding(119, 300)\n",
            "        (1): Embedding(4, 300)\n",
            "        (2-3): 2 x Embedding(12, 300)\n",
            "        (4): Embedding(10, 300)\n",
            "        (5-6): 2 x Embedding(6, 300)\n",
            "        (7-8): 2 x Embedding(2, 300)\n",
            "      )\n",
            "    )\n",
            "    (gnns): ModuleList(\n",
            "      (0-4): 5 x GINConv()\n",
            "    )\n",
            "    (batch_norms): ModuleList(\n",
            "      (0-4): 5 x BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(molecule_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N-Y-eF0H9qm",
        "outputId": "5fd7b3d7-e737-44df-eb76-0a624f074bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading from /content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/text2latent_model.pth...\n",
            "Loading from /content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/mol2latent_model.pth...\n",
            "text2latent: Linear(in_features=768, out_features=256, bias=True)\n",
            "mol2latent: Linear(in_features=300, out_features=256, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# 6. Load Projection Layers\n",
        "text2latent = nn.Linear(text_dim, args.SSL_emb_dim)\n",
        "input_model_path = \"/content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/text2latent_model.pth\"\n",
        "print(\"Loading from {}...\".format(input_model_path))\n",
        "state_dict = torch.load(input_model_path, map_location='cpu')\n",
        "text2latent.load_state_dict(state_dict)\n",
        "\n",
        "mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim)\n",
        "input_model_path = \"/content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/mol2latent_model.pth\"\n",
        "print(\"Loading from {}...\".format(input_model_path))\n",
        "state_dict = torch.load(input_model_path, map_location='cpu')\n",
        "mol2latent.load_state_dict(state_dict)\n",
        "\n",
        "print(f\"text2latent: {text2latent}\")\n",
        "print(f\"mol2latent: {mol2latent}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr69ai8aINGY"
      },
      "outputs": [],
      "source": [
        "# 7. support functions\n",
        "def cycle_index(num, shift):\n",
        "    arr = torch.arange(num) + shift\n",
        "    arr[-shift:] = torch.arange(shift)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def do_CL_eval(X, Y, neg_Y, args):\n",
        "    X = F.normalize(X, dim=-1)\n",
        "    X = X.unsqueeze(1) # B, 1, d\n",
        "\n",
        "    Y = Y.unsqueeze(0)\n",
        "    Y = torch.cat([Y, neg_Y], dim=0) # T, B, d\n",
        "    Y = Y.transpose(0, 1)  # B, T, d\n",
        "    Y = F.normalize(Y, dim=-1)\n",
        "\n",
        "    logits = torch.bmm(X, Y.transpose(1, 2)).squeeze()  # B*T\n",
        "    B = X.size()[0]\n",
        "    labels = torch.zeros(B).long().to(logits.device)  # B*1\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    CL_loss = criterion(logits, labels)\n",
        "    pred = logits.argmax(dim=1, keepdim=False)\n",
        "    confidence = logits\n",
        "    CL_conf = confidence.max(dim=1)[0]\n",
        "    CL_conf = CL_conf.cpu().numpy()\n",
        "\n",
        "    CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
        "    return CL_loss, CL_conf, CL_acc\n",
        "\n",
        "\n",
        "def get_text_repr(text):\n",
        "    text_tokens_ids, text_masks = prepare_text_tokens(\n",
        "        device=device, description=text, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
        "    text_output = text_model(input_ids=text_tokens_ids, attention_mask=text_masks)\n",
        "    text_repr = text_output[\"pooler_output\"]\n",
        "    text_repr = text2latent(text_repr)\n",
        "    return text_repr\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(dataloader):\n",
        "    text_model.eval()\n",
        "    molecule_model.eval()\n",
        "    text2latent.eval()\n",
        "    mol2latent.eval()\n",
        "\n",
        "    accum_acc_list = [0 for _ in args.T_list]\n",
        "    if args.verbose:\n",
        "        L = tqdm(dataloader)\n",
        "    else:\n",
        "        L = dataloader\n",
        "    for batch in L:\n",
        "        text = batch[0]\n",
        "        molecule_data = batch[1]\n",
        "        neg_text = batch[2]\n",
        "        neg_molecule_data = batch[3]\n",
        "\n",
        "        text_repr = get_text_repr(text)\n",
        "\n",
        "        molecule_data = molecule_data.to(device)\n",
        "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
        "            molecule_data, mol2latent=mol2latent,\n",
        "            molecule_type=\"Graph\", molecule_model=molecule_model)\n",
        "\n",
        "        if test_mode == \"given_text\":\n",
        "            neg_molecule_repr = [\n",
        "                get_molecule_repr_MoleculeSTM(\n",
        "                    neg_molecule_data[idx].to(device), mol2latent=mol2latent,\n",
        "                    molecule_type=\"Graph\", molecule_model=molecule_model) for idx in range(T_max)\n",
        "            ]\n",
        "            neg_molecule_repr = torch.stack(neg_molecule_repr)\n",
        "            for T_idx, T in enumerate(args.T_list):\n",
        "                _, _, acc = do_CL_eval(text_repr, molecule_repr, neg_molecule_repr[:T-1], args)\n",
        "                accum_acc_list[T_idx] += acc\n",
        "        elif test_mode == \"given_molecule\":\n",
        "            neg_text_repr = [get_text_repr(neg_text[idx]) for idx in range(T_max)]\n",
        "            neg_text_repr = torch.stack(neg_text_repr)\n",
        "            for T_idx, T in enumerate(args.T_list):\n",
        "                _, _, acc = do_CL_eval(molecule_repr, text_repr, neg_text_repr[:T-1], args)\n",
        "                accum_acc_list[T_idx] += acc\n",
        "        else:\n",
        "            raise Exception\n",
        "\n",
        "    accum_acc_list = np.array(accum_acc_list)\n",
        "    accum_acc_list /= len(dataloader)\n",
        "    return accum_acc_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rrh8VHaIQRe"
      },
      "outputs": [],
      "source": [
        "# 8. text retrival\n",
        "text_model = text_model.to(device)\n",
        "molecule_model = molecule_model.to(device)\n",
        "text2latent = text2latent.to(device)\n",
        "mol2latent = mol2latent.to(device)\n",
        "\n",
        "T_max = max(args.T_list) - 1\n",
        "\n",
        "initial_test_acc_list = []\n",
        "test_mode = args.test_mode\n",
        "dataset_folder = os.path.join(args.dataspace_path, \"DrugBank_data\")\n",
        "\n",
        "\n",
        "dataset_class = DrugBank_Datasets_Graph_retrieval\n",
        "dataloader_class = pyg_DataLoader\n",
        "processed_dir_prefix = args.task\n",
        "\n",
        "if args.task == \"molecule_description\":\n",
        "    template = \"SMILES_description_{}.txt\"\n",
        "elif args.task == \"molecule_description_removed_PubChem\":\n",
        "    template = \"SMILES_description_removed_from_PubChem_{}.txt\"\n",
        "elif args.task == \"molecule_description_Raw\":\n",
        "    template = \"SMILES_description_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_description_removed_PubChem_Raw\":\n",
        "    template = \"SMILES_description_removed_from_PubChem_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics\":\n",
        "    template = \"SMILES_pharmacodynamics_{}.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_removed_PubChem\":\n",
        "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_Raw\":\n",
        "    template = \"SMILES_pharmacodynamics_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_removed_PubChem_Raw\":\n",
        "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}_Raw.txt\"\n",
        "\n",
        "full_dataset = dataset_class(dataset_folder, 'full', neg_sample_size=T_max, processed_dir_prefix=processed_dir_prefix, template=template)\n",
        "full_dataloader = dataloader_class(full_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers) # The program will get blcoked with none-zero num_workers\n",
        "\n",
        "initial_test_acc_list = eval_epoch(full_dataloader)\n",
        "print('Results', initial_test_acc_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co3EqwE5908C"
      },
      "source": [
        "```\n",
        "Data: Data(x=[40309, 2], edge_index=[2, 85886], edge_attr=[85886, 2], id=[1168])\n",
        "Index(['text', 'smiles'], dtype='object')\n",
        "Loading negative samples from ../data/DrugBank_data/index/SMILES_description_full.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJHi_2OQ86QJ"
      },
      "source": [
        "#### **2 Downstream: Zero-shot Text-based Molecule Editing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvYcCjyL8-jr"
      },
      "outputs": [],
      "source": [
        "# For graphs\n",
        "! python downstream_02_molecule_edit_step_01_MoleculeSTM_Space_Alignment.py \\\n",
        "    --MoleculeSTM_molecule_type=Graph \\\n",
        "    --MoleculeSTM_model_dir=../data/demo/demo_checkpoints_Graph\n",
        "\n",
        "\n",
        "! python downstream_02_molecule_edit_step_02_MoleculeSTM_Latent_Optimization.py \\\n",
        "    --MoleculeSTM_molecule_type=Graph \\\n",
        "    --MoleculeSTM_model_dir=../data/demo/demo_checkpoints_Graph \\\n",
        "    --language_edit_model_dir=../data/demo/demo_checkpoints_Graph \\\n",
        "    --input_description_id=101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLQYp0R_8-98"
      },
      "source": [
        "#### **3 Downstream: Molecular Property Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_Sb4KGzLRIo"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/MoleculeSTM')\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader as torch_DataLoader\n",
        "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
        "\n",
        "from MoleculeSTM.datasets import MoleculeNetSMILESDataset, MoleculeNetGraphDataset\n",
        "from MoleculeSTM.splitters import scaffold_split\n",
        "from MoleculeSTM.utils import get_num_task_and_type, get_molecule_repr_MoleculeSTM\n",
        "from MoleculeSTM.models import GNN, GNN_graphpred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq64aO1nLZUp",
        "outputId": "2970fe45-8e84-49f1-b726-4b50543e7cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arguments\t Namespace(seed=42, device=0, training_mode='fine_tuning', molecule_type='Graph', dataspace_path='../data', dataset='bace', split='scaffold', batch_size=32, lr=0.0001, lr_scale=1, num_workers=1, epochs=5, weight_decay=0, schedule='cycle', warm_up_steps=10, gnn_emb_dim=300, num_layer=5, JK='last', dropout_ratio=0.5, gnn_type='gin', graph_pooling='mean', eval_train=0, verbose=1, input_model_path='/content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/molecule_model.pth', output_model_dir=None)\n"
          ]
        }
      ],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--seed\", type=int, default=42)\n",
        "parser.add_argument(\"--device\", type=int, default=0)\n",
        "parser.add_argument(\"--training_mode\", type=str, default=\"fine_tuning\", choices=[\"fine_tuning\", \"linear_probing\"])\n",
        "parser.add_argument(\"--molecule_type\", type=str, default=\"Graph\", choices=[\"SMILES\", \"Graph\"])\n",
        "\n",
        "########## for dataset and split ##########\n",
        "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
        "parser.add_argument(\"--dataset\", type=str, default=\"bace\")\n",
        "parser.add_argument(\"--split\", type=str, default=\"scaffold\")\n",
        "\n",
        "########## for optimization ##########\n",
        "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--lr_scale\", type=float, default=1)\n",
        "parser.add_argument(\"--num_workers\", type=int, default=1)\n",
        "parser.add_argument(\"--epochs\", type=int, default=5)\n",
        "parser.add_argument(\"--weight_decay\", type=float, default=0)\n",
        "parser.add_argument(\"--schedule\", type=str, default=\"cycle\")\n",
        "parser.add_argument(\"--warm_up_steps\", type=int, default=10)\n",
        "\n",
        "########## for 2D GNN ##########\n",
        "parser.add_argument(\"--gnn_emb_dim\", type=int, default=300)\n",
        "parser.add_argument(\"--num_layer\", type=int, default=5)\n",
        "parser.add_argument('--JK', type=str, default='last')\n",
        "parser.add_argument(\"--dropout_ratio\", type=float, default=0.5)\n",
        "parser.add_argument(\"--gnn_type\", type=str, default=\"gin\")\n",
        "parser.add_argument('--graph_pooling', type=str, default='mean')\n",
        "\n",
        "########## for saver ##########\n",
        "parser.add_argument(\"--eval_train\", type=int, default=0)\n",
        "parser.add_argument(\"--verbose\", type=int, default=1)\n",
        "\n",
        "parser.add_argument(\"--input_model_path\", type=str, default=\"/content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/molecule_model.pth\")\n",
        "parser.add_argument(\"--output_model_dir\", type=str, default=None)\n",
        "\n",
        "args = parser.parse_args(\"\")\n",
        "print(\"arguments\\t\", args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pY_raYALgDK"
      },
      "outputs": [],
      "source": [
        "# setup seed\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
        "    if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3gFLdSiLuZA"
      },
      "outputs": [],
      "source": [
        "num_tasks, task_mode = get_num_task_and_type(args.dataset)\n",
        "dataset_folder = os.path.join(args.dataspace_path, \"MoleculeNet_data\", args.dataset)\n",
        "\n",
        "dataset = MoleculeNetGraphDataset(dataset_folder, args.dataset)\n",
        "dataloader_class = pyg_DataLoader\n",
        "use_pyg_dataset = True\n",
        "\n",
        "smiles_list = pd.read_csv(\n",
        "    dataset_folder + \"/processed/smiles.csv\", header=None)[0].tolist()\n",
        "train_dataset, valid_dataset, test_dataset = scaffold_split(\n",
        "    dataset, smiles_list, null_value=0, frac_train=0.8,\n",
        "    frac_valid=0.1, frac_test=0.1, pyg_dataset=use_pyg_dataset)\n",
        "\n",
        "\n",
        "train_loader = dataloader_class(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
        "val_loader = dataloader_class(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
        "test_loader = dataloader_class(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPX1VkzGSCWQ",
        "outputId": "ef2a286d-ce44-4f48-baf7-5ab87698a812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Name: bace\n",
            "Number of Graphs: 1210\n",
            "Transformations: None\n",
            "Pre-transformations: None\n",
            "Pre-filters: None\n",
            "Processed Path: ['../data/MoleculeNet_data/bace/processed/geometric_data_processed.pt']\n",
            "Raw Files: ['bace.csv']\n"
          ]
        }
      ],
      "source": [
        "def print_dataset_info(dataset):\n",
        "    print(f\"Dataset Name: {dataset.dataset}\")\n",
        "    print(f\"Number of Graphs: {len(dataset)}\")\n",
        "    print(f\"Transformations: {dataset.transform}\")\n",
        "    print(f\"Pre-transformations: {dataset.pre_transform}\")\n",
        "    print(f\"Pre-filters: {dataset.pre_filter}\")\n",
        "    print(f\"Processed Path: {dataset.processed_paths}\")\n",
        "    print(f\"Raw Files: {dataset.raw_file_names}\")\n",
        "\n",
        "print_dataset_info(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74jdnrFoL33M",
        "outputId": "fbe3e6ab-e062-4609-ec4b-44cb93e4d013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start from pretrained model (MoleculeSTM) in /content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/molecule_model.pth.\n"
          ]
        }
      ],
      "source": [
        "molecule_node_model = GNN(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim,\n",
        "    JK=args.JK, drop_ratio=args.dropout_ratio,\n",
        "    gnn_type=args.gnn_type)\n",
        "model = GNN_graphpred(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim, JK=args.JK, graph_pooling=args.graph_pooling,\n",
        "    num_tasks=1, molecule_node_model=molecule_node_model)\n",
        "molecule_dim = args.gnn_emb_dim\n",
        "\n",
        "if \"GraphMVP\" in args.input_model_path:\n",
        "    print(\"Start from pretrained model (GraphMVP) in {}.\".format(args.input_model_path))\n",
        "    model.from_pretrained(args.input_model_path)\n",
        "else:\n",
        "    print(\"Start from pretrained model (MoleculeSTM) in {}.\".format(args.input_model_path))\n",
        "    state_dict = torch.load(args.input_model_path, map_location='cpu')\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "linear_model = nn.Linear(molecule_dim, num_tasks).to(device)\n",
        "\n",
        "# Rewrite the seed by MegaMolBART\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(args.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51WwsG0dPqcs",
        "outputId": "b81f0b5e-f50f-4a81-d4be-7e5b78562b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model: GNN_graphpred(\n",
            "  (molecule_node_model): GNN(\n",
            "    (atom_encoder): AtomEncoder(\n",
            "      (atom_embedding_list): ModuleList(\n",
            "        (0): Embedding(119, 300)\n",
            "        (1): Embedding(4, 300)\n",
            "        (2-3): 2 x Embedding(12, 300)\n",
            "        (4): Embedding(10, 300)\n",
            "        (5-6): 2 x Embedding(6, 300)\n",
            "        (7-8): 2 x Embedding(2, 300)\n",
            "      )\n",
            "    )\n",
            "    (gnns): ModuleList(\n",
            "      (0-4): 5 x GINConv()\n",
            "    )\n",
            "    (batch_norms): ModuleList(\n",
            "      (0-4): 5 x BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)\n",
            ")\n",
            "linear model: Linear(in_features=300, out_features=1, bias=True)\n"
          ]
        }
      ],
      "source": [
        "print(f\"model: {model}\")\n",
        "print(f\"linear model: {linear_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWcVSZWXMhGX"
      },
      "outputs": [],
      "source": [
        "if args.training_mode == \"fine_tuning\":\n",
        "    model_param_group = [\n",
        "        {\"params\": model.parameters()},\n",
        "        {\"params\": linear_model.parameters(), 'lr': args.lr * args.lr_scale}\n",
        "    ]\n",
        "else:\n",
        "    model_param_group = [\n",
        "        {\"params\": linear_model.parameters(), 'lr': args.lr * args.lr_scale}\n",
        "    ]\n",
        "optimizer = optim.Adam(model_param_group, lr=args.lr, weight_decay=args.weight_decay)\n",
        "print(optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUiaWzhjMj8P"
      },
      "outputs": [],
      "source": [
        "def train_classification(model, device, loader, optimizer):\n",
        "    if args.training_mode == \"fine_tuning\":\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    linear_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    if args.verbose:\n",
        "        L = tqdm(loader)\n",
        "    else:\n",
        "        L = loader\n",
        "    for step, batch in enumerate(L):\n",
        "        batch = batch.to(device)\n",
        "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
        "            batch, mol2latent=None,\n",
        "            molecule_type=\"Graph\", molecule_model=model)\n",
        "        pred = linear_model(molecule_repr)\n",
        "        pred = pred.float()\n",
        "        y = batch.y.view(pred.shape).to(device).float()\n",
        "\n",
        "        is_valid = y ** 2 > 0\n",
        "        loss_mat = criterion(pred, (y + 1) / 2)\n",
        "        loss_mat = torch.where(\n",
        "            is_valid, loss_mat,\n",
        "            torch.zeros(loss_mat.shape).to(device).to(loss_mat.dtype))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.sum(loss_mat) / torch.sum(is_valid)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.detach().item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_classification(model, device, loader):\n",
        "    model.eval()\n",
        "    linear_model.eval()\n",
        "    y_true, y_scores = [], []\n",
        "\n",
        "    if args.verbose:\n",
        "        L = tqdm(loader)\n",
        "    else:\n",
        "        L = loader\n",
        "    for step, batch in enumerate(L):\n",
        "        batch = batch.to(device)\n",
        "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
        "            batch, mol2latent=None,\n",
        "            molecule_type=\"Graph\", molecule_model=model)\n",
        "        pred = linear_model(molecule_repr)\n",
        "        pred = pred.float()\n",
        "        y = batch.y.view(pred.shape).to(device).float()\n",
        "\n",
        "        y_true.append(y)\n",
        "        y_scores.append(pred)\n",
        "\n",
        "    y_true = torch.cat(y_true, dim=0).cpu().numpy()\n",
        "    y_scores = torch.cat(y_scores, dim=0).cpu().numpy()\n",
        "\n",
        "    roc_list = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        # AUC is only defined when there is at least one positive data.\n",
        "        if np.sum(y_true[:, i] == 1) > 0 and np.sum(y_true[:, i] == -1) > 0:\n",
        "            is_valid = y_true[:, i] ** 2 > 0\n",
        "            roc_list.append(roc_auc_score((y_true[is_valid, i] + 1) / 2, y_scores[is_valid, i]))\n",
        "        else:\n",
        "            print(\"{} is invalid\".format(i))\n",
        "\n",
        "    if len(roc_list) < y_true.shape[1]:\n",
        "        print(len(roc_list))\n",
        "        print(\"Some target is missing!\")\n",
        "        print(\"Missing ratio: %f\" %(1 - float(len(roc_list)) / y_true.shape[1]))\n",
        "\n",
        "    return sum(roc_list) / len(roc_list), 0, y_true, y_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bg3p-SCMnJ1"
      },
      "outputs": [],
      "source": [
        "train_func = train_classification\n",
        "eval_func = eval_classification\n",
        "\n",
        "train_roc_list, val_roc_list, test_roc_list = [], [], []\n",
        "train_acc_list, val_acc_list, test_acc_list = [], [], []\n",
        "best_val_roc, best_val_idx = -1, 0\n",
        "criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    loss_acc = train_func(model, device, train_loader, optimizer)\n",
        "    print(\"Epoch: {}\\nLoss: {}\".format(epoch, loss_acc))\n",
        "\n",
        "    if args.eval_train:\n",
        "        train_roc, train_acc, train_target, train_pred = eval_func(model, device, train_loader)\n",
        "    else:\n",
        "        train_roc = train_acc = 0\n",
        "    val_roc, val_acc, val_target, val_pred = eval_func(model, device, val_loader)\n",
        "    test_roc, test_acc, test_target, test_pred = eval_func(model, device, test_loader)\n",
        "\n",
        "    train_roc_list.append(train_roc)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_roc_list.append(val_roc)\n",
        "    val_acc_list.append(val_acc)\n",
        "    test_roc_list.append(test_roc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(\"train: {:.6f}\\tval: {:.6f}\\ttest: {:.6f}\".format(train_roc, val_roc, test_roc))\n",
        "    print()\n",
        "\n",
        "print(\"best train: {:.6f}\\tval: {:.6f}\\ttest: {:.6f}\".format(train_roc_list[best_val_idx], val_roc_list[best_val_idx], test_roc_list[best_val_idx]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Git LFS initialized.\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# WARNING: these files are huge\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/chao1224/MoleculeSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH_TO_MOLECULE_MODEL = \"pretrainedModels/molecule_model.pth\"\n",
        "PATH_TO_TEXT_MODEL = \"pretrainedModels/text_model.pth\"\n",
        "PATH_TO_TEXT2LATENT_MODEL = \"pretrainedModels/text2latent_model.pth\"\n",
        "PATH_TO_MOL2LATENT_MODEL = \"pretrainedModels/mol2latent_model.pth\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    molecule_model_loaded = torch.load(PATH_TO_MOLECULE_MODEL)\n",
        "    text_model_loaded = torch.load(PATH_TO_TEXT_MODEL)\n",
        "    text2latent_loaded = torch.load(PATH_TO_TEXT2LATENT_MODEL)\n",
        "    mol2latent_loaded = torch.load(PATH_TO_MOL2LATENT_MODEL)\n",
        "else:\n",
        "    molecule_model_loaded = torch.load(PATH_TO_MOLECULE_MODEL, map_location=torch.device('cpu'))\n",
        "    text_model_loaded = torch.load(PATH_TO_TEXT_MODEL, map_location=torch.device('cpu'))\n",
        "    text2latent_loaded = torch.load(PATH_TO_TEXT2LATENT_MODEL, map_location=torch.device('cpu'))\n",
        "    mol2latent_loaded = torch.load(PATH_TO_MOL2LATENT_MODEL, map_location=torch.device('cpu'))\n",
        "\n",
        "molecule_model = GNN_graphpred()\n",
        "molecule_model.load_state_dict(molecule_model_loaded)\n",
        "# TODO: add other models here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D61YLwnIfgJz"
      },
      "source": [
        "##   Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS7jMVtNfjWS"
      },
      "source": [
        "### Metric descriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D0aAK0l9202"
      },
      "source": [
        "This evaluation uses several different metrics:\n",
        "\n",
        "\n",
        "*   Contrastive loss - measuring the the performance of the model in correctly identifying true matches from a set of possible matches\n",
        "*   Accuracy - determines the proportion of correctly labeled predictive matches\n",
        "*   Confidence Scores - determines a measure of confidence for each prediction\n",
        "\n",
        "\n",
        "Additionally, the code uses a variety of negative samples to help measure the model's resistence to varying conditions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts2-ejAyfn1g"
      },
      "source": [
        "### Implementation code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Gj_Wp88r65"
      },
      "source": [
        "#### Support Functions and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5ZFsNDA6fqP0"
      },
      "outputs": [],
      "source": [
        "# from demo downstream retrieval Graph\n",
        "def cycle_index(num, shift):\n",
        "    arr = torch.arange(num) + shift\n",
        "    arr[-shift:] = torch.arange(shift)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def do_CL_eval(X, Y, neg_Y, args):\n",
        "    X = F.normalize(X, dim=-1)\n",
        "    X = X.unsqueeze(1) # B, 1, d\n",
        "\n",
        "    Y = Y.unsqueeze(0)\n",
        "    Y = torch.cat([Y, neg_Y], dim=0) # T, B, d\n",
        "    Y = Y.transpose(0, 1)  # B, T, d\n",
        "    Y = F.normalize(Y, dim=-1)\n",
        "\n",
        "    logits = torch.bmm(X, Y.transpose(1, 2)).squeeze()  # B*T\n",
        "    B = X.size()[0]\n",
        "    labels = torch.zeros(B).long().to(logits.device)  # B*1\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    CL_loss = criterion(logits, labels)\n",
        "    pred = logits.argmax(dim=1, keepdim=False)\n",
        "    confidence = logits\n",
        "    CL_conf = confidence.max(dim=1)[0]\n",
        "    CL_conf = CL_conf.cpu().numpy()\n",
        "\n",
        "    CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
        "    return CL_loss, CL_conf, CL_acc\n",
        "\n",
        "\n",
        "def get_text_repr(text):\n",
        "    text_tokens_ids, text_masks = prepare_text_tokens(\n",
        "        device=device, description=text, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
        "    text_output = text_model(input_ids=text_tokens_ids, attention_mask=text_masks)\n",
        "    text_repr = text_output[\"pooler_output\"]\n",
        "    text_repr = text2latent(text_repr)\n",
        "    return text_repr\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(dataloader):\n",
        "    text_model.eval()\n",
        "    molecule_model.eval()\n",
        "    text2latent.eval()\n",
        "    mol2latent.eval()\n",
        "\n",
        "    accum_acc_list = [0 for _ in args.T_list]\n",
        "    if args.verbose:\n",
        "        L = tqdm(dataloader)\n",
        "    else:\n",
        "        L = dataloader\n",
        "    for batch in L:\n",
        "        text = batch[0]\n",
        "        molecule_data = batch[1]\n",
        "        neg_text = batch[2]\n",
        "        neg_molecule_data = batch[3]\n",
        "\n",
        "        text_repr = get_text_repr(text)\n",
        "\n",
        "        molecule_data = molecule_data.to(device)\n",
        "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
        "            molecule_data, mol2latent=mol2latent,\n",
        "            molecule_type=\"Graph\", molecule_model=molecule_model)\n",
        "\n",
        "        if test_mode == \"given_text\":\n",
        "            neg_molecule_repr = [\n",
        "                get_molecule_repr_MoleculeSTM(\n",
        "                    neg_molecule_data[idx].to(device), mol2latent=mol2latent,\n",
        "                    molecule_type=\"Graph\", molecule_model=molecule_model) for idx in range(T_max)\n",
        "            ]\n",
        "            neg_molecule_repr = torch.stack(neg_molecule_repr)\n",
        "            for T_idx, T in enumerate(args.T_list):\n",
        "                _, _, acc = do_CL_eval(text_repr, molecule_repr, neg_molecule_repr[:T-1], args)\n",
        "                accum_acc_list[T_idx] += acc\n",
        "        elif test_mode == \"given_molecule\":\n",
        "            neg_text_repr = [get_text_repr(neg_text[idx]) for idx in range(T_max)]\n",
        "            neg_text_repr = torch.stack(neg_text_repr)\n",
        "            for T_idx, T in enumerate(args.T_list):\n",
        "                _, _, acc = do_CL_eval(molecule_repr, text_repr, neg_text_repr[:T-1], args)\n",
        "                accum_acc_list[T_idx] += acc\n",
        "        else:\n",
        "            raise Exception\n",
        "\n",
        "    accum_acc_list = np.array(accum_acc_list)\n",
        "    accum_acc_list /= len(dataloader)\n",
        "    return accum_acc_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8LWvzss9KX5"
      },
      "source": [
        "#### Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "pI4qY8gi9I1A"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'collections.OrderedDict' object has no attribute 'to'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from demo downstream retrieval Graph\u001b[39;00m\n\u001b[0;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m text_model \u001b[38;5;241m=\u001b[39m \u001b[43mtext_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[0;32m      4\u001b[0m molecule_model \u001b[38;5;241m=\u001b[39m molecule_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m text2latent \u001b[38;5;241m=\u001b[39m text2latent\u001b[38;5;241m.\u001b[39mto(device)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'to'"
          ]
        }
      ],
      "source": [
        "# from demo downstream retrieval Graph\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "text_model = text_model.to(device)\n",
        "molecule_model = molecule_model.to(device)\n",
        "text2latent = text2latent.to(device)\n",
        "mol2latent = mol2latent.to(device)\n",
        "\n",
        "T_max = max(args.T_list) - 1\n",
        "\n",
        "initial_test_acc_list = []\n",
        "test_mode = args.test_mode\n",
        "dataset_folder = os.path.join(args.dataspace_path, \"DrugBank_data\")\n",
        "\n",
        "\n",
        "dataset_class = DrugBank_Datasets_Graph_retrieval\n",
        "dataloader_class = pyg_DataLoader\n",
        "processed_dir_prefix = args.task\n",
        "\n",
        "if args.task == \"molecule_description\":\n",
        "    template = \"SMILES_description_{}.txt\"\n",
        "elif args.task == \"molecule_description_removed_PubChem\":\n",
        "    template = \"SMILES_description_removed_from_PubChem_{}.txt\"\n",
        "elif args.task == \"molecule_description_Raw\":\n",
        "    template = \"SMILES_description_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_description_removed_PubChem_Raw\":\n",
        "    template = \"SMILES_description_removed_from_PubChem_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics\":\n",
        "    template = \"SMILES_pharmacodynamics_{}.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_removed_PubChem\":\n",
        "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_Raw\":\n",
        "    template = \"SMILES_pharmacodynamics_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_removed_PubChem_Raw\":\n",
        "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}_Raw.txt\"\n",
        "\n",
        "full_dataset = dataset_class(dataset_folder, 'full', neg_sample_size=T_max, processed_dir_prefix=processed_dir_prefix, template=template)\n",
        "full_dataloader = dataloader_class(full_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers) # The program will get blcoked with none-zero num_workers\n",
        "\n",
        "initial_test_acc_list = eval_epoch(full_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMHv_2C5f6wV"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNQWmk3P7gse"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problems & Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Problem: We had great difficulty setting up the environment. Specifically, much of the instructions in the README were outdated and did not work as intended. The authors of this paper did not share the versioning of many of their python dependedencies.\n",
        "\n",
        "Solution: Despite testing countless different package versions and environments, we were unable to solve this problem. In an effort to continue with our hypothesis we decided to remove some dependencies (and its related code) from the project\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Problem: After adjusting the code for our modified environment, we ultimately found ourselves unable to run the training for the model.\n",
        "\n",
        "Solution: First, we tried to debug this error by looking into the code ourselves. Unfortunately, we were not able to determine the solution for this issue. To attempt to try and resolve this we opened an issue on Git to hopefully get some resolution from the author of this repository. Fortunately, we were able to find a pretrained model from the author on hugging face to allow us to continue with our hypothesis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4j7hn1WK6WMR",
        "2NbPHUTMbkD3",
        "4ErFv9PosM5R",
        "JJHi_2OQ86QJ",
        "hLQYp0R_8-98",
        "D61YLwnIfgJz",
        "F1Gj_Wp88r65",
        "Y8LWvzss9KX5"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
