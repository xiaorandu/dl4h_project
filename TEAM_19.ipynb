{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "4j7hn1WK6WMR",
        "2NbPHUTMbkD3",
        "4ErFv9PosM5R",
        "JJHi_2OQ86QJ",
        "hLQYp0R_8-98",
        "D61YLwnIfgJz",
        "F1Gj_Wp88r65",
        "Y8LWvzss9KX5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<!-- # Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary. -->"
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "**Github: https://github.com/xiaorandu/dl4h_project**\n",
        "<br>**Background of the problem**\n",
        "  * **type of problem:** Artificial intelligence (AI) is being used to help aid drug discovery; however, many of these processes focus on the studies of chemical structures and largely ignoring the plethora of information found in text-based instructions. This limitation hinders the advancement of textual descriptions for drug design, molecule editing, and predicting complex biological activities.\n",
        "\n",
        "  * **importance/meaning of solving the problem:** Solving this problem will help enable faster iterations of drug discovery, such as re-purposing and multi-objective lead optimization.\n",
        "\n",
        "  * **the difficulty of the problem:** This problem contains many difficult aspects including the zero shot learning tasks (which are especially difficult in the context of bio chemistry) and understanding natural language. Data insufficiency (PubChemSTM consists of 250,000 molecules and 281,000 structure-text pairs vs. 400 million in the vision-language domain used by peers from other domains) is another limitation, and the expressiveness of chemical structure models is also a bottleneck of this work.\n",
        "\n",
        "  * **the state of the art methods and effectiveness:** A multi modal model was designed that incorporates both molecular structural information and textual knowledge. A multi modal model, MoleculeSTM, which consists of two brances, the chemical structure branch (to handle molecules' internal structure)  and textual description branch (to handle external domain knowledge) was designed. Such design enables the model to be integrated with existing models trained on each seperately , i.e., molecular structural models and scientific language models. A large multi-modal structure-text dataset was created to align the two branhes of MoleculeSTM. Two challenging downstream tasks were desinged, the structure retrieval task and text based molecule editing task and petrained MoleculeSTM was applied on them in a zero-shot manner. By studing these tasks two main attributes of MoleculeSTM were summaried, open vocabulary and composibilty. Open vocabulary means the model can support exploring a wide range of biochemical concepts with unbound vocabulary. Compositionality means complex concepts can be expressed by decomposing it into several simpler concepts. Results had shown the effectiveness of MoleculeSTM can reach the best performance on six zero-shot retrival tasks, which is up to 50% higher accuracy and twenty zero-shot text-based editing tasks, which is up to 40% higher hit ratio when comparing with the stsate-of-the-art methods. Additionally, MoleculeSTM was able to detect critical structure inferred by text descriptions for molecular editing tasks.\n",
        "\n",
        "<br>**Paper explanation**\n",
        "  * **what did the paper propose:** The paper introduced MoleculeSTM, a model that integrates chemical structures of molecules with their textual descriptions using a contrastive learning approach. The model aims to perform tasks such as structure-text retrieval and molecule editing based on text instructions in a zero-shot setting. It utilizes a vast, multi-modal dataset, PubChemSTM, containing over 280,000 chemical structure-text pairs.\n",
        "\n",
        "  * **innovations of the method:** MoleculeSTM uniquely combines chemical structural data with textual information, enhancing the model's understanding and generalization capabilities across various biochemical contexts. It demonstrates significant effectiveness in zero-shot scenarios, where the model performs tasks without having been explicitly trained on them. The model supports open-ended vocabularies and can decompose complex instructions into simpler concepts, making it versatile in handling diverse and novel scientific queries.\n",
        "\n",
        "  * **how well the proposed method work (in its own metrics):** MoleculeSTM significantly outperformed existing methods in zero-shot retrieval and text-based molecule editing tasks. It achieved up to 50% higher accuracy in retrieval tasks and up to 40% higher hit ratios in editing tasks compared to state-of-the-art methods. This indicates a robust ability to generalize and effectively apply learned knowledge to new and unseen data.\n",
        "\n",
        "  * **what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem):** The research incorporated textual descriptions with chemical structures for molecule representation learning.The multi-modal model, MoleculeSTM, consistently showed improved performance when compared to the existing models. MoleculeSTM might accelerate various downstream drug discovery practices, since it was observed that the model can successfully modify molecule substructures to gain desired properties and also retrieve novel drug-target relations. This paper is important as it was able to illustrate the effictiveness of incorporating textual descriptions in addition to chemical structures for molecule representation learning. It did have two limitations,data insufficiency and expressiveness of the chemical structure models.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper we will test and the corresponding experiments you will run.\n",
        "\n",
        "Hypothesis:\n",
        "1. MoleculeSTM achieves state-of-the-art performance in zero-shot structure-text retrieval and molecule editing tasks compared to the existing method.\n",
        "2. Incorporating textual descriptions through contrastive learning will significantly improve the model's performance on zero-shot retrieval and molecule editing tasks.\n",
        "\n",
        "Experiments:\n",
        "Retraining the model on the constructed PubChemSTM dataset, consisting of over 280,000 chemical structure-text pairs, and evaluating it on specified zero-shot tasks: structure-text retrieval and molecule editing.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Environment"
      ],
      "metadata": {
        "id": "4j7hn1WK6WMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python version\n"
      ],
      "metadata": {
        "id": "zLF7R1Kv6r05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "t2WqmBOX64Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TIfhWJ9ZQqYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python 3.10.12"
      ],
      "metadata": {
        "id": "Lt2nzVvzhZfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependencies/packages needed"
      ],
      "metadata": {
        "id": "fIHuhq7A6myH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installed packages\n",
        "!pip install rdkit\n",
        "!pip install torch torchvision\n",
        "!pip install requests tqdm matplotlib spacy Levenshtein boto3 deepspeed\n",
        "!pip install ogb==1.2.0\n",
        "!pip install transformers==4.30.2\n",
        "\n",
        "!pip install torch_geometric\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
        "!pip install git+https://github.com/MolecularAI/pysmilesutils.git"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "Zd2xQJO8NTmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install packaging\n",
        "!pip install \"jedi>=0.16\"\n",
        "!pip install \"cxxfilt>=0.2.0\"\n",
        "!pip install \"PyYAML>=5.1\""
      ],
      "metadata": {
        "id": "BRL9wMqqHWBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "ETR79LAPGk4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pkg_resources\n",
        "\n",
        "# List of packages from apex requirements.txt\n",
        "required_packages = {\n",
        "    \"cxxfilt\": \"0.2.0\",\n",
        "    \"tqdm\": \"4.28.1\",\n",
        "    \"numpy\": \"1.15.3\",\n",
        "    \"PyYAML\": \"5.1\",\n",
        "    \"pytest\": \"3.5.1\",\n",
        "    \"packaging\": \"14.0\"\n",
        "}\n",
        "\n",
        "def check_packages(packages):\n",
        "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
        "    for package, version in packages.items():\n",
        "        current_version = pkg_resources.get_distribution(package).version\n",
        "        if current_version and (pkg_resources.parse_version(current_version) >= pkg_resources.parse_version(version)):\n",
        "            print(f\"{package} >= {version} is installed (Current version: {current_version})\")\n",
        "        else:\n",
        "            missing_version = version if not current_version else f\"{current_version} (Required: {version})\"\n",
        "            print(f\"{package} >= {version} is NOT installed. Current/Required version: {missing_version}\")\n",
        "\n",
        "check_packages(required_packages)"
      ],
      "metadata": {
        "id": "cSFPUp47H8LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install apex\n",
        "!git clone https://github.com/chao1224/apex.git\n",
        "%cd apex\n",
        "!pip install -v --disable-pip-version-check --no-cache-dir ./\n",
        "%cd .."
      ],
      "metadata": {
        "id": "syK8SSjkhS2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install metagron\n",
        "# TODO: delete me, lets focus on graph\n",
        "!git clone https://github.com/MolecularAI/MolBART.git --branch megatron-molbart-with-zinc\n",
        "%cd MolBART/megatron_molbart/Megatron-LM-v1.1.5-3D_parallelism\n",
        "!pip install .\n",
        "%cd ../../.."
      ],
      "metadata": {
        "id": "7bwItJOQwJXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "<!-- Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset. -->"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Data download instruction\n",
        "We can use the following python script to download the pretraining dataset and downstream datasets."
      ],
      "metadata": {
        "id": "XzVUQS0CHry0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, snapshot_download\n",
        "api = HfApi()\n",
        "snapshot_download(repo_id=\"chao1224/MoleculeSTM\", repo_type=\"dataset\", local_dir='data')"
      ],
      "metadata": {
        "id": "2qrB1KAwzX5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data folder can be found at [google drive link](https://drive.google.com/drive/u/0/folders/1pCr0WrY-3lbxxy44D68u2cDzmDONVLBD).\n",
        "<br/>The data folder will include:\n",
        "```\n",
        "data\n",
        "├── PubChemSTM_data/\n",
        "│   └── raw\n",
        "│        └── CID2SMILES.csv\n",
        "│        └── CID2name.json\n",
        "│        └── CID2name_raw.json\n",
        "│        └── molecules.sdf\n",
        "│   └── processed/\n",
        "├── pretrained_SciBERT/\n",
        "├── pretrained_MegaMolBART/\n",
        "├── pretrained_KV-PLM/\n",
        "├── pretrained_GraphMVP/\n",
        "├── pretrained_MoleculeSTM_Raw/\n",
        "├── pretrained_MoleculeSTM/\n",
        "├── DrugBank_data/\n",
        "├── ZINC250K_data/\n",
        "├── Editing_data/\n",
        "│   └── single_multi_property_SMILES.txt\n",
        "│   └── neighbor2drug/\n",
        "│   └── ChEMBL_data/\n",
        "└── MoleculeNet_data/\n",
        "```"
      ],
      "metadata": {
        "id": "Oo7bQ8AwzeSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data descriptions with helpful charts and visualizations"
      ],
      "metadata": {
        "id": "hANVl1yo7R42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MoleculeSTM/data/PubChemSTM_data/raw"
      ],
      "metadata": {
        "id": "Ioy_Wg6j77jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The SMILES string views the molecule as a sequence\n",
        "'''\n",
        "import pandas as pd\n",
        "CID2SMILES = 'CID2SMILES.csv'\n",
        "df_CID2SMILES = pd.read_csv(CID2SMILES, usecols=['CID', 'SMILES'])\n",
        "df_CID2SMILES.head()"
      ],
      "metadata": {
        "id": "jlSN1l2H8JEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "index\tCID\tSMILES\n",
        "0\t     1\tCC(=O)OC(CC(=O)[O-])C[N+](C)(C)C\n",
        "1\t     3\tO=C(O)C1=CC=CC(O)C1O\n",
        "2\t     4\tCC(O)CN\n",
        "3\t     5\tNCC(=O)COP(=O)(O)O\n",
        "4\t     6\tO=[N+]([O-])c1ccc(Cl)c([N+](=O)[O-])c1\n",
        "```"
      ],
      "metadata": {
        "id": "Xbw3AlHBgEAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "CID2name_raw = \"CID2name_raw.json\"\n",
        "with open(CID2name_raw, 'r') as file:\n",
        "  data = json.load(file)\n",
        "\n",
        "df_CID2name_raw = pd.DataFrame(list(data.items()), columns=['CID', 'Names'])\n",
        "df_CID2name_raw.head()"
      ],
      "metadata": {
        "id": "RSB49QE4NsS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "index\tCID\tNames\n",
        "0\t    180\tAcetone,ACETONE,Acetone\n",
        "1\t    222\tAmmonia,AMMONIA SOLUTIONS (CONTAINING MORE THAN 35% BUT NOT MORE THAN 50% AMMONIA),AMMONIA, ANHYDROUS,AMMONIA, SOLUTION, WITH MORE THAN 10% BUT NOT MORE THAN 35% AMMONIA,Ammonia\n",
        "2\t   5359596\tArsenic,ARSENIC,Arsenic,Arsenic atom,Arsenic Compounds\n",
        "3\t    241\tBenzene,BENZENE,Benzene,Benzene\n",
        "4\t   23973\tCadmium,CADMIUM,Cadmium atom,Cadmium,Cadmium Compounds\n",
        "```"
      ],
      "metadata": {
        "id": "PuBWGhV-gWho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "CID2name = \"CID2name.json\"\n",
        "with open(CID2name, 'r') as file:\n",
        "  data = json.load(file)\n",
        "\n",
        "df_CID2name = pd.DataFrame(list(data.items()), columns=['CID', 'Names'])\n",
        "df_CID2name.head()"
      ],
      "metadata": {
        "id": "Im2SnRTR9Umy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "index\tCID\tNames\n",
        "0\t    180\tAcetone,Acetone,Acetone,Acetone,Acetone\n",
        "1\t    222\tAmmonia,Ammonia solutions (containing more than 35% but not more than 50% ammonia),Ammonia, anhydrous,Ammonia, solution, with more than 10% but not more than 35% ammonia,Ammonia,Ammonia,Ammonia,Ammonia\n",
        "2\t   5359596\tArsenic,Arsenic,Arsenic,Arsenic atom,Arsenic, a naturally occurring element,,Arsenic\n",
        "3\t    241\tBenzene,Benzene,Benzene,Benzene,Benzene,Benzene\n",
        "4\t   23973\tCadmium,Cadmium,Cadmium atom,The main sources of cadmium in the air,Cadmium,Cadmium,Cadmium\n",
        "```"
      ],
      "metadata": {
        "id": "hh3sN5gAghSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import PandasTools\n",
        "\n",
        "sdf_file = \"molecules.sdf\"\n",
        "df_molecules = PandasTools.LoadSDF(sdf_file)\n",
        "print(df_molecules.head())"
      ],
      "metadata": {
        "id": "b8rVWwCcCgJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# the output would be like this.\n",
        "  PUBCHEM_COMPOUND_CID PUBCHEM_COMPOUND_CANONICALIZED  \\\n",
        "0             29500027                              1   \n",
        "1             29500038                              1   \n",
        "2             29500039                              1   \n",
        "3             29500070                              1   \n",
        "4             29500073                              1   \n",
        "\n",
        "  PUBCHEM_CACTVS_COMPLEXITY PUBCHEM_CACTVS_HBOND_ACCEPTOR  \\\n",
        "0                       460                             7   \n",
        "1                       480                             6   \n",
        "2                       480                             6   \n",
        "3                       543                             6   \n",
        "4                       451                             6   \n",
        "\n",
        "  PUBCHEM_CACTVS_HBOND_DONOR PUBCHEM_CACTVS_ROTATABLE_BOND  \\\n",
        "0                          1                             4   \n",
        "1                          2                             6   \n",
        "2                          2                             6   \n",
        "3                          1                             4   \n",
        "4                          2                             3   \n",
        "\n",
        "                             PUBCHEM_CACTVS_SUBSKEYS  \\\n",
        "0  AAADccB7oABAAAAAAAAAAAAAAAAAAWLAAAA8QAAAAAAAAA...   \n",
        "1  AAADceB7sABgAAAAAAAAAAAAAAAAAWJAAAAsAAAAAAAAAA...   \n",
        "2  AAADceB7sABgAAAAAAAAAAAAAAAAAWJAAAAsAAAAAAAAAA...   \n",
        "3  AAADceB7oABAAAAAAAAAAAAAAAAAAWLAAAAwYAAAAAAAAA...   \n",
        "4  AAADceB7oABAAAAAAAAAAAAAAAAAAWAAAAA8QAAAAAAAAA...   \n",
        "\n",
        "                          PUBCHEM_IUPAC_OPENEYE_NAME  \\\n",
        "0  N-[4-(2-pyridyl)thiazol-2-yl]-4-(tetrazol-1-yl...   \n",
        "1  (3S)-3-acetamido-N-[4-(2-pyridyl)thiazol-2-yl]...   \n",
        "2  (3R)-3-acetamido-N-[4-(2-pyridyl)thiazol-2-yl]...   \n",
        "3  4-(tetrazol-1-yl)-N-[4-(2,4,5-trimethylphenyl)...   \n",
        "4  3-amino-N-[4-(2,4,5-trimethylphenyl)thiazol-2-...   \n",
        "\n",
        "                              PUBCHEM_IUPAC_CAS_NAME  \\\n",
        "0  N-[4-(2-pyridinyl)-2-thiazolyl]-4-(1-tetrazoly...   \n",
        "1  (3S)-3-acetamido-N-[4-(2-pyridinyl)-2-thiazoly...   \n",
        "2  (3R)-3-acetamido-N-[4-(2-pyridinyl)-2-thiazoly...   \n",
        "3  4-(1-tetrazolyl)-N-[4-(2,4,5-trimethylphenyl)-...   \n",
        "4  3-amino-N-[4-(2,4,5-trimethylphenyl)-2-thiazol...   \n",
        "\n",
        "                           PUBCHEM_IUPAC_NAME_MARKUP  ...  \\\n",
        "0  <I>N</I>-(4-pyridin-2-yl-1,3-thiazol-2-yl)-4-(...  ...   \n",
        "1  (3<I>S</I>)-3-acetamido-<I>N</I>-(4-pyridin-2-...  ...   \n",
        "2  (3<I>R</I>)-3-acetamido-<I>N</I>-(4-pyridin-2-...  ...   \n",
        "3  4-(tetrazol-1-yl)-<I>N</I>-[4-(2,4,5-trimethyl...  ...   \n",
        "4  3-amino-<I>N</I>-[4-(2,4,5-trimethylphenyl)-1,...  ...   \n",
        "\n",
        "  PUBCHEM_BOND_UDEF_STEREO_COUNT PUBCHEM_ISOTOPIC_ATOM_COUNT  \\\n",
        "0                              0                           0   \n",
        "1                              0                           0   \n",
        "2                              0                           0   \n",
        "3                              0                           0   \n",
        "4                              0                           0   \n",
        "\n",
        "  PUBCHEM_COMPONENT_COUNT PUBCHEM_CACTVS_TAUTO_COUNT PUBCHEM_COORDINATE_TYPE  \\\n",
        "0                       1                         -1               1\\n5\\n255   \n",
        "1                       1                         -1               1\\n5\\n255   \n",
        "2                       1                         -1               1\\n5\\n255   \n",
        "3                       1                         -1               1\\n5\\n255   \n",
        "4                       1                         -1               1\\n5\\n255   \n",
        "\n",
        "                             PUBCHEM_BONDANNOTATIONS        ID  \\\n",
        "0  1  18  8\\n1  20  8\\n10  12  8\\n10  13  8\\n11  ...  29500027   \n",
        "1  1  11  8\\n1  16  8\\n11  13  8\\n13  15  8\\n15  ...  29500038   \n",
        "2  1  11  8\\n1  16  8\\n11  13  8\\n13  15  8\\n15  ...  29500039   \n",
        "3  1  19  8\\n1  20  8\\n10  14  8\\n11  12  8\\n11  ...  29500070   \n",
        "4  1  18  8\\n1  19  8\\n10  11  8\\n10  12  8\\n11  ...  29500073   \n",
        "\n",
        "                                              ROMol PUBCHEM_XLOGP3  \\\n",
        "0  <rdkit.Chem.rdchem.Mol object at 0x7d30b3ca4cf0>            NaN   \n",
        "1  <rdkit.Chem.rdchem.Mol object at 0x7d30b3ca4c80>            NaN   \n",
        "2  <rdkit.Chem.rdchem.Mol object at 0x7d30b3ca5070>            NaN   \n",
        "3  <rdkit.Chem.rdchem.Mol object at 0x7d30b3ca51c0>            NaN   \n",
        "4  <rdkit.Chem.rdchem.Mol object at 0x7d30b3ca5310>            NaN   \n",
        "\n",
        "  PUBCHEM_REFERENCE_STANDARDIZATION  \n",
        "0                               NaN  \n",
        "1                               NaN  \n",
        "2                               NaN  \n",
        "3                               NaN  \n",
        "4                               NaN  \n",
        "```"
      ],
      "metadata": {
        "id": "0b_FWKgCMc8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing code + command"
      ],
      "metadata": {
        "id": "KfSQcMIk7lmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preprocessing code can be found at preprocessing/PubchemSTM folder."
      ],
      "metadata": {
        "id": "yv49mufsG2mO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model"
      ],
      "metadata": {
        "id": "MusqjT86m8En"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Citation to the original paper**\n",
        "```\n",
        "@article{liu2023moleculestm,\n",
        "    title={Multi-modal molecule structure-text model for text-based retrieval and editing},\n",
        "    author={Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Anima},\n",
        "    title={Multi-modal molecule structure--text model for text-based retrieval and editing},\n",
        "    journal={Nature Machine Intelligence},\n",
        "    year={2023},\n",
        "    month={Dec},\n",
        "    day={01},\n",
        "    volume={5},\n",
        "    number={12},\n",
        "    pages={1447-1457},\n",
        "    issn={2522-5839},\n",
        "    doi={10.1038/s42256-023-00759-6},\n",
        "    url={https://doi.org/10.1038/s42256-023-00759-6}\n",
        "}"
      ],
      "metadata": {
        "id": "gBy2DO_0rtkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Link to the original paper’s repo**\n",
        "https://github.com/chao1224/MoleculeSTM"
      ],
      "metadata": {
        "id": "ITlAYslsrxmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model descriptions**\n",
        "MoleculeSTM consists of two branches: the **chemical structure branch $x_c$** and the **textual description branch $x_t$**.\n",
        "\n",
        "*   The chemical structure branch illustrates the arrangement of atoms in a moleculem, and we specifically focus on its **two-dimensional molecular graph**, which takes the atoms and bonds as the nodes and edges, respectively.\n",
        "*   The textual description branch provides a high-level description of the molecule’s functionality.\n",
        "\n",
        "\n",
        "This paper applies several models for text-based retrival and editing tasks from multi-modal molecule structure-text model. We apply the models below in our project.\n",
        "\n",
        "*   **Molecule graph encoder $f_c$**: apply a graph neural network (GNN) encoder to get a latent vector as molecule representation. Speciafically, we take a pretrained graph isomorphism network using **GraphMVP** pretraining.\n",
        "\n",
        "  *   GraphMVP doing a multi-view pretraining between the two-dimensional topologies and three-dimensional geometries on 250,000 conformations from the Geometric Ensemble of Molecules (GEOM) dataset.\n",
        "    \n",
        "*   **Text encoder $f_t$**: adapt the pretrained **SciBERT32** languange model, which was pretrained on the textual data from the chemical and biological domain.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOoZE1DRnAmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation code**\n"
      ],
      "metadata": {
        "id": "yWmHSgGgrbky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Molecule graph encoder $f_c$\n",
        "*   `GINConv` and `GCNConv` classes:\n",
        "  *   These are convolutional layers used within the GNN, configured with `*emb_dim` and aggregation method.\n",
        "*   `GNN` class:\n",
        "  *   Constructs a sequence of GNN layers (`gnns`) based on `num_layer` and `gnn_type`.\n",
        "  *   Applies batch normalization (`batch_norms`) across layers.\n",
        "  *   Utilizes dropout as specified by `dropout_ratio`.\n",
        "  *   Incorporates the Jumping Knowledge (JK) network setup through `JK`.\n",
        "*   `GNN_graph` class:\n",
        "  *   Extends the GNN model to predict properties at the graph level.\n",
        "  *   Uses `graph_pooling` for reducing node features to graph features.\n",
        "  *   Integrates the entire node model (`molecule_node_model`) for processing graphs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ErFv9PosM5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import (MessagePassing, global_add_pool,\n",
        "                                global_max_pool, global_mean_pool)\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "from torch_geometric.utils import add_self_loops, softmax, degree\n",
        "from torch_scatter import scatter_add\n",
        "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class GINConv(MessagePassing):\n",
        "    def __init__(self, emb_dim, aggr=\"add\"):\n",
        "        '''\n",
        "            emb_dim (int): node embedding dimensionality\n",
        "        '''\n",
        "        super(GINConv, self).__init__(aggr=aggr)\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, emb_dim))\n",
        "        self.eps = torch.nn.Parameter(torch.Tensor([0]))\n",
        "\n",
        "        self.bond_encoder = BondEncoder(emb_dim = emb_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        edge_embedding = self.bond_encoder(edge_attr)\n",
        "        out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, edge_attr):\n",
        "        return F.relu(x_j + edge_attr)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "\n",
        "class GCNConv(MessagePassing):\n",
        "    def __init__(self, emb_dim, aggr=\"add\"):\n",
        "        super(GCNConv, self).__init__(aggr=aggr)\n",
        "\n",
        "        self.linear = torch.nn.Linear(emb_dim, emb_dim)\n",
        "        self.root_emb = torch.nn.Embedding(1, emb_dim)\n",
        "        self.bond_encoder = BondEncoder(emb_dim = emb_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        x = self.linear(x)\n",
        "        edge_embedding = self.bond_encoder(edge_attr)\n",
        "\n",
        "        row, col = edge_index\n",
        "\n",
        "        #edge_weight = torch.ones((edge_index.size(1), ), device=edge_index.device)\n",
        "        deg = degree(row, x.size(0), dtype = x.dtype) + 1\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        return self.propagate(edge_index, x=x, edge_attr = edge_embedding, norm=norm) + F.relu(x + self.root_emb.weight) * 1./deg.view(-1,1)\n",
        "\n",
        "    def message(self, x_j, edge_attr, norm):\n",
        "        return norm.view(-1, 1) * F.relu(x_j + edge_attr)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, num_layer, emb_dim, JK=\"last\", drop_ratio=0., gnn_type=\"gin\"):\n",
        "\n",
        "        if num_layer < 2:\n",
        "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
        "\n",
        "        super(GNN, self).__init__()\n",
        "        self.drop_ratio = drop_ratio\n",
        "        self.num_layer = num_layer\n",
        "        self.JK = JK\n",
        "\n",
        "        self.atom_encoder = AtomEncoder(emb_dim)\n",
        "\n",
        "        ###List of MLPs\n",
        "        self.gnns = nn.ModuleList()\n",
        "        for layer in range(num_layer):\n",
        "            if gnn_type == \"gin\":\n",
        "                self.gnns.append(GINConv(emb_dim, aggr=\"add\"))\n",
        "            elif gnn_type == \"gcn\":\n",
        "                self.gnns.append(GCNConv(emb_dim))\n",
        "\n",
        "        ###List of batchnorms\n",
        "        self.batch_norms = nn.ModuleList()\n",
        "        for layer in range(num_layer):\n",
        "            self.batch_norms.append(nn.BatchNorm1d(emb_dim))\n",
        "\n",
        "    def forward(self, *argv):\n",
        "        if len(argv) == 3:\n",
        "            x, edge_index, edge_attr = argv[0], argv[1], argv[2]\n",
        "        elif len(argv) == 1:\n",
        "            data = argv[0]\n",
        "            x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        else:\n",
        "            raise ValueError(\"unmatched number of arguments.\")\n",
        "\n",
        "        x = self.atom_encoder(x)\n",
        "\n",
        "        h_list = [x]\n",
        "        for layer in range(self.num_layer):\n",
        "            h = self.gnns[layer](h_list[layer], edge_index, edge_attr)\n",
        "            h = self.batch_norms[layer](h)\n",
        "            # h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
        "            if layer == self.num_layer - 1:\n",
        "                # remove relu for the last layer\n",
        "                h = F.dropout(h, self.drop_ratio, training=self.training)\n",
        "            else:\n",
        "                h = F.dropout(F.relu(h), self.drop_ratio, training=self.training)\n",
        "            h_list.append(h)\n",
        "\n",
        "        ### Different implementations of Jk-concat\n",
        "        if self.JK == \"concat\":\n",
        "            node_representation = torch.cat(h_list, dim=1)\n",
        "        elif self.JK == \"last\":\n",
        "            node_representation = h_list[-1]\n",
        "        elif self.JK == \"max\":\n",
        "            h_list = [h.unsqueeze_(0) for h in h_list]\n",
        "            node_representation = torch.max(torch.cat(h_list, dim=0), dim=0)[0]\n",
        "        elif self.JK == \"sum\":\n",
        "            h_list = [h.unsqueeze_(0) for h in h_list]\n",
        "            node_representation = torch.sum(torch.cat(h_list, dim=0), dim=0)[0]\n",
        "        else:\n",
        "            raise ValueError(\"not implemented.\")\n",
        "        return node_representation\n",
        "\n",
        "\n",
        "class GNN_graphpred(nn.Module):\n",
        "    \"\"\"\n",
        "    Extension of GIN to incorporate edge information by concatenation.\n",
        "\n",
        "    Args:\n",
        "        num_layer (int): the number of GNN layers\n",
        "        arg.emb_dim (int): dimensionality of embeddings\n",
        "        num_tasks (int): number of tasks in multi-task learning scenario\n",
        "        JK (str): last, concat, max or sum.\n",
        "        graph_pooling (str): sum, mean, max, attention, set2set\n",
        "\n",
        "    See https://arxiv.org/abs/1810.00826\n",
        "    JK-net: https://arxiv.org/abs/1806.03536 \"\"\"\n",
        "\n",
        "    def __init__(self, num_layer, emb_dim, num_tasks, JK, graph_pooling, molecule_node_model=None):\n",
        "        super(GNN_graphpred, self).__init__()\n",
        "\n",
        "        if num_layer < 2:\n",
        "            raise ValueError(\"# layers must > 1.\")\n",
        "\n",
        "        self.molecule_node_model = molecule_node_model\n",
        "        self.num_layer = num_layer\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_tasks = num_tasks\n",
        "        self.JK = JK\n",
        "\n",
        "        # Different kind of graph pooling\n",
        "        if graph_pooling == \"sum\":\n",
        "            self.pool = global_add_pool\n",
        "        elif graph_pooling == \"mean\":\n",
        "            self.pool = global_mean_pool\n",
        "        elif graph_pooling == \"max\":\n",
        "            self.pool = global_max_pool\n",
        "        else:\n",
        "            raise ValueError(\"Invalid graph pooling type.\")\n",
        "\n",
        "        # For graph-level binary classification\n",
        "        self.mult = 1\n",
        "\n",
        "        if self.JK == \"concat\":\n",
        "            self.graph_pred_linear = nn.Linear(self.mult * (self.num_layer + 1) * self.emb_dim,\n",
        "                                               self.num_tasks)\n",
        "        else:\n",
        "            self.graph_pred_linear = nn.Linear(self.mult * self.emb_dim, self.num_tasks)\n",
        "        return\n",
        "\n",
        "    def from_pretrained(self, model_file):\n",
        "        print(\"Loading from {} ...\".format(model_file))\n",
        "        state_dict = torch.load(model_file)\n",
        "        self.molecule_node_model.load_state_dict(state_dict)\n",
        "        return\n",
        "\n",
        "    def forward(self, *argv):\n",
        "        if len(argv) == 4:\n",
        "            x, edge_index, edge_attr, batch = argv[0], argv[1], argv[2], argv[3]\n",
        "        elif len(argv) == 1:\n",
        "            data = argv[0]\n",
        "            x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "        else:\n",
        "            raise ValueError(\"unmatched number of arguments.\")\n",
        "\n",
        "        node_representation = self.molecule_node_model(x, edge_index, edge_attr)\n",
        "        graph_representation = self.pool(node_representation, batch)\n",
        "        output = self.graph_pred_linear(graph_representation)\n",
        "        return graph_representation, output"
      ],
      "metadata": {
        "id": "R5fLdx2lsLY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For GraphMVP, check this [repo](https://github.com/chao1224/GraphMVP), and use thie checkpoints on this [link](https://drive.google.com/drive/u/1/folders/1uPsBiQF3bfeCAXSDd4JfyXiTh-qxYfu6).\n",
        "```\n",
        "pretrained_GraphMVP/\n",
        "├── GraphMVP_C\n",
        "│   └── model.pth\n",
        "└── GraphMVP_G\n",
        "    └── model.pth\n",
        "```"
      ],
      "metadata": {
        "id": "vHR25Y5Sv7nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text encoder $f_t$\n",
        "This can be done by calling the following from SciBERT:\n",
        "```\n",
        "SciBERT_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
        "SciBERT_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
        "```"
      ],
      "metadata": {
        "id": "DnYJzdhZxcKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pretrained model**\n",
        "In the pretrain phase, MoleculeSTM aims to map the representations extracted from the chemical structure branch and the textual description branch to a joint space via contrastive learning.\n",
        "We initialize the encoders from both branches with the pretrained single-modal checkpoints, and perform contrastive pretraining on the dataset.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/xiaorandu/dl4h_project/main/img/pretraining.png\" width=1500 />\n",
        "Figure 1: MoleculeSTM Contrastive Pretraining (source: https://github.com/chao1224/MoleculeSTM)\n",
        "\n",
        "\\\n",
        "The comtrastive learning strategy is adopted by using EBM-NCE and InfoNCE. They align the structure–text pairs for the same molecule and contrast the pairs for different molecules simultaneously. The objectives for EBM-NCE and InfoNCE are as follows:\n",
        "<img src=\"https://raw.githubusercontent.com/xiaorandu/dl4h_project/main/img/formula_1.png\" width=500 />\n"
      ],
      "metadata": {
        "id": "S8OX6fDexyrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and Customize Arguments\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"--seed\", type=int, default=42)\n",
        "parser.add_argument(\"--device\", type=int, default=0)\n",
        "\n",
        "parser.add_argument(\"--dataspace_path\", type=str, default=\"/content/drive/MyDrive/MoleculeSTM/data\")\n",
        "parser.add_argument(\"--dataset\", type=str, default=\"PubChemSTM1K\")\n",
        "parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\"])\n",
        "parser.add_argument(\"--molecule_type\", type=str, default=\"Graph\", choices=[\"Graph\"])\n",
        "\n",
        "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"--text_lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--mol_lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--text_lr_scale\", type=float, default=0.1)\n",
        "parser.add_argument(\"--mol_lr_scale\", type=float, default=0.1)\n",
        "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
        "parser.add_argument(\"--epochs\", type=int, default=5) #default=100\n",
        "parser.add_argument(\"--decay\", type=float, default=0)\n",
        "parser.add_argument(\"--verbose\", type=int, default=1)\n",
        "parser.add_argument(\"--output_model_dir\", type=str, default=None)\n",
        "\n",
        "########## for SciBERT ##########\n",
        "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
        "\n",
        "########## for 2D GNN ##########\n",
        "parser.add_argument(\"--pretrain_gnn_mode\", type=str, default=\"GraphMVP_G\", choices=[\"GraphMVP_G\"])\n",
        "parser.add_argument(\"--gnn_emb_dim\", type=int, default=300)\n",
        "parser.add_argument(\"--num_layer\", type=int, default=5)\n",
        "parser.add_argument('--JK', type=str, default='last')\n",
        "parser.add_argument(\"--dropout_ratio\", type=float, default=0.5)\n",
        "parser.add_argument(\"--gnn_type\", type=str, default=\"gin\")\n",
        "parser.add_argument('--graph_pooling', type=str, default='mean')\n",
        "\n",
        "########## for contrastive SSL ##########\n",
        "parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
        "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
        "parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
        "parser.add_argument(\"--T\", type=float, default=0.1)\n",
        "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
        "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
        "parser.set_defaults(normalize=True)\n",
        "\n",
        "args = parser.parse_args(\"\")\n",
        "print(\"arguments\\t\", args)"
      ],
      "metadata": {
        "id": "QyrC_nDe0S_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load Packages\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/MoleculeSTM')\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader as torch_DataLoader\n",
        "\n",
        "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "from MoleculeSTM.datasets import (\n",
        "    PubChemSTM_Datasets_Graph, PubChemSTM_SubDatasets_Graph,\n",
        "    PubChemSTM_Datasets_Raw_Graph, PubChemSTM_SubDatasets_Raw_Graph\n",
        ")\n",
        "from MoleculeSTM.models import GNN, GNN_graphpred\n",
        "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network"
      ],
      "metadata": {
        "id": "4SFK3rvB0qZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Supporting functions\n",
        "\n",
        "# create a cyclically shifted version of an array\n",
        "def cycle_index(num, shift):\n",
        "    arr = torch.arange(num) + shift\n",
        "    arr[-shift:] = torch.arange(shift)\n",
        "    return arr\n",
        "\n",
        "#  perform contrastive learning\n",
        "def do_CL(X, Y, args):\n",
        "    if args.normalize:\n",
        "        X = F.normalize(X, dim=-1)\n",
        "        Y = F.normalize(Y, dim=-1)\n",
        "\n",
        "    # Energy-Based Model with Noise-Contrastive Estimation\n",
        "    if args.SSL_loss == 'EBM_NCE':\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        # Generate negative samples for Y by cyclically shifting each sample\n",
        "        neg_Y = torch.cat([Y[cycle_index(len(Y), i + 1)] for i in range(args.CL_neg_samples)], dim=0)\n",
        "        # Repeat X to match the number of negative samples\n",
        "        neg_X = X.repeat((args.CL_neg_samples, 1))\n",
        "\n",
        "        # Compute positive and negative predictions and apply temperature scaling\n",
        "        pred_pos = torch.sum(X * Y, dim=1) / args.T\n",
        "        pred_neg = torch.sum(neg_X * neg_Y, dim=1) / args.T\n",
        "\n",
        "        # Compute loss for positive and negative predictions\n",
        "        loss_pos = criterion(pred_pos, torch.ones(len(pred_pos)).to(pred_pos.device))\n",
        "        loss_neg = criterion(pred_neg, torch.zeros(len(pred_neg)).to(pred_neg.device))\n",
        "\n",
        "        # Calculate the overall contrastive learning loss\n",
        "        CL_loss = (loss_pos + args.CL_neg_samples * loss_neg) / (1 + args.CL_neg_samples)\n",
        "\n",
        "        # Calculate the accuracy for positive and negative predictions\n",
        "        CL_acc = (torch.sum(pred_pos > 0).float() + torch.sum(pred_neg < 0).float()) / \\\n",
        "                 (len(pred_pos) + len(pred_neg))\n",
        "        CL_acc = CL_acc.detach().cpu().item()\n",
        "\n",
        "    # Information Noise-Contrastive Estimation\n",
        "    elif args.SSL_loss == 'InfoNCE':\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        B = X.size()[0]\n",
        "\n",
        "        # Compute the dot product between all pairs of X and Y and apply temperature scaling\n",
        "        logits = torch.mm(X, Y.transpose(1, 0))  # B*B\n",
        "        logits = torch.div(logits, args.T)\n",
        "        labels = torch.arange(B).long().to(logits.device)  # B*1\n",
        "\n",
        "        # Compute the loss using cross-entropy\n",
        "        CL_loss = criterion(logits, labels)\n",
        "\n",
        "        # Determine the predicted class and calculate accuracy\n",
        "        pred = logits.argmax(dim=1, keepdim=False)\n",
        "        CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
        "\n",
        "    else:\n",
        "        raise Exception\n",
        "\n",
        "    return CL_loss, CL_acc"
      ],
      "metadata": {
        "id": "VHeuyHx21ONN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Training Function\n",
        "def train(\n",
        "    epoch,\n",
        "    dataloader,\n",
        "    text_model, text_tokenizer,\n",
        "    molecule_model, MegaMolBART_wrapper=None):\n",
        "\n",
        "    text_model.train()\n",
        "    molecule_model.train()\n",
        "    text2latent.train()\n",
        "    mol2latent.train()\n",
        "\n",
        "    if args.verbose:\n",
        "        L = tqdm(dataloader)\n",
        "    else:\n",
        "        L = dataloader\n",
        "\n",
        "    start_time = time.time()\n",
        "    accum_loss, accum_acc = 0, 0\n",
        "    for step, batch in enumerate(L):\n",
        "        description = batch[0]\n",
        "        molecule_data = batch[1]\n",
        "\n",
        "        description_tokens_ids, description_masks = prepare_text_tokens(\n",
        "            device=device, description=description, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
        "        description_output = text_model(input_ids=description_tokens_ids, attention_mask=description_masks)\n",
        "        description_repr = description_output[\"pooler_output\"]\n",
        "        description_repr = text2latent(description_repr)\n",
        "\n",
        "        molecule_data = molecule_data.to(device)\n",
        "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
        "            molecule_data, mol2latent=mol2latent,\n",
        "            molecule_type=molecule_type, molecule_model=molecule_model)\n",
        "\n",
        "        loss_01, acc_01 = do_CL(description_repr, molecule_repr, args)\n",
        "        loss_02, acc_02 = do_CL(molecule_repr, description_repr, args)\n",
        "        loss = (loss_01 + loss_02) / 2\n",
        "        acc = (acc_01 + acc_02) / 2\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accum_loss += loss.item()\n",
        "        accum_acc += acc\n",
        "\n",
        "    accum_loss /= len(L)\n",
        "    accum_acc /= len(L)\n",
        "\n",
        "    global optimal_loss\n",
        "    temp_loss = accum_loss\n",
        "    if temp_loss < optimal_loss:\n",
        "        optimal_loss = temp_loss\n",
        "    print(\"CL Loss: {:.5f}\\tCL Acc: {:.5f}\\tTime: {:.5f}\".format(accum_loss, accum_acc, time.time() - start_time))\n",
        "    return"
      ],
      "metadata": {
        "id": "aO_W9fWJ1We5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Pretraining\n",
        "# 5.1 set seed\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
        "    if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(args.seed)"
      ],
      "metadata": {
        "id": "T5Nht4Qc1gKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.2 prepare text model\n",
        "kwargs = {}\n",
        "\n",
        "if args.text_type == \"SciBERT\":\n",
        "    pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
        "    print(\"Download SciBert to {}\".format(pretrained_SciBERT_folder))\n",
        "    text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
        "    text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
        "    kwargs[\"text_tokenizer\"] = text_tokenizer\n",
        "    kwargs[\"text_model\"] = text_model\n",
        "    text_dim = 768\n",
        "else:\n",
        "    raise Exception"
      ],
      "metadata": {
        "id": "EAXcWeUF1nzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_model)"
      ],
      "metadata": {
        "id": "UeTT8yyc1v2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.3 starting training MoleculeSTM-Graph\n",
        "\n",
        "# prepare GraphMVP (Graph Model) and Dataset\n",
        "dataset_root = os.path.join(args.dataspace_path, \"PubChemSTM_data\")\n",
        "\n",
        "molecule_type = \"Graph\"\n",
        "\n",
        "# PubChemSTM_Datasets_Graph(dataset_root)\n",
        "dataset = PubChemSTM_SubDatasets_Graph(dataset_root, size=100) #size = 1000\n",
        "\n",
        "dataloader_class = pyg_DataLoader\n",
        "\n",
        "molecule_node_model = GNN(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim,\n",
        "    JK=args.JK, drop_ratio=args.dropout_ratio,\n",
        "    gnn_type=args.gnn_type)\n",
        "molecule_model = GNN_graphpred(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim, JK=args.JK, graph_pooling=args.graph_pooling,\n",
        "    num_tasks=1, molecule_node_model=molecule_node_model)\n",
        "pretrained_model_path = os.path.join(args.dataspace_path, \"pretrained_GraphMVP\", args.pretrain_gnn_mode, \"model.pth\")\n",
        "molecule_model.from_pretrained(pretrained_model_path)\n",
        "\n",
        "molecule_model = molecule_model.to(device)\n",
        "\n",
        "kwargs[\"molecule_model\"] = molecule_model\n",
        "molecule_dim = args.gnn_emb_dim\n",
        "\n",
        "dataloader = dataloader_class(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)"
      ],
      "metadata": {
        "id": "-DoXzEc611YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"dataset size is: {dataset.size}, sample data shows below:\")\n",
        "for i in range(20):\n",
        "  print(dataset.CID_list[i], dataset.text_list[i])"
      ],
      "metadata": {
        "id": "Mg9A16LL22Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(molecule_model)"
      ],
      "metadata": {
        "id": "j_qGz95j2J78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare two project layers\n",
        "text2latent = nn.Linear(text_dim, args.SSL_emb_dim).to(device)\n",
        "mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim).to(device)\n",
        "print(f\"text2latent: {text2latent}\")\n",
        "print(f\"mol2latent: {mol2latent}\")"
      ],
      "metadata": {
        "id": "3g4LRboq2ZRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare optimizers\n",
        "model_param_group = [\n",
        "    {\"params\": text_model.parameters(), \"lr\": args.text_lr},\n",
        "    {\"params\": molecule_model.parameters(), \"lr\": args.mol_lr},\n",
        "    {\"params\": text2latent.parameters(), \"lr\": args.text_lr * args.text_lr_scale},\n",
        "    {\"params\": mol2latent.parameters(), \"lr\": args.mol_lr * args.mol_lr_scale},\n",
        "]\n",
        "optimizer = optim.Adam(model_param_group, weight_decay=args.decay)\n",
        "optimal_loss = 1e10\n",
        "print(optimizer)"
      ],
      "metadata": {
        "id": "ZB2kgPvr2kwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start training\n",
        "for e in range(3):\n",
        "    print(\"Epoch {}\".format(e))\n",
        "    train(e, dataloader, **kwargs)"
      ],
      "metadata": {
        "id": "Ahej76Bp2svn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Training\n"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Computational requirements**\n",
        "\n",
        "\n",
        "*   Hardware: Google Colab V100 GPU, RAM 16GB\n",
        "*   Seeds: 42\n",
        "*   Training epochs: 100"
      ],
      "metadata": {
        "id": "fdIvSq2i7ExF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation code**"
      ],
      "metadata": {
        "id": "tCXoJGaGcjlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1 Downstream: Zero-shot Structure-text Retrieval**"
      ],
      "metadata": {
        "id": "d6VHPwwvlnjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Packages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/MoleculeSTM')\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader as torch_DataLoader\n",
        "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from MoleculeSTM.datasets import DrugBank_Datasets_SMILES_retrieval, DrugBank_Datasets_Graph_retrieval\n",
        "from MoleculeSTM.models import GNN, GNN_graphpred\n",
        "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network\n",
        "\n",
        "# Set-up the environment variable to ignore warnings\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
      ],
      "metadata": {
        "id": "7PW3BqCIGjbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Setup Arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--seed\", type=int, default=42)\n",
        "parser.add_argument(\"--device\", type=int, default=0)\n",
        "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
        "parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\", \"BioBERT\"])\n",
        "parser.add_argument(\"--load_latent_projector\", type=int, default=1)\n",
        "parser.add_argument(\"--training_mode\", type=str, default=\"zero_shot\", choices=[\"zero_shot\"])\n",
        "\n",
        "########## for dataset and split ##########\n",
        "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
        "parser.add_argument(\"--dataset\", type=str, default=\"PubChem\")\n",
        "parser.add_argument(\"--task\", type=str, default=\"molecule_description\",\n",
        "    choices=[\n",
        "        \"molecule_description\", \"molecule_description_Raw\",\n",
        "        \"molecule_description_removed_PubChem\", \"molecule_description_removed_PubChem_Raw\",\n",
        "        \"molecule_pharmacodynamics\", \"molecule_pharmacodynamics_Raw\",\n",
        "        \"molecule_pharmacodynamics_removed_PubChem\", \"molecule_pharmacodynamics_removed_PubChem_Raw\"])\n",
        "parser.add_argument(\"--test_mode\", type=str, default=\"given_text\", choices=[\"given_text\", \"given_molecule\"])\n",
        "\n",
        "########## for optimization ##########\n",
        "parser.add_argument(\"--T_list\", type=int, nargs=\"+\", default=[4, 10, 20])\n",
        "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
        "parser.add_argument(\"--epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--text_lr\", type=float, default=1e-5)\n",
        "parser.add_argument(\"--mol_lr\", type=float, default=1e-5)\n",
        "parser.add_argument(\"--text_lr_scale\", type=float, default=0.1)\n",
        "parser.add_argument(\"--mol_lr_scale\", type=float, default=0.1)\n",
        "parser.add_argument(\"--decay\", type=float, default=0)\n",
        "\n",
        "########## for contrastive objective ##########\n",
        "parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
        "parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
        "parser.add_argument(\"--T\", type=float, default=0.1)\n",
        "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
        "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
        "parser.set_defaults(normalize=True)\n",
        "\n",
        "########## for BERT model ##########\n",
        "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
        "\n",
        "########## for molecule model ##########\n",
        "parser.add_argument(\"--molecule_type\", type=str, default=\"Graph\", choices=[\"SMILES\", \"Graph\"])\n",
        "\n",
        "########## for 2D GNN ##########\n",
        "parser.add_argument(\"--gnn_emb_dim\", type=int, default=300)\n",
        "parser.add_argument(\"--num_layer\", type=int, default=5)\n",
        "parser.add_argument('--JK', type=str, default='last')\n",
        "parser.add_argument(\"--dropout_ratio\", type=float, default=0.5)\n",
        "parser.add_argument(\"--gnn_type\", type=str, default=\"gin\")\n",
        "parser.add_argument('--graph_pooling', type=str, default='mean')\n",
        "\n",
        "########## for saver ##########\n",
        "parser.add_argument(\"--eval_train\", type=int, default=0)\n",
        "parser.add_argument(\"--verbose\", type=int, default=0)\n",
        "\n",
        "parser.add_argument(\"--input_model_dir\", type=str, default=\"demo/demo_checkpoints_Graph\")\n",
        "parser.add_argument(\"--input_model_path\", type=str, default=\"demo/demo_checkpoints_Graph/molecule_model.pth\")\n",
        "\n",
        "\n",
        "args = parser.parse_args(\"\")\n",
        "print(\"arguments\\t\", args)"
      ],
      "metadata": {
        "id": "XC6TQ3fzGz4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Setup Seed\n",
        "np.random.seed(args.seed)\n",
        "torch.random.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
        "    if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "F2WqGnbBG6IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Load SciBERT\n",
        "pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
        "text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
        "text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
        "text_dim = 768\n",
        "\n",
        "# input_model_path = os.path.join(args.input_model_dir, \"text_model.pth\")\n",
        "input_model_path = \"/content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/text_model.pth\"\n",
        "print(\"Loading from {}...\".format(input_model_path))\n",
        "state_dict = torch.load(input_model_path, map_location='cpu')\n",
        "text_model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "hDt4xeEwG8Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_model)"
      ],
      "metadata": {
        "id": "8wVqX-XB8YJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Load MoleculeSTM-Graph\n",
        "molecule_node_model = GNN(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim,\n",
        "    JK=args.JK, drop_ratio=args.dropout_ratio,\n",
        "    gnn_type=args.gnn_type)\n",
        "molecule_model = GNN_graphpred(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim, JK=args.JK, graph_pooling=args.graph_pooling,\n",
        "    num_tasks=1, molecule_node_model=molecule_node_model)\n",
        "molecule_dim = args.gnn_emb_dim\n",
        "\n",
        "input_model_path = \"/content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/molecule_model.pth\"\n",
        "print(\"Loading from {}...\".format(input_model_path))\n",
        "state_dict = torch.load(input_model_path, map_location='cpu')\n",
        "molecule_model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "cabfBuQI8ljm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(molecule_model)"
      ],
      "metadata": {
        "id": "n4d9aymB9OS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Load Projection Layers\n",
        "text2latent = nn.Linear(text_dim, args.SSL_emb_dim)\n",
        "input_model_path = \"/content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/text2latent_model.pth\"\n",
        "print(\"Loading from {}...\".format(input_model_path))\n",
        "state_dict = torch.load(input_model_path, map_location='cpu')\n",
        "text2latent.load_state_dict(state_dict)\n",
        "\n",
        "mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim)\n",
        "input_model_path = \"/content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/mol2latent_model.pth\"\n",
        "print(\"Loading from {}...\".format(input_model_path))\n",
        "state_dict = torch.load(input_model_path, map_location='cpu')\n",
        "mol2latent.load_state_dict(state_dict)\n",
        "\n",
        "print(f\"text2latent: {text2latent}\")\n",
        "print(f\"mol2latent: {mol2latent}\")"
      ],
      "metadata": {
        "id": "2N-Y-eF0H9qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. support functions\n",
        "def cycle_index(num, shift):\n",
        "    arr = torch.arange(num) + shift\n",
        "    arr[-shift:] = torch.arange(shift)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def do_CL_eval(X, Y, neg_Y, args):\n",
        "    X = F.normalize(X, dim=-1)\n",
        "    X = X.unsqueeze(1) # B, 1, d\n",
        "\n",
        "    Y = Y.unsqueeze(0)\n",
        "    Y = torch.cat([Y, neg_Y], dim=0) # T, B, d\n",
        "    Y = Y.transpose(0, 1)  # B, T, d\n",
        "    Y = F.normalize(Y, dim=-1)\n",
        "\n",
        "    logits = torch.bmm(X, Y.transpose(1, 2)).squeeze()  # B*T\n",
        "    B = X.size()[0]\n",
        "    labels = torch.zeros(B).long().to(logits.device)  # B*1\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    CL_loss = criterion(logits, labels)\n",
        "    pred = logits.argmax(dim=1, keepdim=False)\n",
        "    confidence = logits\n",
        "    CL_conf = confidence.max(dim=1)[0]\n",
        "    CL_conf = CL_conf.cpu().numpy()\n",
        "\n",
        "    CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
        "    return CL_loss, CL_conf, CL_acc\n",
        "\n",
        "\n",
        "def get_text_repr(text):\n",
        "    text_tokens_ids, text_masks = prepare_text_tokens(\n",
        "        device=device, description=text, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
        "    text_output = text_model(input_ids=text_tokens_ids, attention_mask=text_masks)\n",
        "    text_repr = text_output[\"pooler_output\"]\n",
        "    text_repr = text2latent(text_repr)\n",
        "    return text_repr\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(dataloader):\n",
        "    text_model.eval()\n",
        "    molecule_model.eval()\n",
        "    text2latent.eval()\n",
        "    mol2latent.eval()\n",
        "\n",
        "    accum_acc_list = [0 for _ in args.T_list]\n",
        "    if args.verbose:\n",
        "        L = tqdm(dataloader)\n",
        "    else:\n",
        "        L = dataloader\n",
        "    for batch in L:\n",
        "        text = batch[0]\n",
        "        molecule_data = batch[1]\n",
        "        neg_text = batch[2]\n",
        "        neg_molecule_data = batch[3]\n",
        "\n",
        "        text_repr = get_text_repr(text)\n",
        "\n",
        "        molecule_data = molecule_data.to(device)\n",
        "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
        "            molecule_data, mol2latent=mol2latent,\n",
        "            molecule_type=\"Graph\", molecule_model=molecule_model)\n",
        "\n",
        "        if test_mode == \"given_text\":\n",
        "            neg_molecule_repr = [\n",
        "                get_molecule_repr_MoleculeSTM(\n",
        "                    neg_molecule_data[idx].to(device), mol2latent=mol2latent,\n",
        "                    molecule_type=\"Graph\", molecule_model=molecule_model) for idx in range(T_max)\n",
        "            ]\n",
        "            neg_molecule_repr = torch.stack(neg_molecule_repr)\n",
        "            for T_idx, T in enumerate(args.T_list):\n",
        "                _, _, acc = do_CL_eval(text_repr, molecule_repr, neg_molecule_repr[:T-1], args)\n",
        "                accum_acc_list[T_idx] += acc\n",
        "        elif test_mode == \"given_molecule\":\n",
        "            neg_text_repr = [get_text_repr(neg_text[idx]) for idx in range(T_max)]\n",
        "            neg_text_repr = torch.stack(neg_text_repr)\n",
        "            for T_idx, T in enumerate(args.T_list):\n",
        "                _, _, acc = do_CL_eval(molecule_repr, text_repr, neg_text_repr[:T-1], args)\n",
        "                accum_acc_list[T_idx] += acc\n",
        "        else:\n",
        "            raise Exception\n",
        "\n",
        "    accum_acc_list = np.array(accum_acc_list)\n",
        "    accum_acc_list /= len(dataloader)\n",
        "    return accum_acc_list"
      ],
      "metadata": {
        "id": "zr69ai8aINGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. text retrival\n",
        "text_model = text_model.to(device)\n",
        "molecule_model = molecule_model.to(device)\n",
        "text2latent = text2latent.to(device)\n",
        "mol2latent = mol2latent.to(device)\n",
        "\n",
        "T_max = max(args.T_list) - 1\n",
        "\n",
        "initial_test_acc_list = []\n",
        "test_mode = args.test_mode\n",
        "dataset_folder = os.path.join(args.dataspace_path, \"DrugBank_data\")\n",
        "\n",
        "\n",
        "dataset_class = DrugBank_Datasets_Graph_retrieval\n",
        "dataloader_class = pyg_DataLoader\n",
        "processed_dir_prefix = args.task\n",
        "\n",
        "if args.task == \"molecule_description\":\n",
        "    template = \"SMILES_description_{}.txt\"\n",
        "elif args.task == \"molecule_description_removed_PubChem\":\n",
        "    template = \"SMILES_description_removed_from_PubChem_{}.txt\"\n",
        "elif args.task == \"molecule_description_Raw\":\n",
        "    template = \"SMILES_description_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_description_removed_PubChem_Raw\":\n",
        "    template = \"SMILES_description_removed_from_PubChem_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics\":\n",
        "    template = \"SMILES_pharmacodynamics_{}.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_removed_PubChem\":\n",
        "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_Raw\":\n",
        "    template = \"SMILES_pharmacodynamics_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_removed_PubChem_Raw\":\n",
        "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}_Raw.txt\"\n",
        "\n",
        "full_dataset = dataset_class(dataset_folder, 'full', neg_sample_size=T_max, processed_dir_prefix=processed_dir_prefix, template=template)\n",
        "full_dataloader = dataloader_class(full_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers) # The program will get blcoked with none-zero num_workers\n",
        "\n",
        "initial_test_acc_list = eval_epoch(full_dataloader)\n",
        "print('Results', initial_test_acc_list)"
      ],
      "metadata": {
        "id": "3Rrh8VHaIQRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Data: Data(x=[40309, 2], edge_index=[2, 85886], edge_attr=[85886, 2], id=[1168])\n",
        "Index(['text', 'smiles'], dtype='object')\n",
        "Loading negative samples from ../data/DrugBank_data/index/SMILES_description_full.txt\n",
        "```"
      ],
      "metadata": {
        "id": "co3EqwE5908C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2 Downstream: Zero-shot Text-based Molecule Editing**"
      ],
      "metadata": {
        "id": "JJHi_2OQ86QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For graphs\n",
        "! python downstream_02_molecule_edit_step_01_MoleculeSTM_Space_Alignment.py \\\n",
        "    --MoleculeSTM_molecule_type=Graph \\\n",
        "    --MoleculeSTM_model_dir=../data/demo/demo_checkpoints_Graph\n",
        "\n",
        "\n",
        "! python downstream_02_molecule_edit_step_02_MoleculeSTM_Latent_Optimization.py \\\n",
        "    --MoleculeSTM_molecule_type=Graph \\\n",
        "    --MoleculeSTM_model_dir=../data/demo/demo_checkpoints_Graph \\\n",
        "    --language_edit_model_dir=../data/demo/demo_checkpoints_Graph \\\n",
        "    --input_description_id=101"
      ],
      "metadata": {
        "id": "UvYcCjyL8-jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3 Downstream: Molecular Property Prediction**"
      ],
      "metadata": {
        "id": "hLQYp0R_8-98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/MoleculeSTM')\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader as torch_DataLoader\n",
        "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
        "\n",
        "from MoleculeSTM.datasets import MoleculeNetSMILESDataset, MoleculeNetGraphDataset\n",
        "from MoleculeSTM.splitters import scaffold_split\n",
        "from MoleculeSTM.utils import get_num_task_and_type, get_molecule_repr_MoleculeSTM\n",
        "from MoleculeSTM.models import GNN, GNN_graphpred"
      ],
      "metadata": {
        "id": "M_Sb4KGzLRIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--seed\", type=int, default=42)\n",
        "parser.add_argument(\"--device\", type=int, default=0)\n",
        "parser.add_argument(\"--training_mode\", type=str, default=\"fine_tuning\", choices=[\"fine_tuning\", \"linear_probing\"])\n",
        "parser.add_argument(\"--molecule_type\", type=str, default=\"Graph\", choices=[\"SMILES\", \"Graph\"])\n",
        "\n",
        "########## for dataset and split ##########\n",
        "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
        "parser.add_argument(\"--dataset\", type=str, default=\"bace\")\n",
        "parser.add_argument(\"--split\", type=str, default=\"scaffold\")\n",
        "\n",
        "########## for optimization ##########\n",
        "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--lr_scale\", type=float, default=1)\n",
        "parser.add_argument(\"--num_workers\", type=int, default=1)\n",
        "parser.add_argument(\"--epochs\", type=int, default=5)\n",
        "parser.add_argument(\"--weight_decay\", type=float, default=0)\n",
        "parser.add_argument(\"--schedule\", type=str, default=\"cycle\")\n",
        "parser.add_argument(\"--warm_up_steps\", type=int, default=10)\n",
        "\n",
        "########## for 2D GNN ##########\n",
        "parser.add_argument(\"--gnn_emb_dim\", type=int, default=300)\n",
        "parser.add_argument(\"--num_layer\", type=int, default=5)\n",
        "parser.add_argument('--JK', type=str, default='last')\n",
        "parser.add_argument(\"--dropout_ratio\", type=float, default=0.5)\n",
        "parser.add_argument(\"--gnn_type\", type=str, default=\"gin\")\n",
        "parser.add_argument('--graph_pooling', type=str, default='mean')\n",
        "\n",
        "########## for saver ##########\n",
        "parser.add_argument(\"--eval_train\", type=int, default=0)\n",
        "parser.add_argument(\"--verbose\", type=int, default=1)\n",
        "\n",
        "parser.add_argument(\"--input_model_path\", type=str, default=\"/content/drive/MyDrive/MoleculeSTM/data/demo/demo_checkpoints_Graph/molecule_model.pth\")\n",
        "parser.add_argument(\"--output_model_dir\", type=str, default=None)\n",
        "\n",
        "args = parser.parse_args(\"\")\n",
        "print(\"arguments\\t\", args)"
      ],
      "metadata": {
        "id": "jq64aO1nLZUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup seed\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
        "    if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "9pY_raYALgDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tasks, task_mode = get_num_task_and_type(args.dataset)\n",
        "dataset_folder = os.path.join(args.dataspace_path, \"MoleculeNet_data\", args.dataset)\n",
        "\n",
        "dataset = MoleculeNetGraphDataset(dataset_folder, args.dataset)\n",
        "dataloader_class = pyg_DataLoader\n",
        "use_pyg_dataset = True\n",
        "\n",
        "smiles_list = pd.read_csv(\n",
        "    dataset_folder + \"/processed/smiles.csv\", header=None)[0].tolist()\n",
        "train_dataset, valid_dataset, test_dataset = scaffold_split(\n",
        "    dataset, smiles_list, null_value=0, frac_train=0.8,\n",
        "    frac_valid=0.1, frac_test=0.1, pyg_dataset=use_pyg_dataset)\n",
        "\n",
        "\n",
        "train_loader = dataloader_class(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
        "val_loader = dataloader_class(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
        "test_loader = dataloader_class(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n"
      ],
      "metadata": {
        "id": "u3gFLdSiLuZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_dataset_info(dataset):\n",
        "    print(f\"Dataset Name: {dataset.dataset}\")\n",
        "    print(f\"Number of Graphs: {len(dataset)}\")\n",
        "    print(f\"Transformations: {dataset.transform}\")\n",
        "    print(f\"Pre-transformations: {dataset.pre_transform}\")\n",
        "    print(f\"Pre-filters: {dataset.pre_filter}\")\n",
        "    print(f\"Processed Path: {dataset.processed_paths}\")\n",
        "    print(f\"Raw Files: {dataset.raw_file_names}\")\n",
        "\n",
        "print_dataset_info(train_dataset)"
      ],
      "metadata": {
        "id": "wPX1VkzGSCWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "molecule_node_model = GNN(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim,\n",
        "    JK=args.JK, drop_ratio=args.dropout_ratio,\n",
        "    gnn_type=args.gnn_type)\n",
        "model = GNN_graphpred(\n",
        "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim, JK=args.JK, graph_pooling=args.graph_pooling,\n",
        "    num_tasks=1, molecule_node_model=molecule_node_model)\n",
        "molecule_dim = args.gnn_emb_dim\n",
        "\n",
        "if \"GraphMVP\" in args.input_model_path:\n",
        "    print(\"Start from pretrained model (GraphMVP) in {}.\".format(args.input_model_path))\n",
        "    model.from_pretrained(args.input_model_path)\n",
        "else:\n",
        "    print(\"Start from pretrained model (MoleculeSTM) in {}.\".format(args.input_model_path))\n",
        "    state_dict = torch.load(args.input_model_path, map_location='cpu')\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "linear_model = nn.Linear(molecule_dim, num_tasks).to(device)\n",
        "\n",
        "# Rewrite the seed by MegaMolBART\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(args.seed)"
      ],
      "metadata": {
        "id": "74jdnrFoL33M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"model: {model}\")\n",
        "print(f\"linear model: {linear_model}\")"
      ],
      "metadata": {
        "id": "51WwsG0dPqcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.training_mode == \"fine_tuning\":\n",
        "    model_param_group = [\n",
        "        {\"params\": model.parameters()},\n",
        "        {\"params\": linear_model.parameters(), 'lr': args.lr * args.lr_scale}\n",
        "    ]\n",
        "else:\n",
        "    model_param_group = [\n",
        "        {\"params\": linear_model.parameters(), 'lr': args.lr * args.lr_scale}\n",
        "    ]\n",
        "optimizer = optim.Adam(model_param_group, lr=args.lr, weight_decay=args.weight_decay)\n",
        "print(optimizer)"
      ],
      "metadata": {
        "id": "vWcVSZWXMhGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classification(model, device, loader, optimizer):\n",
        "    if args.training_mode == \"fine_tuning\":\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    linear_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    if args.verbose:\n",
        "        L = tqdm(loader)\n",
        "    else:\n",
        "        L = loader\n",
        "    for step, batch in enumerate(L):\n",
        "        batch = batch.to(device)\n",
        "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
        "            batch, mol2latent=None,\n",
        "            molecule_type=\"Graph\", molecule_model=model)\n",
        "        pred = linear_model(molecule_repr)\n",
        "        pred = pred.float()\n",
        "        y = batch.y.view(pred.shape).to(device).float()\n",
        "\n",
        "        is_valid = y ** 2 > 0\n",
        "        loss_mat = criterion(pred, (y + 1) / 2)\n",
        "        loss_mat = torch.where(\n",
        "            is_valid, loss_mat,\n",
        "            torch.zeros(loss_mat.shape).to(device).to(loss_mat.dtype))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.sum(loss_mat) / torch.sum(is_valid)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.detach().item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_classification(model, device, loader):\n",
        "    model.eval()\n",
        "    linear_model.eval()\n",
        "    y_true, y_scores = [], []\n",
        "\n",
        "    if args.verbose:\n",
        "        L = tqdm(loader)\n",
        "    else:\n",
        "        L = loader\n",
        "    for step, batch in enumerate(L):\n",
        "        batch = batch.to(device)\n",
        "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
        "            batch, mol2latent=None,\n",
        "            molecule_type=\"Graph\", molecule_model=model)\n",
        "        pred = linear_model(molecule_repr)\n",
        "        pred = pred.float()\n",
        "        y = batch.y.view(pred.shape).to(device).float()\n",
        "\n",
        "        y_true.append(y)\n",
        "        y_scores.append(pred)\n",
        "\n",
        "    y_true = torch.cat(y_true, dim=0).cpu().numpy()\n",
        "    y_scores = torch.cat(y_scores, dim=0).cpu().numpy()\n",
        "\n",
        "    roc_list = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        # AUC is only defined when there is at least one positive data.\n",
        "        if np.sum(y_true[:, i] == 1) > 0 and np.sum(y_true[:, i] == -1) > 0:\n",
        "            is_valid = y_true[:, i] ** 2 > 0\n",
        "            roc_list.append(roc_auc_score((y_true[is_valid, i] + 1) / 2, y_scores[is_valid, i]))\n",
        "        else:\n",
        "            print(\"{} is invalid\".format(i))\n",
        "\n",
        "    if len(roc_list) < y_true.shape[1]:\n",
        "        print(len(roc_list))\n",
        "        print(\"Some target is missing!\")\n",
        "        print(\"Missing ratio: %f\" %(1 - float(len(roc_list)) / y_true.shape[1]))\n",
        "\n",
        "    return sum(roc_list) / len(roc_list), 0, y_true, y_scores"
      ],
      "metadata": {
        "id": "KUiaWzhjMj8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_func = train_classification\n",
        "eval_func = eval_classification\n",
        "\n",
        "train_roc_list, val_roc_list, test_roc_list = [], [], []\n",
        "train_acc_list, val_acc_list, test_acc_list = [], [], []\n",
        "best_val_roc, best_val_idx = -1, 0\n",
        "criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    loss_acc = train_func(model, device, train_loader, optimizer)\n",
        "    print(\"Epoch: {}\\nLoss: {}\".format(epoch, loss_acc))\n",
        "\n",
        "    if args.eval_train:\n",
        "        train_roc, train_acc, train_target, train_pred = eval_func(model, device, train_loader)\n",
        "    else:\n",
        "        train_roc = train_acc = 0\n",
        "    val_roc, val_acc, val_target, val_pred = eval_func(model, device, val_loader)\n",
        "    test_roc, test_acc, test_target, test_pred = eval_func(model, device, test_loader)\n",
        "\n",
        "    train_roc_list.append(train_roc)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_roc_list.append(val_roc)\n",
        "    val_acc_list.append(val_acc)\n",
        "    test_roc_list.append(test_roc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(\"train: {:.6f}\\tval: {:.6f}\\ttest: {:.6f}\".format(train_roc, val_roc, test_roc))\n",
        "    print()\n",
        "\n",
        "print(\"best train: {:.6f}\\tval: {:.6f}\\ttest: {:.6f}\".format(train_roc_list[best_val_idx], val_roc_list[best_val_idx], test_roc_list[best_val_idx]))\n"
      ],
      "metadata": {
        "id": "7bg3p-SCMnJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Evaluation"
      ],
      "metadata": {
        "id": "D61YLwnIfgJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric descriptions"
      ],
      "metadata": {
        "id": "ZS7jMVtNfjWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This evaluation uses several different metrics:\n",
        "\n",
        "\n",
        "*   Contrastive loss - measuring the the performance of the model in correctly identifying true matches from a set of possible matches\n",
        "*   Accuracy - determines the proportion of correctly labeled predictive matches\n",
        "*   Confidence Scores - determines a measure of confidence for each prediction\n",
        "\n",
        "\n",
        "Additionally, the code uses a variety of negative samples to help measure the model's resistence to varying conditions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2D0aAK0l9202"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation code"
      ],
      "metadata": {
        "id": "Ts2-ejAyfn1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Support Functions and Setup"
      ],
      "metadata": {
        "id": "F1Gj_Wp88r65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from demo downstream retrieval Graph\n",
        "def cycle_index(num, shift):\n",
        "    arr = torch.arange(num) + shift\n",
        "    arr[-shift:] = torch.arange(shift)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def do_CL_eval(X, Y, neg_Y, args):\n",
        "    X = F.normalize(X, dim=-1)\n",
        "    X = X.unsqueeze(1) # B, 1, d\n",
        "\n",
        "    Y = Y.unsqueeze(0)\n",
        "    Y = torch.cat([Y, neg_Y], dim=0) # T, B, d\n",
        "    Y = Y.transpose(0, 1)  # B, T, d\n",
        "    Y = F.normalize(Y, dim=-1)\n",
        "\n",
        "    logits = torch.bmm(X, Y.transpose(1, 2)).squeeze()  # B*T\n",
        "    B = X.size()[0]\n",
        "    labels = torch.zeros(B).long().to(logits.device)  # B*1\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    CL_loss = criterion(logits, labels)\n",
        "    pred = logits.argmax(dim=1, keepdim=False)\n",
        "    confidence = logits\n",
        "    CL_conf = confidence.max(dim=1)[0]\n",
        "    CL_conf = CL_conf.cpu().numpy()\n",
        "\n",
        "    CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
        "    return CL_loss, CL_conf, CL_acc\n",
        "\n",
        "\n",
        "def get_text_repr(text):\n",
        "    text_tokens_ids, text_masks = prepare_text_tokens(\n",
        "        device=device, description=text, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
        "    text_output = text_model(input_ids=text_tokens_ids, attention_mask=text_masks)\n",
        "    text_repr = text_output[\"pooler_output\"]\n",
        "    text_repr = text2latent(text_repr)\n",
        "    return text_repr\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(dataloader):\n",
        "    text_model.eval()\n",
        "    molecule_model.eval()\n",
        "    text2latent.eval()\n",
        "    mol2latent.eval()\n",
        "\n",
        "    accum_acc_list = [0 for _ in args.T_list]\n",
        "    if args.verbose:\n",
        "        L = tqdm(dataloader)\n",
        "    else:\n",
        "        L = dataloader\n",
        "    for batch in L:\n",
        "        text = batch[0]\n",
        "        molecule_data = batch[1]\n",
        "        neg_text = batch[2]\n",
        "        neg_molecule_data = batch[3]\n",
        "\n",
        "        text_repr = get_text_repr(text)\n",
        "\n",
        "        molecule_data = molecule_data.to(device)\n",
        "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
        "            molecule_data, mol2latent=mol2latent,\n",
        "            molecule_type=\"Graph\", molecule_model=molecule_model)\n",
        "\n",
        "        if test_mode == \"given_text\":\n",
        "            neg_molecule_repr = [\n",
        "                get_molecule_repr_MoleculeSTM(\n",
        "                    neg_molecule_data[idx].to(device), mol2latent=mol2latent,\n",
        "                    molecule_type=\"Graph\", molecule_model=molecule_model) for idx in range(T_max)\n",
        "            ]\n",
        "            neg_molecule_repr = torch.stack(neg_molecule_repr)\n",
        "            for T_idx, T in enumerate(args.T_list):\n",
        "                _, _, acc = do_CL_eval(text_repr, molecule_repr, neg_molecule_repr[:T-1], args)\n",
        "                accum_acc_list[T_idx] += acc\n",
        "        elif test_mode == \"given_molecule\":\n",
        "            neg_text_repr = [get_text_repr(neg_text[idx]) for idx in range(T_max)]\n",
        "            neg_text_repr = torch.stack(neg_text_repr)\n",
        "            for T_idx, T in enumerate(args.T_list):\n",
        "                _, _, acc = do_CL_eval(molecule_repr, text_repr, neg_text_repr[:T-1], args)\n",
        "                accum_acc_list[T_idx] += acc\n",
        "        else:\n",
        "            raise Exception\n",
        "\n",
        "    accum_acc_list = np.array(accum_acc_list)\n",
        "    accum_acc_list /= len(dataloader)\n",
        "    return accum_acc_list"
      ],
      "metadata": {
        "id": "5ZFsNDA6fqP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval"
      ],
      "metadata": {
        "id": "Y8LWvzss9KX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from demo downstream retrieval Graph\n",
        "text_model = text_model.to(device)\n",
        "molecule_model = molecule_model.to(device)\n",
        "text2latent = text2latent.to(device)\n",
        "mol2latent = mol2latent.to(device)\n",
        "\n",
        "T_max = max(args.T_list) - 1\n",
        "\n",
        "initial_test_acc_list = []\n",
        "test_mode = args.test_mode\n",
        "dataset_folder = os.path.join(args.dataspace_path, \"DrugBank_data\")\n",
        "\n",
        "\n",
        "dataset_class = DrugBank_Datasets_Graph_retrieval\n",
        "dataloader_class = pyg_DataLoader\n",
        "processed_dir_prefix = args.task\n",
        "\n",
        "if args.task == \"molecule_description\":\n",
        "    template = \"SMILES_description_{}.txt\"\n",
        "elif args.task == \"molecule_description_removed_PubChem\":\n",
        "    template = \"SMILES_description_removed_from_PubChem_{}.txt\"\n",
        "elif args.task == \"molecule_description_Raw\":\n",
        "    template = \"SMILES_description_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_description_removed_PubChem_Raw\":\n",
        "    template = \"SMILES_description_removed_from_PubChem_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics\":\n",
        "    template = \"SMILES_pharmacodynamics_{}.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_removed_PubChem\":\n",
        "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_Raw\":\n",
        "    template = \"SMILES_pharmacodynamics_{}_Raw.txt\"\n",
        "elif args.task == \"molecule_pharmacodynamics_removed_PubChem_Raw\":\n",
        "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}_Raw.txt\"\n",
        "\n",
        "full_dataset = dataset_class(dataset_folder, 'full', neg_sample_size=T_max, processed_dir_prefix=processed_dir_prefix, template=template)\n",
        "full_dataloader = dataloader_class(full_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers) # The program will get blcoked with none-zero num_workers\n",
        "\n",
        "initial_test_acc_list = eval_epoch(full_dataloader)"
      ],
      "metadata": {
        "id": "pI4qY8gi9I1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "zMHv_2C5f6wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion"
      ],
      "metadata": {
        "id": "JNQWmk3P7gse"
      }
    }
  ]
}