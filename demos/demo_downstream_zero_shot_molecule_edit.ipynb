{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef1d4052",
   "metadata": {},
   "source": [
    "# Demo for MoleculeSTM Downstream: Molecule Editing\n",
    "\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e81e82-8fd8-4c68-be10-7e0e3760d6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-30 12:31:55,780] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit import DataStructs\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "from MoleculeSTM.utils import prepare_text_tokens\n",
    "from MoleculeSTM.downstream_molecule_edit_utils import get_SMILES_list, get_description_list, load_language_molecule_and_edit_models, clip_loss_for_edit\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../scripts\")\n",
    "from downstream_02_molecule_edit_step_02_MoleculeSTM_Latent_Optimization import get_lr, mean_pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bb3d4",
   "metadata": {},
   "source": [
    "## Setup Arguments\n",
    "\n",
    "Notice that at this step, we are only using the textual branch (SciBERT) and a pretrained molecule generative model (MegaMolBART). The MoleculeSTM chemical branch (MegaMolBART or GraphMVP) is only used at the module alignment phase, and we can change it in the `MoleculeSTM_model_dir` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c84026-7791-450d-a86c-59f86281fea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(MegaMolBART_generation_model_dir='../data/pretrained_MegaMolBART/checkpoints', MoleculeSTM_model_dir='demo_checkpoints_SMILES', MoleculeSTM_molecule_type='SMILES', SSL_emb_dim=256, dataspace_path='../data', device=0, epochs=100, input_SMILES='OC1C2C1CC2', input_SMILES_file=None, input_description=None, input_description_id=None, language_edit_model_dir='demo_checkpoints_SMILES', lr=0.1, lr_rampup=0.05, max_seq_len=512, mode='edit', normalize=True, output_model_dir=None, seed=42, use_noise_for_init=True, verbose=1, vocab_path='../MoleculeSTM/bart_vocab.txt')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--device\", type=int, default=0)\n",
    "parser.add_argument(\"--verbose\", type=int, default=1)\n",
    "\n",
    "########## for editing ##########\n",
    "parser.add_argument(\"--input_description\", type=str, default=None)\n",
    "parser.add_argument(\"--input_description_id\", type=int, default=None)\n",
    "parser.add_argument(\"--input_SMILES\", type=str, default=\"OC1C2C1CC2\")\n",
    "parser.add_argument(\"--input_SMILES_file\", type=str, default=None)\n",
    "parser.add_argument(\"--output_model_dir\", type=str, default=None)\n",
    "parser.add_argument(\"--mode\", type=str, default=\"edit\", choices=[\"edit\", \"free_generation\"])\n",
    "parser.add_argument(\"--use_noise_for_init\", dest=\"use_noise_for_init\", action=\"store_true\")\n",
    "parser.add_argument(\"--no_noise_for_init\", dest=\"use_noise_for_init\", action=\"store_false\")\n",
    "parser.set_defaults(use_noise_for_init=True)\n",
    "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
    "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
    "parser.set_defaults(normalize=True)\n",
    "\n",
    "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
    "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
    "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
    "\n",
    "########## for foundation ##########\n",
    "parser.add_argument(\"--MoleculeSTM_model_dir\", type=str, default=\"demo_checkpoints_SMILES\")\n",
    "parser.add_argument(\"--MoleculeSTM_molecule_type\", type=str, default=\"SMILES\", choices=[\"SMILES\", \"Graph\"])\n",
    "parser.add_argument(\"--vocab_path\", type=str, default=\"../MoleculeSTM/bart_vocab.txt\")\n",
    "\n",
    "########## for generation ##########\n",
    "parser.add_argument(\"--MegaMolBART_generation_model_dir\", type=str, default=\"../data/pretrained_MegaMolBART/checkpoints\")\n",
    "\n",
    "########## for foundation and generation projection ##########\n",
    "parser.add_argument(\"--language_edit_model_dir\", type=str, default=\"demo_checkpoints_SMILES\")   \n",
    "\n",
    "########## for editing ##########\n",
    "parser.add_argument(\"--lr_rampup\", type=float, default=0.05)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0083fa5c",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5089f92-fec1-45c2-9035-32579ad8725a",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from demo_checkpoints_SMILES/text_model.pth...\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... None\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... False\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ False\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... infer\n",
      "  data_path ....................... None\n",
      "  dataset_path .................... None\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... False\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ None\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 100\n",
      "  exit_interval ................... None\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  gas ............................. 1\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 256\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ ../data/pretrained_MegaMolBART/checkpoints\n",
      "  local_rank ...................... None\n",
      "  log_interval .................... 100\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. None\n",
      "  lr_decay_iters .................. None\n",
      "  lr_decay_style .................. linear\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 512\n",
      "  merge_file ...................... None\n",
      "  min_lr .......................... 0.0\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 8\n",
      "  num_layers ...................... 4\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... False\n",
      "  pipe_parallel_size .............. 0\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ None\n",
      "  save_interval ................... None\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... None\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 969, 30, 1\n",
      "  synchronize_each_layer .......... False\n",
      "  tensorboard_dir ................. None\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  train_iters ..................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  vocab_file ...................... ../MoleculeSTM/bart_vocab.txt\n",
      "  warmup .......................... 0.01\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 0.0\n",
      "  zero_contigious_gradients ....... False\n",
      "  zero_reduce_bucket_size ......... 0.0\n",
      "  zero_reduce_scatter ............. False\n",
      "  zero_stage ...................... 1.0\n",
      "---------------- end of arguments ----------------\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "Loading vocab from ../MoleculeSTM/bart_vocab.txt.\n",
      "Loading from ../data/pretrained_MegaMolBART/checkpoints\n",
      "global rank 0 is loading checkpoint ../data/pretrained_MegaMolBART/checkpoints/iter_0134000/mp_rank_00/model_optim_rng.pt\n",
      "could not find arguments in the checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  successfully loaded ../data/pretrained_MegaMolBART/checkpoints/iter_0134000/mp_rank_00/model_optim_rng.pt\n",
      "Loading from pretrained MegaMolBART (../data/pretrained_MegaMolBART/checkpoints).\n",
      "Loading from demo_checkpoints_SMILES/text2latent_model.pth...\n",
      "Loading from demo_checkpoints_SMILES/mol2latent_model.pth...\n",
      "Loading from demo_checkpoints_SMILES/generation2foundation_model.pth...\n",
      "Loading from demo_checkpoints_SMILES/foundation2generation_model.pth...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model, text_tokenizer, text_dim, molecule_model, MegaMolBART_wrapper, molecule_dim, \\\n",
    "    text2latent, mol2latent, generation2foundation, foundation2generation = load_language_molecule_and_edit_models(args)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
    "    if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "text_model = text_model.to(device)\n",
    "molecule_model = molecule_model.to(device)\n",
    "text2latent = text2latent.to(device)\n",
    "mol2latent = mol2latent.to(device)\n",
    "generation2foundation.to(device)\n",
    "foundation2generation.to(device)\n",
    "text_model.eval()\n",
    "molecule_model.eval()\n",
    "text2latent.eval()\n",
    "mol2latent.eval()\n",
    "generation2foundation.eval()\n",
    "foundation2generation.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c07337",
   "metadata": {},
   "source": [
    "# Reset seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7b38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.random.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
    "    if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c390a992",
   "metadata": {},
   "source": [
    "## Define Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc9a3a3-ebb6-4806-9e49-314213ff4aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_SMILES_list(SMILES_list, description):\n",
    "    print(\"SMILES_list:\", SMILES_list)\n",
    "    mol_list = []\n",
    "    for SMILES in SMILES_list:\n",
    "        mol = Chem.MolFromSmiles(SMILES)\n",
    "        if mol is None:\n",
    "            continue\n",
    "        mol_list.append(mol)\n",
    "\n",
    "    if len(mol_list) < 3:\n",
    "        return [False]\n",
    "\n",
    "    if \"soluble\" in description and \"insoluble\" not in description:\n",
    "        props = [\"MolLogP\"]\n",
    "        prop_pred = [(n, func) for n, func in Descriptors.descList if n.split(\"_\")[-1] in props]\n",
    "        value_list = []\n",
    "        for name, func in prop_pred:\n",
    "            for idx, (SMILES, mol) in enumerate(zip(SMILES_list, mol_list)):\n",
    "                if idx == 1:\n",
    "                    continue\n",
    "                value = func(mol)\n",
    "                value_list.append(value)\n",
    "                print(\"SMILES: {}\\t\\t\\tlogP: {:.5f}\".format(SMILES, value))\n",
    "        if value_list[0] > value_list[-1]:\n",
    "            answer = [True]\n",
    "        else:\n",
    "            answer = [False]\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def check_edit(SMILES, text, device):\n",
    "    text_list = [text]\n",
    "    text_tokens_ids, text_masks = prepare_text_tokens(\n",
    "        device=device, description=text_list, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
    "    text_output = text_model(input_ids=text_tokens_ids, attention_mask=text_masks)\n",
    "    text_repr = text_output[\"pooler_output\"]\n",
    "    text_repr = text2latent(text_repr)\n",
    "\n",
    "    first_and_second_SMILES_list = []\n",
    "\n",
    "    latent_code_init, pad_mask_init = MegaMolBART_wrapper.smileslist2embedding([SMILES])  # [pad, B, d], [pad, B]\n",
    "    first_and_second_SMILES_list.append(SMILES)\n",
    "\n",
    "    regenerated_mols = MegaMolBART_wrapper.inverse_transform([latent_code_init], pad_mask_init.bool().cuda(), k=1, sanitize=True)\n",
    "    first_and_second_SMILES_list.append(regenerated_mols[0])\n",
    "\n",
    "    l2_lambda_list = [1e0]\n",
    "    result_SMILES_list_one_pair, result_eval_list_one_pair = [], []\n",
    "    \n",
    "    if args.use_noise_for_init:\n",
    "        print(\"Use random noise for init\")\n",
    "        random_noise = torch.randn(latent_code_init.size()).to(device)\n",
    "    \n",
    "    for l2_lambda in l2_lambda_list:\n",
    "        print(\"l2 lambda: {}\".format(l2_lambda))\n",
    "        current_SMILES_list = [first_and_second_SMILES_list[0]] + [first_and_second_SMILES_list[1]]\n",
    "        if args.use_noise_for_init:\n",
    "            print(\"Use random noise for init\")\n",
    "            latent = latent_code_init.detach().clone() + random_noise\n",
    "        else:\n",
    "            print(\"No random noise for init\")\n",
    "            latent = latent_code_init.detach().clone()\n",
    "        pad_mask = pad_mask_init.detach().clone()\n",
    "        latent.requires_grad = True\n",
    "        optimizer = optim.Adam([latent], lr=args.lr)\n",
    "        \n",
    "        if args.verbose:\n",
    "            L = tqdm(range(args.epochs))\n",
    "        else:\n",
    "            L = range(args.epochs)\n",
    "\n",
    "        for i in L:\n",
    "            t = i / args.epochs\n",
    "            lr = get_lr(t, args.lr)\n",
    "            optimizer.param_groups[0][\"lr\"] = lr\n",
    "\n",
    "            molecule_repr_generation = mean_pooling(latent, pad_mask) # [B, d]\n",
    "            if args.normalize:\n",
    "                molecule_repr_generation = F.normalize(molecule_repr_generation, dim=-1)\n",
    "            molecule_repr_foundation = generation2foundation(molecule_repr_generation)\n",
    "\n",
    "            clip_loss_ = clip_loss_for_edit(molecule_repr_foundation, text_repr)\n",
    "            l2_loss_ =  l2_lambda * ((latent_code_init - latent) ** 2).mean()\n",
    "\n",
    "            loss = clip_loss_ + l2_loss_\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "        print(\"clip loss: {:.5f}\\tL2 loss: {:.5f}\".format(clip_loss_.item(), l2_loss_.item()))\n",
    "\n",
    "        generated_mols = MegaMolBART_wrapper.inverse_transform([latent], pad_mask.bool().cuda(), k=1, sanitize=True)\n",
    "        current_SMILES_list.append(generated_mols[0])\n",
    "        result_SMILES_list_one_pair.append([text] + current_SMILES_list + ['{}'.format(l2_lambda)])\n",
    "\n",
    "        current_result_list = evaluate_SMILES_list(current_SMILES_list, text)\n",
    "        result_eval_list_one_pair.append(current_result_list)\n",
    "        print()\n",
    "    \n",
    "    result_eval_list_one_pair = np.array(result_eval_list_one_pair)\n",
    "    result_eval_list_one_pair = np.any(result_eval_list_one_pair, axis=0, keepdims=True)\n",
    "    return result_SMILES_list_one_pair, result_eval_list_one_pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7b4ac",
   "metadata": {},
   "source": [
    "## Start Molecule Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2491585e-cae4-4d36-b5df-764cf72e9115",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start editing\n",
      "\n",
      "\n",
      "\n",
      "===== for text prompt: This molecule is soluble in water. =====\n",
      "===== for SMILES OC1C2C1CC2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: MOLECULE VALIDATION AND SANITIZATION CURRENTLY DISABLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use random noise for init\n",
      "l2 lambda: 1.0\n",
      "Use random noise for init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 38.36it/s]\n",
      "WARNING: MOLECULE VALIDATION AND SANITIZATION CURRENTLY DISABLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip loss: -0.92124\tL2 loss: 0.33059\n",
      "SMILES_list: ['OC1C2C1CC2', 'OC12CC1C2', 'OC1CC2CC(O)(C1)C2']\n",
      "SMILES: OC1C2C1CC2\t\t\tlogP: 0.38710\n",
      "SMILES: OC1CC2CC(O)(C1)C2\t\t\tlogP: 0.28220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"start editing\\n\\n\\n\")\n",
    "\n",
    "source_SMILES_list = get_SMILES_list(args)\n",
    "\n",
    "description = \"This molecule is soluble in water.\"\n",
    "\n",
    "\n",
    "print(\"===== for text prompt: {} =====\".format(description))\n",
    "result_SMILES_list, result_acc_list = [], []\n",
    "\n",
    "for SMILES in source_SMILES_list:\n",
    "    print(\"===== for SMILES {} =====\".format(SMILES))\n",
    "    result_SMILES_list_, result_acc_list_ = check_edit(SMILES, description, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
