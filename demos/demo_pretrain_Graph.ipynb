{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for MoleculeSTM pretraining\n",
    "\n",
    "All the scripts can be found in `MoleculeSTM/pretrain.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Customize Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments\t Namespace(CL_neg_samples=1, JK='last', SSL_emb_dim=256, SSL_loss='EBM_NCE', T=0.1, batch_size=4, dataset='PubChemSTM1K', dataspace_path='../data', decay=0, device=0, dropout_ratio=0.5, epochs=100, gnn_emb_dim=300, gnn_type='gin', graph_pooling='mean', max_seq_len=512, megamolbart_input_dir='../data/pretrained_MegaMolBART/checkpoints', mol_lr=0.0001, mol_lr_scale=0.1, molecule_type='Graph', normalize=True, num_layer=5, num_workers=8, output_model_dir=None, pretrain_gnn_mode='GraphMVP_G', seed=42, text_lr=0.0001, text_lr_scale=0.1, text_type='SciBERT', verbose=1)\n"
     ]
    }
   ],
   "source": [
    "# Set-up the environment variable to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--device\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"PubChemSTM1K\")\n",
    "parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\"])\n",
    "parser.add_argument(\"--molecule_type\", type=str, default=\"Graph\", choices=[\"SMILES\", \"Graph\"])\n",
    "\n",
    "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "parser.add_argument(\"--text_lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--mol_lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--text_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--mol_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "parser.add_argument(\"--decay\", type=float, default=0)\n",
    "parser.add_argument(\"--verbose\", type=int, default=1)\n",
    "parser.add_argument(\"--output_model_dir\", type=str, default=None)\n",
    "\n",
    "########## for SciBERT ##########\n",
    "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
    "\n",
    "########## for MegaMolBART ##########\n",
    "parser.add_argument(\"--megamolbart_input_dir\", type=str, default=\"../data/pretrained_MegaMolBART/checkpoints\")\n",
    "\n",
    "########## for 2D GNN ##########\n",
    "parser.add_argument(\"--pretrain_gnn_mode\", type=str, default=\"GraphMVP_G\", choices=[\"GraphMVP_G\"])\n",
    "parser.add_argument(\"--gnn_emb_dim\", type=int, default=300)\n",
    "parser.add_argument(\"--num_layer\", type=int, default=5)\n",
    "parser.add_argument('--JK', type=str, default='last')\n",
    "parser.add_argument(\"--dropout_ratio\", type=float, default=0.5)\n",
    "parser.add_argument(\"--gnn_type\", type=str, default=\"gin\")\n",
    "parser.add_argument('--graph_pooling', type=str, default='mean')\n",
    "\n",
    "########## for contrastive SSL ##########\n",
    "parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
    "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
    "parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
    "parser.add_argument(\"--T\", type=float, default=0.1)\n",
    "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
    "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
    "parser.set_defaults(normalize=True)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "print(\"arguments\\t\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader as torch_DataLoader\n",
    "\n",
    "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from MoleculeSTM.datasets import (\n",
    "    PubChemSTM_Datasets_SMILES, PubChemSTM_SubDatasets_SMILES,\n",
    "    PubChemSTM_Datasets_Graph, PubChemSTM_SubDatasets_Graph,\n",
    "    PubChemSTM_Datasets_Raw_SMILES, PubChemSTM_SubDatasets_Raw_SMILES,\n",
    "    PubChemSTM_Datasets_Raw_Graph, PubChemSTM_SubDatasets_Raw_Graph\n",
    ")\n",
    "from MoleculeSTM.models import GNN, GNN_graphpred\n",
    "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_index(num, shift):\n",
    "    arr = torch.arange(num) + shift\n",
    "    arr[-shift:] = torch.arange(shift)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def do_CL(X, Y, args):\n",
    "    if args.normalize:\n",
    "        X = F.normalize(X, dim=-1)\n",
    "        Y = F.normalize(Y, dim=-1)\n",
    "\n",
    "    if args.SSL_loss == 'EBM_NCE':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        neg_Y = torch.cat([Y[cycle_index(len(Y), i + 1)] for i in range(args.CL_neg_samples)], dim=0)\n",
    "        neg_X = X.repeat((args.CL_neg_samples, 1))\n",
    "\n",
    "        pred_pos = torch.sum(X * Y, dim=1) / args.T\n",
    "        pred_neg = torch.sum(neg_X * neg_Y, dim=1) / args.T\n",
    "\n",
    "        loss_pos = criterion(pred_pos, torch.ones(len(pred_pos)).to(pred_pos.device))\n",
    "        loss_neg = criterion(pred_neg, torch.zeros(len(pred_neg)).to(pred_neg.device))\n",
    "        CL_loss = (loss_pos + args.CL_neg_samples * loss_neg) / (1 + args.CL_neg_samples)\n",
    "\n",
    "        CL_acc = (torch.sum(pred_pos > 0).float() + torch.sum(pred_neg < 0).float()) / \\\n",
    "                 (len(pred_pos) + len(pred_neg))\n",
    "        CL_acc = CL_acc.detach().cpu().item()\n",
    "\n",
    "    elif args.SSL_loss == 'InfoNCE':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        B = X.size()[0]\n",
    "        logits = torch.mm(X, Y.transpose(1, 0))  # B*B\n",
    "        logits = torch.div(logits, args.T)\n",
    "        labels = torch.arange(B).long().to(logits.device)  # B*1\n",
    "\n",
    "        CL_loss = criterion(logits, labels)\n",
    "        pred = logits.argmax(dim=1, keepdim=False)\n",
    "        CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
    "\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    return CL_loss, CL_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    epoch,\n",
    "    dataloader,\n",
    "    text_model, text_tokenizer,\n",
    "    molecule_model, MegaMolBART_wrapper=None):\n",
    "\n",
    "    text_model.train()\n",
    "    molecule_model.train()\n",
    "    text2latent.train()\n",
    "    mol2latent.train()\n",
    "\n",
    "    if args.verbose:\n",
    "        L = tqdm(dataloader)\n",
    "    else:\n",
    "        L = dataloader\n",
    "    \n",
    "    start_time = time.time()\n",
    "    accum_loss, accum_acc = 0, 0\n",
    "    for step, batch in enumerate(L):\n",
    "        description = batch[0]\n",
    "        molecule_data = batch[1]\n",
    "\n",
    "        description_tokens_ids, description_masks = prepare_text_tokens(\n",
    "            device=device, description=description, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
    "        description_output = text_model(input_ids=description_tokens_ids, attention_mask=description_masks)\n",
    "        description_repr = description_output[\"pooler_output\"]\n",
    "        description_repr = text2latent(description_repr)\n",
    "\n",
    "        molecule_data = molecule_data.to(device)\n",
    "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
    "            molecule_data, mol2latent=mol2latent,\n",
    "            molecule_type=molecule_type, molecule_model=molecule_model)\n",
    "\n",
    "        loss_01, acc_01 = do_CL(description_repr, molecule_repr, args)\n",
    "        loss_02, acc_02 = do_CL(molecule_repr, description_repr, args)\n",
    "        loss = (loss_01 + loss_02) / 2\n",
    "        acc = (acc_01 + acc_02) / 2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accum_loss += loss.item()\n",
    "        accum_acc += acc\n",
    "    \n",
    "    accum_loss /= len(L)\n",
    "    accum_acc /= len(L)\n",
    "    \n",
    "    global optimal_loss\n",
    "    temp_loss = accum_loss\n",
    "    if temp_loss < optimal_loss:\n",
    "        optimal_loss = temp_loss\n",
    "    print(\"CL Loss: {:.5f}\\tCL Acc: {:.5f}\\tTime: {:.5f}\".format(accum_loss, accum_acc, time.time() - start_time))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Start Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
    "    if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# if torch.cuda.is_available():\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prepare Text Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download SciBert to ../data/pretrained_SciBERT\n"
     ]
    }
   ],
   "source": [
    "kwargs = {}\n",
    "\n",
    "if args.text_type == \"SciBERT\":\n",
    "    pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
    "    print(\"Download SciBert to {}\".format(pretrained_SciBERT_folder))\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
    "    text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
    "    kwargs[\"text_tokenizer\"] = text_tokenizer\n",
    "    kwargs[\"text_model\"] = text_model\n",
    "    text_dim = 768\n",
    "else:\n",
    "    raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Start training MoleculeSTM-Graph\n",
    "\n",
    "#### 5.3.1 Prepare GraphMVP (Graph Model) and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "  3%|▎         | 8689/250952 [00:19<08:49, 457.21it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'GetProp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m molecule_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# You need to first run the following for data preprocessing if you haven't done so.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# PubChemSTM_Datasets_Graph(dataset_root)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPubChemSTM_SubDatasets_Graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m dataloader_class \u001b[38;5;241m=\u001b[39m pyg_DataLoader\n\u001b[1;32m     11\u001b[0m molecule_node_model \u001b[38;5;241m=\u001b[39m GNN(\n\u001b[1;32m     12\u001b[0m     num_layer\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_layer, emb_dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgnn_emb_dim,\n\u001b[1;32m     13\u001b[0m     JK\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mJK, drop_ratio\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdropout_ratio,\n\u001b[1;32m     14\u001b[0m     gnn_type\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgnn_type)\n",
      "File \u001b[0;32m~/Documents/uiuc_mcs_courses/Spring2024/CS598_dlh/MoleculeSTM/MoleculeSTM/datasets/PubChemSTM.py:204\u001b[0m, in \u001b[0;36mPubChemSTM_SubDatasets_Graph.__init__\u001b[0;34m(self, root, size, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# `process` result file\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCID_text_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed/CID_text_list.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPubChemSTM_Datasets_Graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_Graph_CID_and_text()\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STM/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:81\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     74\u001b[0m     root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     force_reload: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data: Optional[BaseData] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STM/lib/python3.8/site-packages/torch_geometric/data/dataset.py:115\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STM/lib/python3.8/site-packages/torch_geometric/data/dataset.py:260\u001b[0m, in \u001b[0;36mDataset._process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing...\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    259\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_transform.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    263\u001b[0m fs\u001b[38;5;241m.\u001b[39mtorch_save(_repr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transform), path)\n",
      "File \u001b[0;32m~/Documents/uiuc_mcs_courses/Spring2024/CS598_dlh/MoleculeSTM/MoleculeSTM/datasets/PubChemSTM.py:132\u001b[0m, in \u001b[0;36mPubChemSTM_Datasets_Graph.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m CID2graph \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mol \u001b[38;5;129;01min\u001b[39;00m tqdm(suppl):\n\u001b[0;32m--> 132\u001b[0m     CID \u001b[38;5;241m=\u001b[39m \u001b[43mmol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetProp\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUBCHEM_COMPOUND_CID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m     CID \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(CID)\n\u001b[1;32m    134\u001b[0m     graph \u001b[38;5;241m=\u001b[39m mol_to_graph_data_obj_simple(mol)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'GetProp'"
     ]
    }
   ],
   "source": [
    "dataset_root = os.path.join(args.dataspace_path, \"PubChemSTM_data\")\n",
    "    \n",
    "molecule_type = \"Graph\"\n",
    "\n",
    "# You need to first run the following for data preprocessing if you haven't done so.\n",
    "# PubChemSTM_Datasets_Graph(dataset_root)\n",
    "dataset = PubChemSTM_SubDatasets_Graph(dataset_root, size=1000)\n",
    "\n",
    "dataloader_class = pyg_DataLoader\n",
    "\n",
    "molecule_node_model = GNN(\n",
    "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim,\n",
    "    JK=args.JK, drop_ratio=args.dropout_ratio,\n",
    "    gnn_type=args.gnn_type)\n",
    "molecule_model = GNN_graphpred(\n",
    "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim, JK=args.JK, graph_pooling=args.graph_pooling,\n",
    "    num_tasks=1, molecule_node_model=molecule_node_model)\n",
    "pretrained_model_path = os.path.join(args.dataspace_path, \"pretrained_GraphMVP\", args.pretrain_gnn_mode, \"model.pth\")\n",
    "molecule_model.from_pretrained(pretrained_model_path)\n",
    "\n",
    "molecule_model = molecule_model.to(device)\n",
    "\n",
    "kwargs[\"molecule_model\"] = molecule_model\n",
    "molecule_dim = args.gnn_emb_dim\n",
    "\n",
    "dataloader = dataloader_class(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Prepare Two Projection Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'molecule_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text2latent \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(text_dim, args\u001b[38;5;241m.\u001b[39mSSL_emb_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m mol2latent \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[43mmolecule_dim\u001b[49m, args\u001b[38;5;241m.\u001b[39mSSL_emb_dim)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'molecule_dim' is not defined"
     ]
    }
   ],
   "source": [
    "text2latent = nn.Linear(text_dim, args.SSL_emb_dim).to(device)\n",
    "mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Prepare Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_param_group \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtext_model\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mtext_lr},\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: molecule_model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mmol_lr},\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: text2latent\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mtext_lr \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mtext_lr_scale},\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: mol2latent\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mmol_lr \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mmol_lr_scale},\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_param_group, weight_decay\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdecay)\n\u001b[1;32m      8\u001b[0m optimal_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_param_group = [\n",
    "    {\"params\": text_model.parameters(), \"lr\": args.text_lr},\n",
    "    {\"params\": molecule_model.parameters(), \"lr\": args.mol_lr},\n",
    "    {\"params\": text2latent.parameters(), \"lr\": args.text_lr * args.text_lr_scale},\n",
    "    {\"params\": mol2latent.parameters(), \"lr\": args.mol_lr * args.mol_lr_scale},\n",
    "]\n",
    "optimizer = optim.Adam(model_param_group, weight_decay=args.decay)\n",
    "optimal_loss = 1e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:57<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.71635\tCL Acc: 0.50225\tTime: 57.53959\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:56<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.70258\tCL Acc: 0.49950\tTime: 56.35668\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:56<00:00,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.69960\tCL Acc: 0.49900\tTime: 56.90493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(3):\n",
    "    print(\"Epoch {}\".format(e))\n",
    "    train(e, dataloader, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
