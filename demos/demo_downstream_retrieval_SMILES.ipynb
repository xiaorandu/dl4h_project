{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a41a864",
   "metadata": {},
   "source": [
    "# Demo for MoleculeSTM Downstream: Structure-Text Retrieval\n",
    "\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9bc8496",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networkx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader \u001b[38;5;28;01mas\u001b[39;00m pyg_DataLoader\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMoleculeSTM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DrugBank_Datasets_SMILES_retrieval, DrugBank_Datasets_Graph_retrieval\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMoleculeSTM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmega_molbart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmega_mol_bart\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MegaMolBART\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMoleculeSTM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GNN, GNN_graphpred\n",
      "File \u001b[0;32m~/Documents/uiuc_mcs_courses/Spring2024/CS598_dlh/MoleculeSTM/MoleculeSTM/datasets/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMoleculeSTM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPubChemSTM\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PubChemSTM_Datasets_SMILES, PubChemSTM_SubDatasets_SMILES, PubChemSTM_Datasets_Graph, PubChemSTM_SubDatasets_Graph, PubChemSTM_Datasets_Only_SMILES, PubChemSTM_Datasets_SMILES_and_Graph\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMoleculeSTM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPubChemSTM_raw\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PubChemSTM_Datasets_Raw_SMILES, PubChemSTM_SubDatasets_Raw_SMILES, PubChemSTM_Datasets_Raw_Graph, PubChemSTM_SubDatasets_Raw_Graph\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMoleculeSTM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMoleculeNet_Graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MoleculeNetGraphDataset\n",
      "File \u001b[0;32m~/Documents/uiuc_mcs_courses/Spring2024/CS598_dlh/MoleculeSTM/MoleculeSTM/datasets/PubChemSTM.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RDLogger\n\u001b[1;32m     13\u001b[0m RDLogger\u001b[38;5;241m.\u001b[39mDisableLog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrdApp.*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMoleculeSTM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mol_to_graph_data_obj_simple\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPubChemSTM_Datasets_Only_SMILES\u001b[39;00m(Dataset):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root, subset_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/uiuc_mcs_courses/Spring2024/CS598_dlh/MoleculeSTM/MoleculeSTM/datasets/utils.py:2\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'networkx'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader as torch_DataLoader\n",
    "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from MoleculeSTM.datasets import DrugBank_Datasets_SMILES_retrieval, DrugBank_Datasets_Graph_retrieval\n",
    "from MoleculeSTM.models.mega_molbart.mega_mol_bart import MegaMolBART\n",
    "from MoleculeSTM.models import GNN, GNN_graphpred\n",
    "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network\n",
    "\n",
    "# Set-up the environment variable to ignore warnings\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce71cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c3a7eee",
   "metadata": {},
   "source": [
    "## Setup Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d76596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments\t Namespace(CL_neg_samples=1, SSL_emb_dim=256, SSL_loss='EBM_NCE', T=0.1, T_list=[4, 10, 20], batch_size=32, dataspace_path='../data', decay=0, device=0, epochs=1, eval_train=0, input_model_dir='demo_checkpoints_SMILES', input_model_path='demo_checkpoints_SMILES/molecule_model.pth', load_latent_projector=1, max_seq_len=512, mol_lr=1e-05, mol_lr_scale=0.1, molecule_type='SMILES', normalize=True, num_workers=8, seed=42, task='molecule_description', test_mode='given_text', text_lr=1e-05, text_lr_scale=0.1, text_type='SciBERT', training_mode='zero_shot', verbose=0, vocab_path='../MoleculeSTM/bart_vocab.txt')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--device\", type=int, default=0)\n",
    "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
    "parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\", \"BioBERT\"])\n",
    "parser.add_argument(\"--load_latent_projector\", type=int, default=1)\n",
    "parser.add_argument(\"--training_mode\", type=str, default=\"zero_shot\", choices=[\"zero_shot\"])\n",
    "\n",
    "########## for dataset and split ##########\n",
    "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
    "parser.add_argument(\"--task\", type=str, default=\"molecule_description\",\n",
    "    choices=[\n",
    "        \"molecule_description\", \"molecule_description_Raw\",\n",
    "        \"molecule_description_removed_PubChem\", \"molecule_description_removed_PubChem_Raw\",\n",
    "        \"molecule_pharmacodynamics\", \"molecule_pharmacodynamics_Raw\",\n",
    "        \"molecule_pharmacodynamics_removed_PubChem\", \"molecule_pharmacodynamics_removed_PubChem_Raw\"])\n",
    "parser.add_argument(\"--test_mode\", type=str, default=\"given_text\", choices=[\"given_text\", \"given_molecule\"])\n",
    "\n",
    "########## for optimization ##########\n",
    "parser.add_argument(\"--T_list\", type=int, nargs=\"+\", default=[4, 10, 20])\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "parser.add_argument(\"--text_lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--mol_lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--text_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--mol_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--decay\", type=float, default=0)\n",
    "\n",
    "########## for contrastive objective ##########\n",
    "parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
    "parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
    "parser.add_argument(\"--T\", type=float, default=0.1)\n",
    "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
    "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
    "parser.set_defaults(normalize=True)\n",
    "\n",
    "########## for BERT model ##########\n",
    "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
    "\n",
    "########## for molecule model ##########\n",
    "parser.add_argument(\"--molecule_type\", type=str, default=\"SMILES\", choices=[\"SMILES\", \"Graph\"])\n",
    "\n",
    "########## for MegaMolBART ##########\n",
    "parser.add_argument(\"--vocab_path\", type=str, default=\"../MoleculeSTM/bart_vocab.txt\")\n",
    "\n",
    "########## for saver ##########\n",
    "parser.add_argument(\"--eval_train\", type=int, default=0)\n",
    "parser.add_argument(\"--verbose\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--input_model_dir\", type=str, default=\"demo_checkpoints_SMILES\")\n",
    "parser.add_argument(\"--input_model_path\", type=str, default=\"demo_checkpoints_SMILES/molecule_model.pth\")\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "print(\"arguments\\t\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f80fc0",
   "metadata": {},
   "source": [
    "## Setup Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65ca274",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.random.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
    "    if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d7cb65",
   "metadata": {},
   "source": [
    "## Load SciBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e70ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from demo_checkpoints_SMILES/text_model.pth...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
    "text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
    "text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
    "text_dim = 768\n",
    "\n",
    "input_model_path = os.path.join(args.input_model_dir, \"text_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "text_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99247bc",
   "metadata": {},
   "source": [
    "## Load MoleculeSTM-SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4964eb40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from demo_checkpoints_SMILES/molecule_model.pth...\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... None\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... False\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ False\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... infer\n",
      "  data_path ....................... None\n",
      "  dataset_path .................... None\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... False\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ None\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 100\n",
      "  exit_interval ................... None\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  gas ............................. 1\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 256\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ None\n",
      "  local_rank ...................... None\n",
      "  log_interval .................... 100\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. None\n",
      "  lr_decay_iters .................. None\n",
      "  lr_decay_style .................. linear\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 512\n",
      "  merge_file ...................... None\n",
      "  min_lr .......................... 0.0\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 8\n",
      "  num_layers ...................... 4\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... False\n",
      "  pipe_parallel_size .............. 0\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ None\n",
      "  save_interval ................... None\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... None\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 969, 30, 1\n",
      "  synchronize_each_layer .......... False\n",
      "  tensorboard_dir ................. None\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  train_iters ..................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  vocab_file ...................... ../MoleculeSTM/bart_vocab.txt\n",
      "  warmup .......................... 0.01\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 0.0\n",
      "  zero_contigious_gradients ....... False\n",
      "  zero_reduce_bucket_size ......... 0.0\n",
      "  zero_reduce_scatter ............. False\n",
      "  zero_stage ...................... 1.0\n",
      "---------------- end of arguments ----------------\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "Loading vocab from ../MoleculeSTM/bart_vocab.txt.\n"
     ]
    }
   ],
   "source": [
    "input_model_path = os.path.join(args.input_model_dir, \"molecule_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "MegaMolBART_wrapper = MegaMolBART(vocab_path=args.vocab_path, input_dir=None, output_dir=None)\n",
    "molecule_model = MegaMolBART_wrapper.model\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "molecule_model.load_state_dict(state_dict)\n",
    "molecule_dim = 256\n",
    "\n",
    "# Rewrite the seed by MegaMolBART\n",
    "np.random.seed(args.seed)\n",
    "torch.random.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4a0cf",
   "metadata": {},
   "source": [
    "## Load Projection Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d28fd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from demo_checkpoints_SMILES/text2latent_model.pth...\n",
      "Loading from demo_checkpoints_SMILES/mol2latent_model.pth...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2latent = nn.Linear(text_dim, args.SSL_emb_dim)\n",
    "input_model_path = os.path.join(args.input_model_dir, \"text2latent_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "text2latent.load_state_dict(state_dict)\n",
    "\n",
    "mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim)\n",
    "input_model_path = os.path.join(args.input_model_dir, \"mol2latent_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "mol2latent.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cd050",
   "metadata": {},
   "source": [
    "## Define Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146e5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_index(num, shift):\n",
    "    arr = torch.arange(num) + shift\n",
    "    arr[-shift:] = torch.arange(shift)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def do_CL_eval(X, Y, neg_Y, args):\n",
    "    X = F.normalize(X, dim=-1)\n",
    "    X = X.unsqueeze(1) # B, 1, d\n",
    "\n",
    "    Y = Y.unsqueeze(0)\n",
    "    Y = torch.cat([Y, neg_Y], dim=0) # T, B, d\n",
    "    Y = Y.transpose(0, 1)  # B, T, d\n",
    "    Y = F.normalize(Y, dim=-1)\n",
    "\n",
    "    logits = torch.bmm(X, Y.transpose(1, 2)).squeeze()  # B*T\n",
    "    B = X.size()[0]\n",
    "    labels = torch.zeros(B).long().to(logits.device)  # B*1\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    CL_loss = criterion(logits, labels)\n",
    "    pred = logits.argmax(dim=1, keepdim=False)\n",
    "    confidence = logits\n",
    "    CL_conf = confidence.max(dim=1)[0]\n",
    "    CL_conf = CL_conf.cpu().numpy()\n",
    "\n",
    "    CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
    "    return CL_loss, CL_conf, CL_acc\n",
    "\n",
    "\n",
    "def get_text_repr(text):\n",
    "    text_tokens_ids, text_masks = prepare_text_tokens(\n",
    "        device=device, description=text, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
    "    text_output = text_model(input_ids=text_tokens_ids, attention_mask=text_masks)\n",
    "    text_repr = text_output[\"pooler_output\"]\n",
    "    text_repr = text2latent(text_repr)\n",
    "    return text_repr\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(dataloader):\n",
    "    text_model.eval()\n",
    "    molecule_model.eval()\n",
    "    text2latent.eval()\n",
    "    mol2latent.eval()\n",
    "\n",
    "    accum_acc_list = [0 for _ in args.T_list]\n",
    "    if args.verbose:\n",
    "        L = tqdm(dataloader)\n",
    "    else:\n",
    "        L = dataloader\n",
    "    for batch in L:\n",
    "        text = batch[0]\n",
    "        molecule_data = batch[1]\n",
    "        neg_text = batch[2]\n",
    "        neg_molecule_data = batch[3]\n",
    "\n",
    "        text_repr = get_text_repr(text)\n",
    "        SMILES_list = list(molecule_data)\n",
    "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
    "            SMILES_list, mol2latent=mol2latent,\n",
    "            molecule_type=\"SMILES\", MegaMolBART_wrapper=MegaMolBART_wrapper)\n",
    "\n",
    "        if test_mode == \"given_text\":\n",
    "            neg_molecule_repr = [\n",
    "                get_molecule_repr_MoleculeSTM(\n",
    "                    list(neg_molecule_data[idx]), mol2latent=mol2latent,\n",
    "                    molecule_type=\"SMILES\", MegaMolBART_wrapper=MegaMolBART_wrapper) for idx in range(T_max)\n",
    "            ]\n",
    "            neg_molecule_repr = torch.stack(neg_molecule_repr)\n",
    "\n",
    "            for T_idx, T in enumerate(args.T_list):\n",
    "                _, _, acc = do_CL_eval(text_repr, molecule_repr, neg_molecule_repr[:T-1], args)\n",
    "                accum_acc_list[T_idx] += acc\n",
    "        elif test_mode == \"given_molecule\":\n",
    "            neg_text_repr = [get_text_repr(neg_text[idx]) for idx in range(T_max)]\n",
    "            neg_text_repr = torch.stack(neg_text_repr)\n",
    "            for T_idx, T in enumerate(args.T_list):\n",
    "                _, _, acc = do_CL_eval(molecule_repr, text_repr, neg_text_repr[:T-1], args)\n",
    "                accum_acc_list[T_idx] += acc\n",
    "        else:\n",
    "            raise Exception\n",
    "    \n",
    "    accum_acc_list = np.array(accum_acc_list)\n",
    "    accum_acc_list /= len(dataloader)\n",
    "    return accum_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b41532",
   "metadata": {},
   "source": [
    "## Start Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebfb842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading negative samples from ../data/DrugBank_data/index/SMILES_description_full.txt\n",
      "Results [0.94256757 0.89864865 0.84797297]\n"
     ]
    }
   ],
   "source": [
    "text_model = text_model.to(device)\n",
    "molecule_model = molecule_model.to(device)\n",
    "text2latent = text2latent.to(device)\n",
    "mol2latent = mol2latent.to(device)\n",
    "\n",
    "T_max = max(args.T_list) - 1\n",
    "\n",
    "initial_test_acc_list = []\n",
    "test_mode = args.test_mode\n",
    "dataset_folder = os.path.join(args.dataspace_path, \"DrugBank_data\")\n",
    "\n",
    "dataset_class = DrugBank_Datasets_SMILES_retrieval\n",
    "dataloader_class = torch_DataLoader\n",
    "\n",
    "if args.task == \"molecule_description\":\n",
    "    template = \"SMILES_description_{}.txt\"\n",
    "elif args.task == \"molecule_description_removed_PubChem\":\n",
    "    template = \"SMILES_description_removed_from_PubChem_{}.txt\"\n",
    "elif args.task == \"molecule_description_Raw\":\n",
    "    template = \"SMILES_description_{}_Raw.txt\"\n",
    "elif args.task == \"molecule_description_removed_PubChem_Raw\":\n",
    "    template = \"SMILES_description_removed_from_PubChem_{}_Raw.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics\":\n",
    "    template = \"SMILES_pharmacodynamics_{}.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics_removed_PubChem\":\n",
    "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics_Raw\":\n",
    "    template = \"SMILES_pharmacodynamics_{}_Raw.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics_removed_PubChem_Raw\":\n",
    "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}_Raw.txt\"\n",
    "\n",
    "full_dataset = dataset_class(dataset_folder, 'full', neg_sample_size=T_max, template=template)\n",
    "full_dataloader = dataloader_class(full_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers) # The program will get blcoked with none-zero num_workers\n",
    "\n",
    "initial_test_acc_list = eval_epoch(full_dataloader)\n",
    "print('Results', initial_test_acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
