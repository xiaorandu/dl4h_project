{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for MoleculeSTM pretraining\n",
    "\n",
    "All the scripts can be found in `MoleculeSTM/pretrain.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Customize Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments\t Namespace(CL_neg_samples=1, SSL_emb_dim=256, SSL_loss='EBM_NCE', T=0.1, batch_size=4, dataset='PubChemSTM1K', dataspace_path='../data', decay=0, device=0, epochs=100, max_seq_len=512, megamolbart_input_dir='../data/pretrained_MegaMolBART/checkpoints', mol_lr=0.0001, mol_lr_scale=0.1, molecule_type='SMILES', normalize=True, num_workers=8, output_model_dir=None, seed=42, text_lr=0.0001, text_lr_scale=0.1, text_type='SciBERT', verbose=1)\n"
     ]
    }
   ],
   "source": [
    "# Set-up the environment variable to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--device\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"PubChemSTM1K\")\n",
    "parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\"])\n",
    "parser.add_argument(\"--molecule_type\", type=str, default=\"SMILES\", choices=[\"SMILES\", \"Graph\"])\n",
    "\n",
    "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "parser.add_argument(\"--text_lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--mol_lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--text_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--mol_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "parser.add_argument(\"--decay\", type=float, default=0)\n",
    "parser.add_argument(\"--verbose\", type=int, default=1)\n",
    "parser.add_argument(\"--output_model_dir\", type=str, default=None)\n",
    "\n",
    "########## for SciBERT ##########\n",
    "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
    "\n",
    "########## for MegaMolBART ##########\n",
    "parser.add_argument(\"--megamolbart_input_dir\", type=str, default=\"../data/pretrained_MegaMolBART/checkpoints\")\n",
    "\n",
    "########## for contrastive SSL ##########\n",
    "parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
    "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
    "parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
    "parser.add_argument(\"--T\", type=float, default=0.1)\n",
    "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
    "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
    "parser.set_defaults(normalize=True)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "print(\"arguments\\t\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-02 20:18:58,964] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: APEX is not installed, multi_tensor_applier will not be available.\n",
      "WARNING: APEX is not installed, using torch.nn.LayerNorm instead of apex.normalization.FusedLayerNorm!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'apex.normalization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMoleculeSTM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GNN, GNN_graphpred\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMoleculeSTM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMoleculeSTM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmega_molbart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmega_mol_bart\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MegaMolBART\n",
      "File \u001b[0;32m~/Documents/uiuc_mcs_courses/Spring2024/CS598_dlh/MoleculeSTM/MoleculeSTM/models/mega_molbart/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMoleculeSTM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmega_molbart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmegatron_bart\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MegatronBART\n",
      "File \u001b[0;32m~/Documents/uiuc_mcs_courses/Spring2024/CS598_dlh/MoleculeSTM/MoleculeSTM/models/mega_molbart/megatron_bart.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmegatron\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MegatronModule\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FusedLayerNorm\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmegatron\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mpu\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'apex.normalization'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader as torch_DataLoader\n",
    "\n",
    "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from MoleculeSTM.datasets import (\n",
    "    PubChemSTM_Datasets_SMILES, PubChemSTM_SubDatasets_SMILES,\n",
    "    PubChemSTM_Datasets_Graph, PubChemSTM_SubDatasets_Graph,\n",
    "    PubChemSTM_Datasets_Raw_SMILES, PubChemSTM_SubDatasets_Raw_SMILES,\n",
    "    PubChemSTM_Datasets_Raw_Graph, PubChemSTM_SubDatasets_Raw_Graph\n",
    ")\n",
    "from MoleculeSTM.models import GNN, GNN_graphpred\n",
    "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network\n",
    "from MoleculeSTM.models.mega_molbart.mega_mol_bart import MegaMolBART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_index(num, shift):\n",
    "    arr = torch.arange(num) + shift\n",
    "    arr[-shift:] = torch.arange(shift)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def do_CL(X, Y, args):\n",
    "    if args.normalize:\n",
    "        X = F.normalize(X, dim=-1)\n",
    "        Y = F.normalize(Y, dim=-1)\n",
    "\n",
    "    if args.SSL_loss == 'EBM_NCE':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        neg_Y = torch.cat([Y[cycle_index(len(Y), i + 1)] for i in range(args.CL_neg_samples)], dim=0)\n",
    "        neg_X = X.repeat((args.CL_neg_samples, 1))\n",
    "\n",
    "        pred_pos = torch.sum(X * Y, dim=1) / args.T\n",
    "        pred_neg = torch.sum(neg_X * neg_Y, dim=1) / args.T\n",
    "\n",
    "        loss_pos = criterion(pred_pos, torch.ones(len(pred_pos)).to(pred_pos.device))\n",
    "        loss_neg = criterion(pred_neg, torch.zeros(len(pred_neg)).to(pred_neg.device))\n",
    "        CL_loss = (loss_pos + args.CL_neg_samples * loss_neg) / (1 + args.CL_neg_samples)\n",
    "\n",
    "        CL_acc = (torch.sum(pred_pos > 0).float() + torch.sum(pred_neg < 0).float()) / \\\n",
    "                 (len(pred_pos) + len(pred_neg))\n",
    "        CL_acc = CL_acc.detach().cpu().item()\n",
    "\n",
    "    elif args.SSL_loss == 'InfoNCE':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        B = X.size()[0]\n",
    "        logits = torch.mm(X, Y.transpose(1, 0))  # B*B\n",
    "        logits = torch.div(logits, args.T)\n",
    "        labels = torch.arange(B).long().to(logits.device)  # B*1\n",
    "\n",
    "        CL_loss = criterion(logits, labels)\n",
    "        pred = logits.argmax(dim=1, keepdim=False)\n",
    "        CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
    "\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    return CL_loss, CL_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    epoch,\n",
    "    dataloader,\n",
    "    text_model, text_tokenizer,\n",
    "    molecule_model, MegaMolBART_wrapper=None):\n",
    "\n",
    "    text_model.train()\n",
    "    molecule_model.train()\n",
    "    text2latent.train()\n",
    "    mol2latent.train()\n",
    "\n",
    "    if args.verbose:\n",
    "        L = tqdm(dataloader)\n",
    "    else:\n",
    "        L = dataloader\n",
    "    \n",
    "    start_time = time.time()\n",
    "    accum_loss, accum_acc = 0, 0\n",
    "    for step, batch in enumerate(L):\n",
    "        description = batch[0]\n",
    "        molecule_data = batch[1]\n",
    "\n",
    "        description_tokens_ids, description_masks = prepare_text_tokens(\n",
    "            device=device, description=description, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
    "        description_output = text_model(input_ids=description_tokens_ids, attention_mask=description_masks)\n",
    "        description_repr = description_output[\"pooler_output\"]\n",
    "        description_repr = text2latent(description_repr)\n",
    "\n",
    "        molecule_data = list(molecule_data) # for SMILES_list\n",
    "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
    "            molecule_data, mol2latent=mol2latent,\n",
    "            molecule_type=molecule_type, MegaMolBART_wrapper=MegaMolBART_wrapper)\n",
    "\n",
    "        loss_01, acc_01 = do_CL(description_repr, molecule_repr, args)\n",
    "        loss_02, acc_02 = do_CL(molecule_repr, description_repr, args)\n",
    "        loss = (loss_01 + loss_02) / 2\n",
    "        acc = (acc_01 + acc_02) / 2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accum_loss += loss.item()\n",
    "        accum_acc += acc\n",
    "    \n",
    "    accum_loss /= len(L)\n",
    "    accum_acc /= len(L)\n",
    "    \n",
    "    global optimal_loss\n",
    "    temp_loss = accum_loss\n",
    "    if temp_loss < optimal_loss:\n",
    "        optimal_loss = temp_loss        \n",
    "    print(\"CL Loss: {:.5f}\\tCL Acc: {:.5f}\\tTime: {:.5f}\".format(accum_loss, accum_acc, time.time() - start_time))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Start Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
    "    if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# if torch.cuda.is_available():\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prepare Text Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download SciBert to ../data/pretrained_SciBERT\n"
     ]
    }
   ],
   "source": [
    "kwargs = {}\n",
    "\n",
    "if args.text_type == \"SciBERT\":\n",
    "    pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
    "    print(\"Download SciBert to {}\".format(pretrained_SciBERT_folder))\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
    "    text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
    "    kwargs[\"text_tokenizer\"] = text_tokenizer\n",
    "    kwargs[\"text_model\"] = text_model\n",
    "    text_dim = 768\n",
    "else:\n",
    "    raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Start training MoleculeSTM-SMILES\n",
    "\n",
    "#### 5.3.1 Prepare MegaMolBART (SMILES Model) and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/PubChemSTM_data/raw/CID2text.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mdataspace_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPubChemSTM_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m molecule_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMILES\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPubChemSTM_SubDatasets_SMILES\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m dataloader_class \u001b[38;5;241m=\u001b[39m torch_DataLoader\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39moutput_model_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/uiuc_mcs_courses/Spring2024/CS598_dlh/MoleculeSTM/MoleculeSTM/datasets/PubChemSTM.py:91\u001b[0m, in \u001b[0;36mPubChemSTM_SubDatasets_SMILES.__init__\u001b[0;34m(self, root, size)\u001b[0m\n\u001b[1;32m     89\u001b[0m CID2text_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw/CID2text.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m CID2SMILES_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw/CID2SMILES.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_CID2SMILES\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCID2text_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCID2SMILES_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m CID, value_list \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCID2text_data\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Documents/uiuc_mcs_courses/Spring2024/CS598_dlh/MoleculeSTM/MoleculeSTM/datasets/PubChemSTM.py:63\u001b[0m, in \u001b[0;36mPubChemSTM_Datasets_SMILES.load_CID2SMILES\u001b[0;34m(self, CID2text_file, CID2SMILES_file)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_CID2SMILES\u001b[39m(\u001b[38;5;28mself\u001b[39m, CID2text_file, CID2SMILES_file):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCID2text_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCID2text_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen of CID2text: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCID2text_data\u001b[38;5;241m.\u001b[39mkeys())))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/PubChemSTM_data/raw/CID2text.json'"
     ]
    }
   ],
   "source": [
    "dataset_root = os.path.join(args.dataspace_path, \"PubChemSTM_data\")\n",
    "    \n",
    "molecule_type = \"SMILES\"\n",
    "\n",
    "dataset = PubChemSTM_SubDatasets_SMILES(dataset_root, size=1000)\n",
    "dataloader_class = torch_DataLoader\n",
    "\n",
    "if args.output_model_dir is not None:\n",
    "    MegaMolBART_dir = os.path.join(args.output_model_dir, \"SMILES\")\n",
    "else:\n",
    "    MegaMolBART_dir = None\n",
    "MegaMolBART_wrapper = MegaMolBART(\n",
    "    vocab_path=\"../MoleculeSTM/bart_vocab.txt\",\n",
    "    input_dir=args.megamolbart_input_dir,\n",
    "    output_dir=MegaMolBART_dir)\n",
    "molecule_model = MegaMolBART_wrapper.model\n",
    "kwargs[\"MegaMolBART_wrapper\"] = MegaMolBART_wrapper\n",
    "kwargs[\"molecule_model\"] = molecule_model\n",
    "molecule_dim = 256\n",
    "\n",
    "dataloader = dataloader_class(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Prepare Two Projection Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'molecule_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text2latent \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(text_dim, args\u001b[38;5;241m.\u001b[39mSSL_emb_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m mol2latent \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[43mmolecule_dim\u001b[49m, args\u001b[38;5;241m.\u001b[39mSSL_emb_dim)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'molecule_dim' is not defined"
     ]
    }
   ],
   "source": [
    "text2latent = nn.Linear(text_dim, args.SSL_emb_dim).to(device)\n",
    "mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Prepare Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'molecule_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_param_group \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: text_model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mtext_lr},\n\u001b[0;32m----> 3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmolecule_model\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mmol_lr},\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: text2latent\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mtext_lr \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mtext_lr_scale},\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: mol2latent\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mmol_lr \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mmol_lr_scale},\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_param_group, weight_decay\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdecay)\n\u001b[1;32m      8\u001b[0m optimal_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'molecule_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_param_group = [\n",
    "    {\"params\": text_model.parameters(), \"lr\": args.text_lr},\n",
    "    {\"params\": molecule_model.parameters(), \"lr\": args.mol_lr},\n",
    "    {\"params\": text2latent.parameters(), \"lr\": args.text_lr * args.text_lr_scale},\n",
    "    {\"params\": mol2latent.parameters(), \"lr\": args.mol_lr * args.mol_lr_scale},\n",
    "]\n",
    "optimizer = optim.Adam(model_param_group, weight_decay=args.decay)\n",
    "optimal_loss = 1e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.69800\tCL Acc: 0.50400\tTime: 49.79203\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.69504\tCL Acc: 0.50450\tTime: 48.61034\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.69426\tCL Acc: 0.50175\tTime: 48.72926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(3):\n",
    "    print(\"Epoch {}\".format(e))\n",
    "    train(e, dataloader, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
